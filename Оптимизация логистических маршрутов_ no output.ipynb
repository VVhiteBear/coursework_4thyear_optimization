{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptXkbNz0hRt3",
    "outputId": "b1ae055a-551b-4cfe-9a91-a7f0a1e86de5"
   },
   "outputs": [],
   "source": [
    "pip install osmnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwLD-kr0p6tT"
   },
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liVdiafZpvUH"
   },
   "source": [
    "## Загрузка и визуализация датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dSMZ63UzmMEt",
    "outputId": "43598c7a-0785-48ac-d073-a9c68502a51a"
   },
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Настройки osmnx\n",
    "ox.settings.log_console = True\n",
    "ox.settings.use_cache = True\n",
    "ox.settings.timeout = 300\n",
    "\n",
    "def download_moscow_region_optimized():\n",
    "    \"\"\"Скачивание Москвы и Московской области\"\"\"\n",
    "\n",
    "    # Получаем границы регионов\n",
    "    print(\"Получаем границы Москвы и Московской области...\")\n",
    "    places = [\n",
    "        \"Москва, Россия\",\n",
    "        \"Московская область, Россия\"\n",
    "    ]\n",
    "\n",
    "    # Получаем GeoDataFrame с границами\n",
    "    regions_gdf = ox.geocode_to_gdf(places)\n",
    "\n",
    "    # Визуализируем границы\n",
    "    regions_proj = ox.projection.project_gdf(regions_gdf)\n",
    "    fig1, ax1 = plt.subplots(figsize=(10, 10))\n",
    "    regions_proj.plot(ax=ax1, fc=\"gray\", ec=\"blue\", alpha=0.5)\n",
    "    ax1.set_title(\"Границы Москвы и Московской области\")\n",
    "    ax1.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Получаем объединенный полигон\n",
    "    combined_polygon = regions_gdf.unary_union\n",
    "\n",
    "    # Скачиваем дорожную сеть для объединенного региона\n",
    "    print(\"Скачиваем дорожную сеть...\")\n",
    "    graph = ox.graph_from_polygon(\n",
    "        combined_polygon,\n",
    "        network_type='drive',  # только автомобильные дороги\n",
    "        simplify=True\n",
    "    )\n",
    "\n",
    "    return graph\n",
    "\n",
    "# Альтернативный способ - через graph_from_places (проще)\n",
    "def download_moscow_region_simple():\n",
    "    \"\"\"Простой способ скачивания через graph_from_places\"\"\"\n",
    "\n",
    "    places = [\n",
    "        \"Москва, Россия\",\n",
    "        \"Московская область, Россия\"\n",
    "    ]\n",
    "\n",
    "    graph = ox.graph_from_places(\n",
    "        places,\n",
    "        network_type='drive',  # публичные дороги для автомобилей\n",
    "        simplify=True\n",
    "    )\n",
    "\n",
    "    return graph\n",
    "\n",
    "# Скачиваем данные\n",
    "print(\"Скачиваем...\")\n",
    "combined_graph = download_moscow_region_optimized()\n",
    "print(\"Успешно!\")\n",
    "\n",
    "# Основная информация о графе\n",
    "print(f\"Количество узлов: {combined_graph.number_of_nodes()}\")\n",
    "print(f\"Количество рёбер: {combined_graph.number_of_edges()}\")\n",
    "\n",
    "# Визуализируем дорожную сеть\n",
    "print(\"Визуализация дорожной сети...\")\n",
    "fig2, ax2 = ox.plot_graph(\n",
    "    combined_graph,\n",
    "    node_size=0,           # скрываем узлы\n",
    "    edge_linewidth=0.5,\n",
    "    edge_color='#0066cc',\n",
    "    bgcolor='white',\n",
    "    show=False,\n",
    "    close=False\n",
    ")\n",
    "ax2.set_title(\"Дорожная сеть Москвы и Московской области\")\n",
    "plt.show()\n",
    "\n",
    "# Сохраняем граф\n",
    "output_file = \"moscow_region_drive_network.graphml\"\n",
    "ox.save_graphml(combined_graph, output_file)\n",
    "print(f\"Граф сохранен как '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DALWCarvpylC"
   },
   "source": [
    "## Анализ датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3ELwxWJqIN2"
   },
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oq2Wd4alqIn4"
   },
   "outputs": [],
   "source": [
    "# Загружаем граф\n",
    "graph = ox.load_graphml(\"moscow_region_drive_network.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5M4dXQRjrWkE",
    "outputId": "f86a7082-a3c6-4ae1-9f79-b8333328db80"
   },
   "outputs": [],
   "source": [
    "# 1. Основные характеристики\n",
    "print(f\"Количество перекрестков (узлов): {graph.number_of_nodes():,}\")\n",
    "print(f\"Количество дорог (рёбер): {graph.number_of_edges():,}\")\n",
    "print(f\"Плотность сети: {nx.density(graph):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8vwwE1briWB",
    "outputId": "d7838837-0102-4afe-b755-a1d053519666"
   },
   "outputs": [],
   "source": [
    "# 2. Анализ типов дорог\n",
    "def analyze_highway_types(graph):\n",
    "    highway_types = []\n",
    "    road_lengths = {}\n",
    "    oneway_count = 0\n",
    "\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        highway = data.get('highway', 'unknown')\n",
    "        length = data.get('length', 0)\n",
    "\n",
    "        # Считаем односторонние дороги\n",
    "        if data.get('oneway', False) == True:\n",
    "            oneway_count += 1\n",
    "\n",
    "        # Обрабатываем highway типы\n",
    "        if isinstance(highway, list):\n",
    "            for hw in highway:\n",
    "                highway_types.append(hw)\n",
    "                road_lengths[hw] = road_lengths.get(hw, 0) + length\n",
    "        else:\n",
    "            highway_types.append(highway)\n",
    "            road_lengths[highway] = road_lengths.get(highway, 0) + length\n",
    "\n",
    "    return highway_types, road_lengths, oneway_count\n",
    "\n",
    "highway_types, road_lengths, oneway_count = analyze_highway_types(graph)\n",
    "\n",
    "# Статистика по типам дорог\n",
    "type_counts = Counter(highway_types)\n",
    "total_edges = len(highway_types)\n",
    "total_length_km = sum(road_lengths.values()) / 1000\n",
    "\n",
    "print(f\"Всего дорог: {total_edges:,} сегментов\")\n",
    "print(f\"Общая длина: {total_length_km:,.1f} км\")\n",
    "print(f\"Односторонних дорог: {oneway_count} ({oneway_count/total_edges*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nТоп-10 типов дорог по количеству:\")\n",
    "for hw_type, count in type_counts.most_common(10):\n",
    "    percentage = (count / total_edges) * 100\n",
    "    print(f\"• {hw_type:20}: {count:6} сегментов ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\nТоп-10 типов дорог по длине:\")\n",
    "for hw_type in sorted(road_lengths.keys(), key=lambda x: road_lengths[x], reverse=True)[:10]:\n",
    "    length_km = road_lengths[hw_type] / 1000\n",
    "    percentage = (road_lengths[hw_type] / sum(road_lengths.values())) * 100\n",
    "    print(f\"• {hw_type:20}: {length_km:8.1f} км ({percentage:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sqof9WN6rwa0",
    "outputId": "003fc3c7-1e85-47d7-8580-384cdf90839f"
   },
   "outputs": [],
   "source": [
    "# 3. Дополнительные показатели\n",
    "lengths = [data.get('length', 0) for u, v, data in graph.edges(data=True)]\n",
    "if lengths:\n",
    "    print(f\"Средняя длина дороги: {np.mean(lengths):.1f} м\")\n",
    "    print(f\"Медианная длина дороги: {np.median(lengths):.1f} м\")\n",
    "    print(f\"Минимальная длина: {np.min(lengths):.1f} м\")\n",
    "    print(f\"Максимальная длина: {np.max(lengths):.1f} м\")\n",
    "    print(f\"Стандартное отклонение: {np.std(lengths):.1f} м\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9O_q_Yaqe7A",
    "outputId": "86579281-b5a3-4357-a5cf-0642a11b2018"
   },
   "outputs": [],
   "source": [
    "# 5. Степени узлов\n",
    "degrees = [degree for node, degree in graph.degree()]\n",
    "print(f\"Средняя степень узла: {np.mean(degrees):.2f}\")\n",
    "print(f\"Максимальная степень: {np.max(degrees)}\")\n",
    "print(f\"Минимальная степень: {np.min(degrees)}\")\n",
    "\n",
    "# Анализ распределения степеней\n",
    "degree_counts = Counter(degrees)\n",
    "print(\"\\nРаспределение степеней узлов:\")\n",
    "for degree, count in degree_counts.most_common(10):\n",
    "    print(f\"  Степень {degree:2}: {count:5} узлов ({count/graph.number_of_nodes()*100:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QzF-ft_qhSg",
    "outputId": "efa9983a-bc99-4765-e25d-f2ec4e48c15b"
   },
   "outputs": [],
   "source": [
    "# 6. Плотность\n",
    "\n",
    "# Конвертируем в GeoDataFrames\n",
    "nodes_gdf, edges_gdf = ox.graph_to_gdfs(graph)\n",
    "\n",
    "# Рассчитываем bounding box\n",
    "minx, miny, maxx, maxy = nodes_gdf.total_bounds\n",
    "width_km = (maxx - minx) * 111  # приблизительно в км (1 градус ~ 111 км)\n",
    "height_km = (maxy - miny) * 111\n",
    "area_approx = width_km * height_km\n",
    "\n",
    "print(f\"Приблизительная площадь покрытия: {area_approx:,.0f} км²\")\n",
    "print(f\"Плотность узлов: {graph.number_of_nodes()/area_approx:.1f} узлов/км²\")\n",
    "print(f\"Плотность дорог: {total_length_km/area_approx:.1f} км дорог/км²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jIgmJHU0pCeW",
    "outputId": "e9de467e-c823-42e3-b5c1-f4a00db5825a"
   },
   "outputs": [],
   "source": [
    "# 7. Визуализация основных типов дорог\n",
    "def plot_highway_types_simplified(graph):\n",
    "    \"\"\"Упрощенная визуализация с группировкой типов дорог\"\"\"\n",
    "\n",
    "    # Группируем типы дорог для лучшей читаемости\n",
    "    def get_road_category(highway_type):\n",
    "        if isinstance(highway_type, list):\n",
    "            highway_type = highway_type[0]\n",
    "\n",
    "        main_roads = ['motorway', 'trunk', 'primary', 'secondary']\n",
    "        local_roads = ['tertiary', 'unclassified', 'residential']\n",
    "        service_roads = ['service', 'living_street']\n",
    "\n",
    "        if highway_type in main_roads:\n",
    "            return 'main_roads'\n",
    "        elif highway_type in local_roads:\n",
    "            return 'local_roads'\n",
    "        elif highway_type in service_roads:\n",
    "            return 'service_roads'\n",
    "        else:\n",
    "            return 'other'\n",
    "\n",
    "    # Цвета для категорий\n",
    "    color_map = {\n",
    "        'main_roads': 'red',\n",
    "        'local_roads': 'blue',\n",
    "        'service_roads': 'green',\n",
    "        'other': 'gray'\n",
    "    }\n",
    "\n",
    "    # Создаем списки для визуализации\n",
    "    edge_colors = []\n",
    "    categories_count = Counter()\n",
    "\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        highway = data.get('highway', 'other')\n",
    "        category = get_road_category(highway)\n",
    "        categories_count[category] += 1\n",
    "        edge_colors.append(color_map[category])\n",
    "\n",
    "    # Визуализация\n",
    "    fig, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "    ox.plot_graph(\n",
    "        graph,\n",
    "        ax=ax,\n",
    "        node_size=0,\n",
    "        edge_color=edge_colors,\n",
    "        edge_linewidth=0.7,\n",
    "        edge_alpha=0.8,\n",
    "        bgcolor='white',\n",
    "        show=False,\n",
    "        close=False\n",
    "    )\n",
    "\n",
    "    # Легенда\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = []\n",
    "    for category, color in color_map.items():\n",
    "        if category in categories_count:\n",
    "            count = categories_count[category]\n",
    "            legend_elements.append(\n",
    "                Line2D([0], [0], color=color, lw=3,\n",
    "                      label=f\"{category} ({count} roads)\")\n",
    "            )\n",
    "\n",
    "    ax.legend(handles=legend_elements, loc='upper right', fontsize=12)\n",
    "    ax.set_title(\"Дорожная сеть Московского региона\\n(Группировка по типам дорог)\", fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return categories_count\n",
    "\n",
    "# Запускаем визуализацию\n",
    "categories_count = plot_highway_types_simplified(graph)\n",
    "\n",
    "print(\"\\nРаспределение по категориям дорог:\")\n",
    "for category, count in categories_count.most_common():\n",
    "    percentage = count / total_edges * 100\n",
    "    print(f\"  {category:15}: {count:6} дорог ({percentage:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAThguvFjTuY"
   },
   "source": [
    "## Склады"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPz2F-jJ8xHi"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "def geocode_yandex(address, api_key):\n",
    "    \"\"\"Геокодирование через Яндекс API\"\"\"\n",
    "    base_url = \"https://geocode-maps.yandex.ru/1.x/\"\n",
    "    params = {\n",
    "        \"apikey\": api_key,\n",
    "        \"geocode\": address,\n",
    "        \"format\": \"json\",\n",
    "        \"lang\": \"ru_RU\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(f\"Геокодируем: {address}\")\n",
    "        response = requests.get(base_url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "\n",
    "        # Парсим ответ\n",
    "        found = data['response']['GeoObjectCollection']['metaDataProperty']['GeocoderResponseMetaData']['found']\n",
    "        if int(found) > 0:\n",
    "            # Берем первый результат\n",
    "            pos = data['response']['GeoObjectCollection']['featureMember'][0]['GeoObject']['Point']['pos']\n",
    "            # Яндекс возвращает в формате \"долгота широта\"\n",
    "            lon, lat = map(float, pos.split())\n",
    "            print(f\"Найдено: {lat}, {lon}\")\n",
    "            return lat, lon\n",
    "        else:\n",
    "            print(f\"Яндекс не нашел координаты для: {address}\")\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при геокодировании {address} через Яндекс: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def save_to_json(df, filename):\n",
    "    \"\"\"Сохраняет DataFrame в JSON файл\"\"\"\n",
    "    result = []\n",
    "    for _, row in df.iterrows():\n",
    "        result.append({\n",
    "            'name': row['name'],\n",
    "            'address': row['address'],\n",
    "            'latitude': row['latitude'],\n",
    "            'longitude': row['longitude']\n",
    "        })\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Данные сохранены в файл: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Js6axJPM8_qb"
   },
   "outputs": [],
   "source": [
    "# API ключ\n",
    "YANDEX_API_KEY = \"мой api\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQZlKbhj9KIK",
    "outputId": "866436ac-adf5-4f9f-d8ca-af80bc96b403"
   },
   "outputs": [],
   "source": [
    "# Обаботка распределительных и фулфилмент-центров (РЦ/РФЦ)\n",
    "# Список складов РЦ/РФЦ\n",
    "warehouses_data = [\n",
    "    {\"name\": \"ПАВЛО_СЛОБОДСКОЕ_РЦ\", \"address\": \"143581, Московская обл., Истринский район, с. Павловская Слобода, территория квартала 0050343, зд. 2\"},\n",
    "    {\"name\": \"ХОРУГВИНО_РФЦ\", \"address\": \"141533, Московская обл., Солнечногорский р-н, с.п. Пешковское, дер. Хоругвино, стр. 32/2\"},\n",
    "    {\"name\": \"Хоругвино_НЕГАБАРИТ\", \"address\": \"141533, Московская обл., Солнечногорский р-н, с.п. Пешковское, дер. Хоругвино, стр. 32/2\"},\n",
    "    {\"name\": \"Новая_Рига_РФЦ\", \"address\": \"Россия, Московская обл, г. Истра, с. Петровское, территория МПСК Ориентир-Запад, зд. 1A\"},\n",
    "    {\"name\": \"Жуковский\", \"address\": \"140182, Московская область, г. Жуковский, район Замоскворечье, д. 457, стр. 5\"},\n",
    "    {\"name\": \"Софьино\", \"address\": \"Российская Федерация, Московская область, Раменский городской округ, квартал 4/218, стр. 2/1\"},\n",
    "    {\"name\": \"Ногинск\", \"address\": \"142440, РФ, МО, Богородский городской округ, р.п. Обухово, тер. Обухово-Парк, дом 2, строение 1\"},\n",
    "    {\"name\": \"Гривно\", \"address\": \"142184, МО, г.о. Подольск, д. Гривно, тер. пром. парка Гривно, д. 1, к. 13\"},\n",
    "    {\"name\": \"Пушкино-1\", \"address\": \"Россия, Московская область, Пушкинский городской округ, г. Пушкино, Ярославское шоссе, д. 216\"},\n",
    "    {\"name\": \"Пушкино-2\", \"address\": \"Россия, Московская область, г.о. Пушкинский, г. Пушкино, Ярославское шоссе, д. 218\"}\n",
    "]\n",
    "\n",
    "warehouses_df = pd.DataFrame(warehouses_data)\n",
    "\n",
    "# Геокодируем РЦ/РФЦ\n",
    "print(\"Используем Яндекс API для геокодирования РЦ/РФЦ\")\n",
    "coordinates = []\n",
    "for i, address in enumerate(warehouses_df['address']):\n",
    "    print(f\"\\n--- Обрабатываем {i+1}/{len(warehouses_df)} ---\")\n",
    "    lat, lon = geocode_yandex(address, YANDEX_API_KEY)\n",
    "    coordinates.append((lat, lon))\n",
    "    time.sleep(0.5)  # Пауза между запросами\n",
    "\n",
    "# Добавляем координаты в DataFrame\n",
    "warehouses_df['latitude'] = [coord[0] for coord in coordinates]\n",
    "warehouses_df['longitude'] = [coord[1] for coord in coordinates]\n",
    "\n",
    "# Проверяем результаты РЦ/РФЦ\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"Результаты:\")\n",
    "print(\"-\"*40)\n",
    "print(warehouses_df[['name', 'latitude', 'longitude']])\n",
    "\n",
    "# Сохраняем РЦ/РФЦ в файл\n",
    "save_to_json(warehouses_df, \"warehouses_rc_rfc_coordinates.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5ucBpaT988M",
    "outputId": "292c9c94-5890-4f4e-b337-e26b23e11c57"
   },
   "outputs": [],
   "source": [
    "# Обаботка сортировочных центров (СЦ)\n",
    "# Список сортировочных центров (СЦ) в Москве и МО\n",
    "sorting_centers_data = [\n",
    "    {\"name\": \"Анненский\", \"address\": \"г. Москва, Анненский проезд, д. 3, стр. 1\"},\n",
    "    {\"name\": \"Варшавская\", \"address\": \"г. Москва, 1-й Варшавский пр-д, д. 2, стр. 9А\"},\n",
    "    {\"name\": \"Железнодорожный\", \"address\": \"обл. Московская, г. Балашиха, д. Пестово, д. 14А\"},\n",
    "    {\"name\": \"Истра\", \"address\": \"Московская обл., г. Истра, д. Давыдовское, ул. Дачная, стр. 3\"},\n",
    "    {\"name\": \"Королев\", \"address\": \"Московская обл., г. Королев, ул. Силикатная, д. 10А\"},\n",
    "    {\"name\": \"Огуднево\", \"address\": \"Московская область, городской округ Щёлково, деревня Огуднево, ул. Полевая, д. 1/1\"},\n",
    "    {\"name\": \"Щербинка\", \"address\": \"обл. Московская, г. Подольск, д. Борисовка, пром. зона ПромТехАльянс, д. 1, стр. 2\"},\n",
    "    {\"name\": \"Челобитьево\", \"address\": \"г. Мытищи, д. Челобитьево, Осташковское шоссе, владение 15, стр. 1\"},\n",
    "    {\"name\": \"Нижнее Велино\", \"address\": \"обл. Московская, р-н Раменский, д. Нижнее Велино, ш. Старо-Рязанское, корпус 2, стр. 2\"},\n",
    "    {\"name\": \"Воровского\", \"address\": \"Московская область, р.п. имени Воровского, ул. Мира, д. 5, склад 16\"},\n",
    "    {\"name\": \"Львовский (ТСЦ)\", \"address\": \"142155, обл. Московская, г. Подольск, пром. зона Львовский, ул. Московская, д. 69, стр. 5\"},\n",
    "    {\"name\": \"Химки\", \"address\": \"141401, обл. Московская, г. Химки, пр-д Коммунальный, 30А, стр. 1\"},\n",
    "    {\"name\": \"Сухарево\", \"address\": \"Московская обл., Городской округ Мытищи, д. Сухарево, д. 135\"},\n",
    "    {\"name\": \"Чёрная Грязь\", \"address\": \"Московская обл., Солнечногорский р-н, дер. Черная Грязь, ул. Сходненская, стр. 1\"},\n",
    "    {\"name\": \"Балабаново\", \"address\": \"Калужская область, г. Балабаново, Киевское шоссе, д. 96, стр. 2\"},\n",
    "    {\"name\": \"МКШВ\", \"address\": \"г. Москва, ул. Рябиновая, д. 44, стр. 28\"},\n",
    "    {\"name\": \"Скотопрогонная\", \"address\": \"г. Москва, ул. Скотопрогонная, д. 35, стр. 3\"}\n",
    "]\n",
    "\n",
    "sorting_centers_df = pd.DataFrame(sorting_centers_data)\n",
    "\n",
    "# Геокодируем СЦ\n",
    "print(\"Используем Яндекс API для геокодирования СЦ...\")\n",
    "coordinates_sc = []\n",
    "for i, address in enumerate(sorting_centers_df['address']):\n",
    "    print(f\"\\n--- Обрабатываем СЦ {i+1}/{len(sorting_centers_df)} ---\")\n",
    "    lat, lon = geocode_yandex(address, YANDEX_API_KEY)\n",
    "    coordinates_sc.append((lat, lon))\n",
    "    time.sleep(0.5)  # Пауза между запросами\n",
    "\n",
    "# Добавляем координаты в DataFrame\n",
    "sorting_centers_df['latitude'] = [coord[0] for coord in coordinates_sc]\n",
    "sorting_centers_df['longitude'] = [coord[1] for coord in coordinates_sc]\n",
    "\n",
    "# Проверяем результаты СЦ\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"Результаты:\")\n",
    "print(\"-\"*40)\n",
    "print(sorting_centers_df[['name', 'latitude', 'longitude']])\n",
    "\n",
    "# Сохраняем СЦ в файл\n",
    "save_to_json(sorting_centers_df, \"sorting_centers_coordinates.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lftnFqYm8tvM",
    "outputId": "3de3600d-1d11-4f42-cf39-823b25504a7b"
   },
   "outputs": [],
   "source": [
    "# Финальная статистика\n",
    "print(f\"Обработано РЦ/РФЦ: {len(warehouses_df)}\")\n",
    "print(f\"Обработано СЦ: {len(sorting_centers_df)}\")\n",
    "print(f\"Всего объектов: {len(warehouses_df) + len(sorting_centers_df)}\")\n",
    "\n",
    "# Создаем объединенный DataFrame\n",
    "all_warehouses_df = pd.concat([warehouses_df, sorting_centers_df], ignore_index=True)\n",
    "print(f\"Объединенный файл создан: {len(all_warehouses_df)} объектов\")\n",
    "\n",
    "# Сохраняем объединенный файл\n",
    "save_to_json(all_warehouses_df, \"all_warehouses_coordinates.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_E47nzR4YEpJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import osmnx as ox\n",
    "\n",
    "# 1. Карта с 10 складами (РЦ/РФЦ)\n",
    "fig1, ax1 = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "# Рисуем дорожную сеть\n",
    "ox.plot_graph(graph,\n",
    "              ax=ax1,\n",
    "              node_size=0,  # не показываем узлы\n",
    "              edge_color='lightgray',\n",
    "              edge_linewidth=0.5,\n",
    "              bgcolor='white',\n",
    "              show=False,\n",
    "              close=False)\n",
    "\n",
    "# Рисуем 10 складов\n",
    "ax1.scatter(warehouses_df['longitude'],\n",
    "            warehouses_df['latitude'],\n",
    "            c='red', s=100, edgecolors='black', linewidth=1.5,\n",
    "            label='Склады РЦ/РФЦ')\n",
    "\n",
    "# Подписываем склады\n",
    "for idx, row in warehouses_df.iterrows():\n",
    "    ax1.annotate(row['name'][:15] + '...' if len(row['name']) > 15 else row['name'],\n",
    "                (row['longitude'], row['latitude']),\n",
    "                xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=9, fontweight='bold',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "ax1.set_title('10 складов РЦ/РФЦ на карте Московского региона', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Долгота')\n",
    "ax1.set_ylabel('Широта')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('warehouses_10_map.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. Карта со всеми складами\n",
    "fig2, ax2 = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "# Рисуем дорожную сеть\n",
    "ox.plot_graph(graph,\n",
    "              ax=ax2,\n",
    "              node_size=0,\n",
    "              edge_color='lightgray',\n",
    "              edge_linewidth=0.5,\n",
    "              bgcolor='white',\n",
    "              show=False,\n",
    "              close=False)\n",
    "\n",
    "# Рисуем все склады разными цветами\n",
    "# РЦ/РФЦ - красные, СЦ - синие\n",
    "colors = []\n",
    "for idx, row in all_warehouses_df.iterrows():\n",
    "    if idx < len(warehouses_df):  # первые 10 - РЦ/РФЦ\n",
    "        colors.append('red')\n",
    "    else:  # остальные - СЦ\n",
    "        colors.append('blue')\n",
    "\n",
    "ax2.scatter(all_warehouses_df['longitude'],\n",
    "            all_warehouses_df['latitude'],\n",
    "            c=colors, s=100, edgecolors='black', linewidth=1.5)\n",
    "\n",
    "# Подписываем только начальный склад и несколько крупных\n",
    "for idx, row in all_warehouses_df.iterrows():\n",
    "    # Подписываем начальный склад\n",
    "    if idx == 0:\n",
    "        ax2.annotate(row['name'][:20] + '...' if len(row['name']) > 20 else row['name'],\n",
    "                    (row['longitude'], row['latitude']),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=9, fontweight='bold',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    # Подписываем еще 5 случайных складов\n",
    "    elif idx in [5, 10, 15, 20, 25] and idx < len(all_warehouses_df):\n",
    "        ax2.annotate(row['name'][:15] + '...' if len(row['name']) > 15 else row['name'],\n",
    "                    (row['longitude'], row['latitude']),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=8,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.5))\n",
    "\n",
    "# Добавляем легенду\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', label='Склады РЦ/РФЦ',\n",
    "           markerfacecolor='red', markersize=10, markeredgecolor='black'),\n",
    "    Line2D([0], [0], marker='o', color='w', label='Сортировочные центры',\n",
    "           markerfacecolor='blue', markersize=10, markeredgecolor='black')\n",
    "]\n",
    "ax2.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "ax2.set_title(f'Все склады на карте ({len(all_warehouses_df)} объектов)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Долгота')\n",
    "ax2.set_ylabel('Широта')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('warehouses_all_map.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kDcyy4IKSqd"
   },
   "source": [
    "# Классические методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1TBVLURKWAz"
   },
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjrPTChMKfAc",
    "outputId": "0c9bace5-e2ee-4857-f0bb-9e8c0b4e0284"
   },
   "outputs": [],
   "source": [
    "# Загрузка дорожной сети из файла\n",
    "graph = ox.load_graphml(\"moscow_region_drive_network.graphml\")\n",
    "print(\"Дорожный граф загружен из файла\")\n",
    "\n",
    "# Получаем данные об узлах и ребрах графа\n",
    "nodes, edges = ox.graph_to_gdfs(graph)\n",
    "print(f\"Граф содержит: {graph.number_of_nodes()} узлов, {graph.number_of_edges()} дорог\")\n",
    "\n",
    "# Загрузка данных о складах из JSON файла\n",
    "with open('warehouses_rc_rfc_coordinates.json', 'r', encoding='utf-8') as f:\n",
    "    warehouses_json = json.load(f)\n",
    "\n",
    "# Создаем DataFrame из JSON данных\n",
    "warehouses_data = []\n",
    "for warehouse in warehouses_json:\n",
    "    warehouses_data.append({\n",
    "        \"name\": warehouse['name'],\n",
    "        \"latitude\": warehouse['latitude'],\n",
    "        \"longitude\": warehouse['longitude']\n",
    "    })\n",
    "\n",
    "warehouses_df = pd.DataFrame(warehouses_data)\n",
    "print(f\"Загружено {len(warehouses_df)} складов из JSON файла\")\n",
    "\n",
    "# Определяем начальную точку (ПАВЛО_СЛОБОДСКОЕ_РЦ)\n",
    "START_WAREHOUSE = 0\n",
    "print(f\"Начальная точка: {warehouses_df.iloc[START_WAREHOUSE]['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "fbPhRcjOLDgB",
    "outputId": "f38b3b4c-e990-40b4-be40-9063b232a173"
   },
   "outputs": [],
   "source": [
    "warehouses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzfjOpv-L6FS"
   },
   "source": [
    "\"Подключаем\" склады к ближайщим узлам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsKS_zbGMBgX"
   },
   "outputs": [],
   "source": [
    "#анализ подключения складов\n",
    "def calculate_geodesic_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Вычисляет расстояние между двумя точками на Земле по формуле гаверсинусов\n",
    "    Возвращает расстояние в метрах\n",
    "    \"\"\"\n",
    "    from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "    R = 6371000  # Радиус Земли в метрах\n",
    "\n",
    "    # Перевод координат в радианы\n",
    "    lat1_rad = radians(lat1)\n",
    "    lon1_rad = radians(lon1)\n",
    "    lat2_rad = radians(lat2)\n",
    "    lon2_rad = radians(lon2)\n",
    "\n",
    "    # Разницы координат\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Формула гаверсинусов\n",
    "    a = sin(dlat/2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "\n",
    "    return R * c\n",
    "\n",
    "def analyze_warehouse_connections(graph, warehouse_point, warehouse_node, warehouse_name, radius_meters=2000):\n",
    "    \"\"\"\n",
    "    Детальный анализ подключения конкретного склада к дорожной сети\n",
    "\n",
    "    Args:\n",
    "        graph: дорожный граф\n",
    "        warehouse_point: координаты склада (lat, lon)\n",
    "        warehouse_node: привязанный узел графа\n",
    "        warehouse_name: название склада\n",
    "        radius_meters: радиус анализа в метрах\n",
    "\n",
    "    Returns:\n",
    "        dict: детальная информация о подключении\n",
    "    \"\"\"\n",
    "    # Получаем данные привязанного узла\n",
    "    node_data = graph.nodes[warehouse_node]\n",
    "    node_point = (node_data['y'], node_data['x'])  # (lat, lon)\n",
    "\n",
    "    # Вычисляем расстояние от склада до узла\n",
    "    distance_to_node = calculate_geodesic_distance(\n",
    "        warehouse_point[0], warehouse_point[1],\n",
    "        node_point[0], node_point[1]\n",
    "    )\n",
    "\n",
    "    # Анализируем окрестности узла (подграф в радиусе)\n",
    "    subgraph = ox.graph.graph_from_point(\n",
    "        node_point,\n",
    "        dist=radius_meters,\n",
    "        network_type='drive'\n",
    "    )\n",
    "\n",
    "    # Анализ типов дорог в окрестностях\n",
    "    if subgraph.number_of_edges() > 0:\n",
    "        highway_types = []\n",
    "        for u, v, data in subgraph.edges(data=True):\n",
    "            highway = data.get('highway', 'unknown')\n",
    "            if isinstance(highway, list):\n",
    "                highway_types.extend(highway)\n",
    "            else:\n",
    "                highway_types.append(highway)\n",
    "        road_type_counts = pd.Series(highway_types).value_counts()\n",
    "    else:\n",
    "        road_type_counts = pd.Series()\n",
    "\n",
    "    # Анализ подключенных дорог к конкретному узлу\n",
    "    connected_roads_info = []\n",
    "    for neighbor in graph.neighbors(warehouse_node):\n",
    "        edge_data = graph.get_edge_data(warehouse_node, neighbor)\n",
    "        if edge_data:\n",
    "            # Берем первую дорогу (может быть несколько между узлами)\n",
    "            first_edge = list(edge_data.values())[0]\n",
    "            road_type = first_edge.get('highway', 'unknown')\n",
    "            if isinstance(road_type, list):\n",
    "                road_type = road_type[0]\n",
    "            length = first_edge.get('length', 0)\n",
    "            name = first_edge.get('name', 'Без названия')\n",
    "            connected_roads_info.append({\n",
    "                'neighbor_node': neighbor,\n",
    "                'road_type': road_type,\n",
    "                'length_m': length,\n",
    "                'name': name\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        'warehouse_name': warehouse_name,\n",
    "        'warehouse_coords': warehouse_point,\n",
    "        'node_id': warehouse_node,\n",
    "        'node_coords': node_point,\n",
    "        'distance_to_node_m': distance_to_node,\n",
    "        'subgraph_stats': {\n",
    "            'nodes': subgraph.number_of_nodes(),\n",
    "            'edges': subgraph.number_of_edges()\n",
    "        },\n",
    "        'road_types_in_radius': road_type_counts,\n",
    "        'directly_connected_roads': connected_roads_info,\n",
    "        'analysis_radius_m': radius_meters\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NCTYR8gxMtsY",
    "outputId": "aa9fec22-e816-4c9b-88c0-40087036037f"
   },
   "outputs": [],
   "source": [
    "# Привязка складов к узлам графа и детальный анализ\n",
    "warehouse_nodes = []\n",
    "detailed_connections = []\n",
    "\n",
    "for idx, warehouse in warehouses_df.iterrows():\n",
    "    point = (warehouse['latitude'], warehouse['longitude'])\n",
    "\n",
    "    # Находим ближайший узел дорожной сети к складу\n",
    "    nearest_node = ox.distance.nearest_nodes(graph, point[1], point[0])\n",
    "    warehouse_nodes.append(nearest_node)\n",
    "\n",
    "    # Детальный анализ подключения\n",
    "    connection_analysis = analyze_warehouse_connections(\n",
    "        graph, point, nearest_node, warehouse['name']\n",
    "    )\n",
    "    detailed_connections.append(connection_analysis)\n",
    "\n",
    "    print(f\"{warehouse['name']:25} -> Узел {nearest_node}\")\n",
    "    print(f\"  Расстояние до узла: {connection_analysis['distance_to_node_m']:.1f} м\")\n",
    "    print(f\"  Подключен к {len(connection_analysis['directly_connected_roads'])} дорогам\")\n",
    "\n",
    "warehouses_df['node_id'] = warehouse_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rjtbf1CyPrXc",
    "outputId": "63a9affd-5382-4916-b8ad-55daa3a688c4"
   },
   "outputs": [],
   "source": [
    "# Детальный анализ для конкретного склада\n",
    "first_connection = detailed_connections[0]  # Первый склад - нашей начальной точки\n",
    "print(f\"Склад: {first_connection['warehouse_name']}\")\n",
    "print(f\"Узел графа: {first_connection['node_id']}\")\n",
    "print(f\"Координаты склада: {first_connection['warehouse_coords']}\")\n",
    "print(f\"Координаты узла: {first_connection['node_coords']}\")\n",
    "print(f\"Расстояние до узла: {first_connection['distance_to_node_m']:.1f} м\")\n",
    "\n",
    "print(f\"\\nДорожная сеть в радиусе {first_connection['analysis_radius_m']} м:\")\n",
    "print(f\"  - Узлов: {first_connection['subgraph_stats']['nodes']}\")\n",
    "print(f\"  - Дорог: {first_connection['subgraph_stats']['edges']}\")\n",
    "\n",
    "print(\"\\nТипы дорог в окрестностях:\")\n",
    "if not first_connection['road_types_in_radius'].empty:\n",
    "    for road_type, count in first_connection['road_types_in_radius'].head(5).items():\n",
    "        print(f\"  - {road_type}: {count} дорог\")\n",
    "else:\n",
    "    print(\"  - Нет данных о дорогах\")\n",
    "\n",
    "print(\"\\nНепосредственные подключения:\")\n",
    "if first_connection['directly_connected_roads']:\n",
    "    for i, road in enumerate(first_connection['directly_connected_roads'], 1):\n",
    "        print(f\"  {i}. К узлу {road['neighbor_node']}:\")\n",
    "        print(f\"     Тип: {road['road_type']}\")\n",
    "        print(f\"     Длина: {road['length_m']:.0f} м\")\n",
    "        print(f\"     Название: {road['name']}\")\n",
    "else:\n",
    "    print(\"  - Нет непосредственных подключений\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ad0PFzdjRXDt",
    "outputId": "b4c978b4-9d25-4356-c030-d65b392db694"
   },
   "outputs": [],
   "source": [
    "# Анализ качества подключения складов\n",
    "def analyze_connection_quality(detailed_connections):\n",
    "    \"\"\"Анализирует качество подключения складов к дорожной сети\"\"\"\n",
    "\n",
    "    good_connections = 0\n",
    "    medium_connections = 0\n",
    "    poor_connections = 0\n",
    "\n",
    "    for connection in detailed_connections:\n",
    "        distance = connection['distance_to_node_m']\n",
    "        road_count = len(connection['directly_connected_roads'])\n",
    "\n",
    "        # Критерии качества\n",
    "        if distance <= 200 and road_count >= 2:\n",
    "            quality = \"ХОРОШЕЕ\"\n",
    "            good_connections += 1\n",
    "        elif distance <= 500 and road_count >= 1:\n",
    "            quality = \"СРЕДНЕЕ\"\n",
    "            medium_connections += 1\n",
    "        else:\n",
    "            quality = \"ПЛОХОЕ\"\n",
    "            poor_connections += 1\n",
    "\n",
    "        print(f\"{connection['warehouse_name']:25} -> {quality} \"\n",
    "              f\"({distance:.0f} м, {road_count} дорог)\")\n",
    "\n",
    "    print(f\"\\nИТОГО: Хороших: {good_connections}, Средних: {medium_connections}, \"\n",
    "          f\"Плохих: {poor_connections}\")\n",
    "\n",
    "# Запускаем анализ\n",
    "analyze_connection_quality(detailed_connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xi-QqlZN_go",
    "outputId": "14eb3c4e-7444-4574-d76b-5407fd3d6bee"
   },
   "outputs": [],
   "source": [
    "# вычисление матрицы расстояния между складами\n",
    "def calculate_route_distance(route, distance_matrix):\n",
    "    \"\"\"\n",
    "    Вычисляет общее расстояние маршрута на основе матрицы расстояний\n",
    "    \"\"\"\n",
    "    total_distance = 0\n",
    "    for i in range(len(route) - 1):\n",
    "        from_node = route[i]\n",
    "        to_node = route[i+1]\n",
    "        total_distance += distance_matrix[from_node][to_node]\n",
    "    return total_distance\n",
    "\n",
    "# Создаем матрицу расстояний между всеми складами\n",
    "n_warehouses = len(warehouse_nodes)\n",
    "distance_matrix = np.zeros((n_warehouses, n_warehouses))\n",
    "\n",
    "print(\"Вычисляем попарные расстояния между складами\")\n",
    "for i in range(n_warehouses):\n",
    "    for j in range(i+1, n_warehouses):\n",
    "        try:\n",
    "            # Вычисляем кратчайшее расстояние по дорожной сети\n",
    "            distance = nx.shortest_path_length(graph, warehouse_nodes[i], warehouse_nodes[j], weight='length')\n",
    "            distance_matrix[i][j] = distance\n",
    "            distance_matrix[j][i] = distance\n",
    "        except nx.NetworkXNoPath:\n",
    "            # Если пути нет, ставим большое число\n",
    "            distance_matrix[i][j] = 1e9\n",
    "            distance_matrix[j][i] = 1e9\n",
    "            print(f\"Предупреждение: нет пути между складом {i} и складом {j}\")\n",
    "\n",
    "print(\"Матрица расстояний создана!\")\n",
    "print(f\"Размер матрицы: {distance_matrix.shape}\")\n",
    "\n",
    "# Выводим примеры расстояний\n",
    "print(\"\\nПримеры расстояний между складами:\")\n",
    "for i in range(min(3, n_warehouses)):\n",
    "    for j in range(i+1, min(4, n_warehouses)):\n",
    "        dist_km = distance_matrix[i][j] / 1000\n",
    "        print(f\"  {warehouses_df.iloc[i]['name']} -> {warehouses_df.iloc[j]['name']}: {dist_km:.1f} км\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX9gYJQXRyEE"
   },
   "source": [
    "Реализация классических алгоритмов рещения TSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnceV8qVS1-y"
   },
   "outputs": [],
   "source": [
    "# Список для хранения результатов всех алгоритмов\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hV8FDAFBS-31"
   },
   "source": [
    "Алгоритм 1: Жадный алгоритм\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JnC4atIR8Ao"
   },
   "outputs": [],
   "source": [
    "def greedy_tsp(distance_matrix, start_index=0):\n",
    "    \"\"\"\n",
    "    Жадный алгоритм ближайшего соседа для решения TSP\n",
    "\n",
    "    Алгоритм:\n",
    "    1. Начинаем с начальной точки\n",
    "    2. На каждом шаге выбираем ближайший непосещенный город\n",
    "    3. Повторяем пока не посетим все города\n",
    "    4. Возвращаемся в начальную точку\n",
    "\n",
    "    Сложность: O(n^2)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    n = distance_matrix.shape[0]\n",
    "    visited = [False] * n\n",
    "    route = [start_index]\n",
    "    visited[start_index] = True\n",
    "    current = start_index\n",
    "\n",
    "    # Последовательно выбираем ближайшие непосещенные города\n",
    "    while len(route) < n:\n",
    "        nearest_distance = float('inf')\n",
    "        nearest_city = -1\n",
    "\n",
    "        for next_city in range(n):\n",
    "            if not visited[next_city] and distance_matrix[current][next_city] < nearest_distance:\n",
    "                nearest_distance = distance_matrix[current][next_city]\n",
    "                nearest_city = next_city\n",
    "\n",
    "        route.append(nearest_city)\n",
    "        visited[nearest_city] = True\n",
    "        current = nearest_city\n",
    "\n",
    "    # Замыкаем маршрут - возвращаемся в начальную точку\n",
    "    route.append(start_index)\n",
    "    total_distance = calculate_route_distance(route, distance_matrix)\n",
    "\n",
    "    end_time = time.time()\n",
    "    computation_time = end_time - start_time\n",
    "\n",
    "    return route, total_distance, computation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TAVzHO9CS6St",
    "outputId": "8b440b7d-9756-4065-8d10-9c184fa092b2"
   },
   "outputs": [],
   "source": [
    "greedy_route, greedy_distance, greedy_time = greedy_tsp(distance_matrix, START_WAREHOUSE)\n",
    "\n",
    "results.append({\n",
    "    'method': 'Жадный алгоритм',\n",
    "    'route': greedy_route,\n",
    "    'distance_km': greedy_distance / 1000,\n",
    "    'time_sec': greedy_time,\n",
    "    'description': 'На каждом шаге выбирает ближайший непосещенный склад'\n",
    "})\n",
    "\n",
    "print(f\"Завершено: {greedy_distance/1000:.2f} км за {greedy_time:.6f} сек\")\n",
    "print(f\"Маршрут: {greedy_route}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NIbTb8VTJwL"
   },
   "source": [
    "Алгоритм 2: 2-OPT Локальный поиск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gpLQ4i8SGD1"
   },
   "outputs": [],
   "source": [
    "def tsp_2opt(route, distance_matrix, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Алгоритм 2-opt для улучшения существующего маршрута\n",
    "\n",
    "    Алгоритм:\n",
    "    1. Берем существующий маршрут\n",
    "    2. Пытаемся улучшить его, переставляя пары ребер\n",
    "    3. Если находим улучшение, принимаем его\n",
    "    4. Повторяем пока есть улучшения или до достижения максимума итераций\n",
    "\n",
    "    Сложность: O(n^2 * max_iterations)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_route = route.copy()\n",
    "    best_distance = calculate_route_distance(route, distance_matrix)\n",
    "    improved = True\n",
    "    iterations = 0\n",
    "\n",
    "    while improved and iterations < max_iterations:\n",
    "        improved = False\n",
    "\n",
    "        # Перебираем все возможные пары индексов для перестановки\n",
    "        for i in range(1, len(route) - 2):\n",
    "            for j in range(i + 1, len(route) - 1):\n",
    "                if j - i == 1:\n",
    "                    continue\n",
    "\n",
    "                # Создаем новый маршрут с перевернутой подпоследовательностью\n",
    "                new_route = route[:i] + route[i:j+1][::-1] + route[j+1:]\n",
    "                new_distance = calculate_route_distance(new_route, distance_matrix)\n",
    "\n",
    "                # Если нашли улучшение, обновляем лучший маршрут\n",
    "                if new_distance < best_distance:\n",
    "                    best_route = new_route\n",
    "                    best_distance = new_distance\n",
    "                    improved = True\n",
    "                    route = new_route\n",
    "                    break\n",
    "\n",
    "            if improved:\n",
    "                break\n",
    "\n",
    "        iterations += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    computation_time = end_time - start_time\n",
    "\n",
    "    return best_route, best_distance, computation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6K0II7PTFWF",
    "outputId": "137c2b17-63dc-48d2-89e8-6893d3b3221e"
   },
   "outputs": [],
   "source": [
    "opt2_route, opt2_distance, opt2_time = tsp_2opt(greedy_route, distance_matrix)\n",
    "\n",
    "results.append({\n",
    "    'method': '2-OPT',\n",
    "    'route': opt2_route,\n",
    "    'distance_km': opt2_distance / 1000,\n",
    "    'time_sec': greedy_time + opt2_time,\n",
    "    'description': 'Улучшает существующий маршрут перестановкой пар ребер',\n",
    "    'improvement': f\"{(greedy_distance - opt2_distance) / greedy_distance * 100:.1f}%\"\n",
    "})\n",
    "\n",
    "print(f\"Завершено: {opt2_distance/1000:.2f} км за {opt2_time:.6f} сек\")\n",
    "print(f\"Улучшение: {results[-1]['improvement']}\")\n",
    "print(f\"Маршрут: {opt2_route}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwaz3VAUTUMa"
   },
   "source": [
    "Алгоритм 3: Метод минимального остовного дерева (MST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9q2e95HSio8"
   },
   "outputs": [],
   "source": [
    "def mst_tsp(distance_matrix, start_index=0):\n",
    "    \"\"\"\n",
    "    Метод минимального остовного дерева для приближенного решения TSP\n",
    "\n",
    "    Алгоритм:\n",
    "    1. Строим полный граф городов\n",
    "    2. Находим минимальное остовное дерево\n",
    "    3. Обходим дерево в глубину для получения маршрута\n",
    "    4. Удаляем повторяющиеся города\n",
    "\n",
    "    Сложность: O(n^2) для плотных графов\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    n = distance_matrix.shape[0]\n",
    "\n",
    "    # Строим полный граф городов\n",
    "    G = nx.Graph()\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            G.add_edge(i, j, weight=distance_matrix[i][j])\n",
    "\n",
    "    # Строим минимальное остовное дерево\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "\n",
    "    # Обход в глубину для получения маршрута\n",
    "    route_dfs = list(nx.dfs_preorder_nodes(mst, start_index))\n",
    "\n",
    "    # Убеждаемся, что посетили все города\n",
    "    visited = set(route_dfs)\n",
    "    missing = set(range(n)) - visited\n",
    "    if missing:\n",
    "        route_dfs.extend(missing)\n",
    "\n",
    "    # Замыкаем маршрут\n",
    "    route_dfs.append(start_index)\n",
    "    total_distance = calculate_route_distance(route_dfs, distance_matrix)\n",
    "\n",
    "    end_time = time.time()\n",
    "    computation_time = end_time - start_time\n",
    "\n",
    "    return route_dfs, total_distance, computation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQ0bZ_7lTQlY",
    "outputId": "6c6c1be2-6a8b-4e06-9d9d-f450cc0db701"
   },
   "outputs": [],
   "source": [
    "mst_route, mst_distance, mst_time = mst_tsp(distance_matrix, START_WAREHOUSE)\n",
    "\n",
    "results.append({\n",
    "    'method': 'MST',\n",
    "    'route': mst_route,\n",
    "    'distance_km': mst_distance / 1000,\n",
    "    'time_sec': mst_time,\n",
    "    'description': 'Строит минимальное остовное дерево и обходит его'\n",
    "})\n",
    "\n",
    "print(f\"Завершено: {mst_distance/1000:.2f} км за {mst_time:.6f} сек\")\n",
    "print(f\"Маршрут: {mst_route}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8j88HtzqTXge"
   },
   "source": [
    "Алгоритм 4: Генетический алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meH4elGpSlyP"
   },
   "outputs": [],
   "source": [
    "def genetic_algorithm_tsp(distance_matrix, start_index=0, pop_size=50, generations=100, mutation_rate=0.1):\n",
    "    \"\"\"\n",
    "    Генетический алгоритм для решения TSP\n",
    "\n",
    "    Алгоритм:\n",
    "    1. Создаем начальную популяцию случайных маршрутов\n",
    "    2. Оцениваем качество каждого маршрута (фитнес-функция)\n",
    "    3. Отбираем лучшие маршруты для размножения\n",
    "    4. Применяем кроссовер и мутацию для создания нового поколения\n",
    "    5. Повторяем заданное количество поколений\n",
    "\n",
    "    Сложность: O(pop_size * generations * n)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    n = distance_matrix.shape[0]\n",
    "\n",
    "    def initialize_population():\n",
    "        \"\"\"Создает начальную популяцию случайных маршрутов\"\"\"\n",
    "        population = []\n",
    "        for _ in range(pop_size):\n",
    "            route = list(range(n))\n",
    "            route.remove(start_index)\n",
    "            np.random.shuffle(route)\n",
    "            route = [start_index] + route + [start_index]\n",
    "            population.append(route)\n",
    "        return population\n",
    "\n",
    "    def fitness(route):\n",
    "        \"\"\"Фитнес-функция: обратная пропорция расстоянию (чем короче маршрут, тем лучше)\"\"\"\n",
    "        return 1 / calculate_route_distance(route, distance_matrix)\n",
    "\n",
    "    def order_crossover(parent1, parent2):\n",
    "        \"\"\"Оператор упорядоченного кроссовера (OX)\"\"\"\n",
    "        p1 = parent1[1:-1]  # Убираем начальную/конечную точку\n",
    "        p2 = parent2[1:-1]\n",
    "\n",
    "        size = len(p1)\n",
    "        child = [-1] * size\n",
    "\n",
    "        # Выбираем случайный сегмент из первого родителя\n",
    "        start, end = sorted(np.random.choice(range(size), 2, replace=False))\n",
    "        child[start:end+1] = p1[start:end+1]\n",
    "\n",
    "        # Заполняем оставшиеся позиции генами из второго родителя\n",
    "        pointer = (end + 1) % size\n",
    "        for gene in p2:\n",
    "            if gene not in child:\n",
    "                while child[pointer] != -1:\n",
    "                    pointer = (pointer + 1) % size\n",
    "                child[pointer] = gene\n",
    "\n",
    "        return [start_index] + child + [start_index]\n",
    "\n",
    "    def mutate(route):\n",
    "        \"\"\"Оператор мутации: перестановка двух случайных городов\"\"\"\n",
    "        if np.random.random() < mutation_rate:\n",
    "            idx1, idx2 = np.random.choice(range(1, len(route)-1), 2, replace=False)\n",
    "            route[idx1], route[idx2] = route[idx2], route[idx1]\n",
    "        return route\n",
    "\n",
    "    # Основной цикл генетического алгоритма\n",
    "    population = initialize_population()\n",
    "    best_route = None\n",
    "    best_distance = float('inf')\n",
    "\n",
    "    for generation in range(generations):\n",
    "        # Оценка качества популяции\n",
    "        fitness_scores = [fitness(route) for route in population]\n",
    "\n",
    "        # Отбор лучшей особи текущего поколения\n",
    "        current_best_idx = np.argmax(fitness_scores)\n",
    "        current_best_route = population[current_best_idx]\n",
    "        current_best_distance = calculate_route_distance(current_best_route, distance_matrix)\n",
    "\n",
    "        # Обновление глобального лучшего решения\n",
    "        if current_best_distance < best_distance:\n",
    "            best_distance = current_best_distance\n",
    "            best_route = current_best_route.copy()\n",
    "\n",
    "        # Создание нового поколения\n",
    "        new_population = [best_route.copy()]  # Элитизм: сохраняем лучшую особь\n",
    "\n",
    "        while len(new_population) < pop_size:\n",
    "            # Турнирный отбор\n",
    "            tournament_size = 3\n",
    "            tournament_indices = np.random.choice(range(pop_size), tournament_size)\n",
    "            tournament_fitness = [fitness_scores[i] for i in tournament_indices]\n",
    "            parent1_idx = tournament_indices[np.argmax(tournament_fitness)]\n",
    "            parent2_idx = tournament_indices[np.argmax(tournament_fitness)]\n",
    "\n",
    "            parent1 = population[parent1_idx]\n",
    "            parent2 = population[parent2_idx]\n",
    "\n",
    "            # Кроссовер и мутация\n",
    "            child = order_crossover(parent1, parent2)\n",
    "            child = mutate(child)\n",
    "\n",
    "            new_population.append(child)\n",
    "\n",
    "        population = new_population\n",
    "\n",
    "    end_time = time.time()\n",
    "    computation_time = end_time - start_time\n",
    "\n",
    "    return best_route, best_distance, computation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qt3A4_LpTeKY",
    "outputId": "883a4217-c5a7-45f6-a2c5-433c984158d2"
   },
   "outputs": [],
   "source": [
    "ga_runs = []\n",
    "for run in range(5):\n",
    "    print(f\"Запуск {run+1}/5...\", end=\" \")\n",
    "    ga_route, ga_distance, ga_time = genetic_algorithm_tsp(\n",
    "        distance_matrix, START_WAREHOUSE, pop_size=50, generations=100\n",
    "    )\n",
    "    ga_runs.append({\n",
    "        'route': ga_route,\n",
    "        'distance_km': ga_distance / 1000,\n",
    "        'time_sec': ga_time\n",
    "    })\n",
    "    print(f\"{ga_distance/1000:.2f} км\")\n",
    "\n",
    "# Анализируем результаты генетического алгоритма\n",
    "ga_distances = [run['distance_km'] for run in ga_runs]\n",
    "best_ga_idx = np.argmin(ga_distances)\n",
    "best_ga = ga_runs[best_ga_idx]\n",
    "\n",
    "results.append({\n",
    "    'method': 'Генетический алгоритм',\n",
    "    'route': best_ga['route'],\n",
    "    'distance_km': best_ga['distance_km'],\n",
    "    'time_sec': np.mean([run['time_sec'] for run in ga_runs]),\n",
    "    'description': 'Эволюционный поиск с кроссовером и мутацией',\n",
    "    'statistics': {\n",
    "        'best_km': best_ga['distance_km'],\n",
    "        'mean_km': np.mean(ga_distances),\n",
    "        'std_km': np.std(ga_distances),\n",
    "        'runs': len(ga_runs)\n",
    "    }\n",
    "})\n",
    "\n",
    "print(f\"Лучший результат: {best_ga['distance_km']:.2f} км\")\n",
    "print(f\"Статистика: среднее {np.mean(ga_distances):.2f} ± {np.std(ga_distances):.2f} км\")\n",
    "print(f\"Лучший маршрут: {best_ga['route']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buMgTH-bTcUX"
   },
   "source": [
    "Анализ и визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WayjbuHoUdC1"
   },
   "outputs": [],
   "source": [
    "# Создаем расширенный DataFrame с дополнительной статистикой\n",
    "results_data = []\n",
    "for r in results:\n",
    "    row_data = {\n",
    "        'Метод': r['method'],\n",
    "        'Расстояние_км': r['distance_km'],\n",
    "        'Время_сек': r['time_sec'],\n",
    "        'Описание': r['description'],\n",
    "    }\n",
    "\n",
    "    results_data.append(row_data)\n",
    "\n",
    "# Создаем DataFrame\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Сортируем по расстоянию\n",
    "results_df = results_df.sort_values('Расстояние_км')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "oPtFX52gUi4Y",
    "outputId": "1706fb7d-0c46-452c-8195-1178df2ea7d5"
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "GPHnZKKnWsND",
    "outputId": "b3ff6674-b3b9-4eb8-9c22-bd6cdebfaa86"
   },
   "outputs": [],
   "source": [
    "# Создаем графики\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# График 1: Расстояние по методам\n",
    "results_df.set_index('Метод')['Расстояние_км'].plot(\n",
    "    kind='bar', ax=axes[0], color='skyblue',\n",
    "    title='Сравнение длины маршрутов по методам',\n",
    "    ylabel='Расстояние (км)'\n",
    ")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Добавляем значения на столбцы первого графика\n",
    "for i, v in enumerate(results_df['Расстояние_км']):\n",
    "    axes[0].text(i, v + 5, f'{v:.1f} км', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# График 2: Время выполнения\n",
    "results_df.set_index('Метод')['Время_сек'].plot(\n",
    "    kind='bar', ax=axes[1], color='lightcoral',\n",
    "    title='Время выполнения алгоритмов',\n",
    "    ylabel='Время (сек)'\n",
    ")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Добавляем значения на столбцы второго графика\n",
    "for i, v in enumerate(results_df['Время_сек']):\n",
    "    axes[1].text(i, v + 0.001, f'{v:.4f} сек', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pandas_analysis_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-QJKIP5XGyD",
    "outputId": "6ac59c8c-90c7-4116-d5be-6f50797b98be"
   },
   "outputs": [],
   "source": [
    "# Детальный анализ маршрутов\n",
    "for result in results:\n",
    "    method = result['method']\n",
    "    route = result['route']\n",
    "    distance = result['distance_km']\n",
    "\n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"Общее расстояние: {distance:.2f} км\")\n",
    "    print(f\"Количество посещений: {len(route)} точек\")\n",
    "    print(f\"Порядок посещения: \", end=\"\")\n",
    "\n",
    "    # выводим порядок посещения\n",
    "    for i, point in enumerate(route[:-1]):\n",
    "        if i == 0:\n",
    "            print(f\"{warehouses_df.iloc[point]['name']}\", end=\"\")\n",
    "        else:\n",
    "            print(f\" -> {warehouses_df.iloc[point]['name']}\", end=\"\")\n",
    "    print(f\" -> {warehouses_df.iloc[route[0]]['name']} (возврат)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x60ZnUEDZJ3K"
   },
   "source": [
    "## Эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wX1ryd8_cO8Y"
   },
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wwpy9wZwaBD8",
    "outputId": "443cd250-df6f-4b31-918a-2d68521a29c9"
   },
   "outputs": [],
   "source": [
    "# Загрузка исходного графа дорожной сети\n",
    "graph = ox.load_graphml(\"moscow_region_drive_network.graphml\")\n",
    "print(\"Исходный граф дорожной сети загружен\")\n",
    "\n",
    "# Загрузка данных о складах из JSON файла\n",
    "print(\"Загружаем данные о складах из JSON файла...\")\n",
    "with open('warehouses_rc_rfc_coordinates.json', 'r', encoding='utf-8') as f:\n",
    "    warehouses_json = json.load(f)\n",
    "\n",
    "# Создаем DataFrame из JSON данных\n",
    "warehouses_data = []\n",
    "for warehouse in warehouses_json:\n",
    "    warehouses_data.append({\n",
    "        \"name\": warehouse['name'],\n",
    "        \"latitude\": warehouse['latitude'],\n",
    "        \"longitude\": warehouse['longitude']\n",
    "    })\n",
    "\n",
    "warehouses_df = pd.DataFrame(warehouses_data)\n",
    "START_WAREHOUSE = 0  # ПАВЛО_СЛОБОДСКОЕ_РЦ как начальная точка\n",
    "\n",
    "print(f\"Загружено {len(warehouses_df)} складов из JSON файла\")\n",
    "\n",
    "# Привязка складов к узлам графа\n",
    "print(\"Привязка складов к ближайшим узлам графа...\")\n",
    "warehouse_nodes = []\n",
    "for idx, warehouse in warehouses_df.iterrows():\n",
    "    point = (warehouse['latitude'], warehouse['longitude'])\n",
    "    nearest_node = ox.distance.nearest_nodes(graph, point[1], point[0])\n",
    "    warehouse_nodes.append(nearest_node)\n",
    "    print(f\"   {warehouse['name']} -> Узел {nearest_node}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "UyDHl2foaB0g",
    "outputId": "fa305299-47a1-4df3-e3ae-6a99ca1b0d12"
   },
   "outputs": [],
   "source": [
    "warehouses_df['node_id'] = warehouse_nodes\n",
    "warehouses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlU92Gktcask"
   },
   "outputs": [],
   "source": [
    "# Добавлеине шума\n",
    "def add_traffic_jams(graph, jam_roads, traffic_multiplier=10):\n",
    "    \"\"\"\n",
    "    Моделирует пробки на указанных дорогах путем умножения их длины\n",
    "\n",
    "    Принцип работы:\n",
    "    - Увеличивает длину дорог в N раз, что эквивалентно увеличению времени проезда\n",
    "    - Используется для моделирования заторов и снижения скорости движения\n",
    "    \"\"\"\n",
    "    graph_jammed = graph.copy()\n",
    "\n",
    "    print(f\"Добавляем пробки на {len(jam_roads)} дорог (умножение длины в {traffic_multiplier} раз)\")\n",
    "\n",
    "    modified_edges = 0\n",
    "    for u, v, key in jam_roads:\n",
    "        if graph_jammed.has_edge(u, v, key):\n",
    "            original_length = graph_jammed[u][v][key].get('length', 0)\n",
    "            graph_jammed[u][v][key]['length'] = original_length * traffic_multiplier\n",
    "            graph_jammed[u][v][key]['traffic_jam'] = True\n",
    "            graph_jammed[u][v][key]['original_length'] = original_length\n",
    "            modified_edges += 1\n",
    "\n",
    "    print(f\"Изменено {modified_edges} участков дорог\")\n",
    "    return graph_jammed\n",
    "\n",
    "# можно считать, как блокировка/ какой-то инцидент/ непроходимая пробка\n",
    "def block_roads(graph, blocked_roads, block_weight=1e9):\n",
    "    \"\"\"\n",
    "    Полностью перекрывает дороги, делая их непроезжими\n",
    "\n",
    "    Принцип работы:\n",
    "    - Устанавливает очень большую длину для заблокированных дорог\n",
    "    - Алгоритмы поиска пути будут избегать эти дороги\n",
    "    - Моделирует ремонтные работы, аварии или другие препятствия\n",
    "    \"\"\"\n",
    "    graph_blocked = graph.copy()\n",
    "\n",
    "    print(f\"Блокируем {len(blocked_roads)} дорог\")\n",
    "\n",
    "    blocked_count = 0\n",
    "    for u, v, key in blocked_roads:\n",
    "        if graph_blocked.has_edge(u, v, key):\n",
    "            graph_blocked[u][v][key]['length'] = block_weight\n",
    "            graph_blocked[u][v][key]['blocked'] = True\n",
    "            blocked_count += 1\n",
    "\n",
    "    print(f\"Заблокировано {blocked_count} дорог\")\n",
    "    return graph_blocked\n",
    "\n",
    "def find_important_roads(graph, road_types=['motorway', 'trunk', 'primary']):\n",
    "    \"\"\"\n",
    "    Находит важные дороги по их типу для добавления шума\n",
    "    \"\"\"\n",
    "    important_roads = []\n",
    "\n",
    "    for u, v, key, data in graph.edges(keys=True, data=True):\n",
    "        highway = data.get('highway', '')\n",
    "        if isinstance(highway, list):\n",
    "            highway = highway[0] if highway else ''\n",
    "\n",
    "        if highway in road_types:\n",
    "            important_roads.append((u, v, key))\n",
    "\n",
    "    return important_roads\n",
    "\n",
    "def find_roads_by_name(graph, name_patterns):\n",
    "    \"\"\"\n",
    "    Находит дороги по названию (например, МКАД)\n",
    "    \"\"\"\n",
    "    matched_roads = []\n",
    "\n",
    "    for u, v, key, data in graph.edges(keys=True, data=True):\n",
    "        name = data.get('name', '')\n",
    "        if isinstance(name, list):\n",
    "            name = ' '.join(name)\n",
    "\n",
    "        if any(pattern.lower() in str(name).lower() for pattern in name_patterns):\n",
    "            matched_roads.append((u, v, key))\n",
    "\n",
    "    return matched_roads\n",
    "\n",
    "def select_random_roads(roads_list, percentage):\n",
    "    \"\"\"\n",
    "    Выбирает случайные дороги из списка для добавления шума\n",
    "    \"\"\"\n",
    "    if not roads_list:\n",
    "        return []\n",
    "\n",
    "    n_to_select = max(1, int(len(roads_list) * percentage))\n",
    "    n_to_select = min(n_to_select, len(roads_list))\n",
    "\n",
    "    indices = np.random.choice(len(roads_list), size=n_to_select, replace=False)\n",
    "    selected_roads = [roads_list[i] for i in indices]\n",
    "\n",
    "    return selected_roads\n",
    "\n",
    "def visualize_modified_roads(original_graph, modified_graph, scenario_name, warehouses_df, warehouse_nodes):\n",
    "    \"\"\"\n",
    "    Визуализирует дороги, которые были изменены в сценарии\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "    # Рисуем исходную дорожную сеть серым цветом\n",
    "    ox.plot_graph(original_graph, ax=ax, node_size=0, edge_linewidth=0.5,\n",
    "                  edge_color='lightgray', bgcolor='white', show=False, close=False)\n",
    "\n",
    "    # Находим измененные дороги\n",
    "    jammed_roads = []\n",
    "    blocked_roads = []\n",
    "\n",
    "    for u, v, key, data in modified_graph.edges(keys=True, data=True):\n",
    "        if data.get('traffic_jam', False):\n",
    "            jammed_roads.append((u, v, key))\n",
    "        if data.get('blocked', False):\n",
    "            blocked_roads.append((u, v, key))\n",
    "\n",
    "    # Рисуем дороги с пробками оранжевым\n",
    "    if jammed_roads:\n",
    "        for u, v, key in jammed_roads:\n",
    "            if modified_graph.has_edge(u, v, key):\n",
    "                edge_data = modified_graph[u][v][key]\n",
    "                coords = []\n",
    "                if 'geometry' in edge_data:\n",
    "                    # Извлекаем координаты из LineString\n",
    "                    coords = list(edge_data['geometry'].coords)\n",
    "                else:\n",
    "                    # Если нет геометрии, используем координаты узлов\n",
    "                    u_data = modified_graph.nodes[u]\n",
    "                    v_data = modified_graph.nodes[v]\n",
    "                    coords = [(u_data['x'], u_data['y']), (v_data['x'], v_data['y'])]\n",
    "\n",
    "                if coords:\n",
    "                    x_vals, y_vals = zip(*coords)\n",
    "                    ax.plot(x_vals, y_vals, color='orange', linewidth=3, alpha=0.8, label='Пробки' if not jammed_roads.index((u,v,key)) else \"\")\n",
    "\n",
    "    # Рисуем заблокированные дороги красным\n",
    "    if blocked_roads:\n",
    "        for u, v, key in blocked_roads:\n",
    "            if modified_graph.has_edge(u, v, key):\n",
    "                edge_data = modified_graph[u][v][key]\n",
    "                coords = []\n",
    "                if 'geometry' in edge_data:\n",
    "                    coords = list(edge_data['geometry'].coords)\n",
    "                else:\n",
    "                    u_data = modified_graph.nodes[u]\n",
    "                    v_data = modified_graph.nodes[v]\n",
    "                    coords = [(u_data['x'], u_data['y']), (v_data['x'], v_data['y'])]\n",
    "\n",
    "                if coords:\n",
    "                    x_vals, y_vals = zip(*coords)\n",
    "                    ax.plot(x_vals, y_vals, color='red', linewidth=4, alpha=0.8, label='Блокировки' if not blocked_roads.index((u,v,key)) else \"\")\n",
    "\n",
    "    # Рисуем склады\n",
    "    for idx, warehouse in warehouses_df.iterrows():\n",
    "        color = 'blue' if idx == START_WAREHOUSE else 'green'\n",
    "        ax.scatter(warehouse['longitude'], warehouse['latitude'],\n",
    "                  c=color, s=100, alpha=0.8, edgecolors='black', zorder=5)\n",
    "        ax.annotate(warehouse['name'],\n",
    "                   (warehouse['longitude'], warehouse['latitude']),\n",
    "                   xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "    # Легенда\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='lightgray', lw=2, label='Обычные дороги'),\n",
    "        Line2D([0], [0], color='orange', lw=3, label='Дороги с пробками'),\n",
    "        Line2D([0], [0], color='red', lw=4, label='Заблокированные дороги'),\n",
    "        Line2D([0], [0], marker='o', color='blue', markersize=8, label='Начальный склад'),\n",
    "        Line2D([0], [0], marker='o', color='green', markersize=8, label='Остальные склады')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    ax.set_title(f'Измененные дороги в сценарии: {scenario_name}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'modified_roads_{scenario_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return len(jammed_roads), len(blocked_roads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QuotuT1ydfXl",
    "outputId": "d6eff617-68bd-4b27-8e7f-23348e9dea4b"
   },
   "outputs": [],
   "source": [
    "# Создание раздичных \"сценариев\" с шумом\n",
    "np.random.seed(42)  # Для воспроизводимости результатов\n",
    "\n",
    "# Находим ключевые дороги для моделирования шума\n",
    "important_roads = find_important_roads(graph)\n",
    "mkad_roads = find_roads_by_name(graph, ['МКАД', 'MKAD', 'Московская кольцевая'])\n",
    "highway_roads = find_roads_by_name(graph, [\n",
    "    'Ленинградское', 'Ярославское', 'Дмитровское', 'Рижское',\n",
    "    'Волоколамское', 'Пятницкое', 'Новорижское', 'Киевское',\n",
    "    'Минское', 'Можайское', 'Калужское', 'Варшавское'\n",
    "])\n",
    "\n",
    "print(f\"Найдено важных дорог: {len(important_roads)}\")\n",
    "print(f\"Найдено дорог МКАД: {len(mkad_roads)}\")\n",
    "print(f\"Найдено основных шоссе: {len(highway_roads)}\")\n",
    "\n",
    "# СЦЕНАРИЙ 1: Только пробки на основных дорогах\n",
    "print(\"\\n1. СЦЕНАРИЙ 1: Только пробки на основных дорогах\")\n",
    "print(\"   Моделирование: Умеренные пробки на 15% основных дорог\")\n",
    "jam_roads_scenario1 = select_random_roads(important_roads, 0.15)\n",
    "graph_traffic_jam = add_traffic_jams(graph, jam_roads_scenario1, traffic_multiplier=8)\n",
    "\n",
    "# Визуализируем измененные дороги\n",
    "jammed_count1, blocked_count1 = visualize_modified_roads(graph, graph_traffic_jam,\n",
    "                                                         \"Пробки на основных дорогах\",\n",
    "                                                         warehouses_df, warehouse_nodes)\n",
    "\n",
    "# СЦЕНАРИЙ 2: Блокировка ключевых дорог + пробки\n",
    "print(\"\\n2. СЦЕНАРИЙ 2: Блокировка ключевых дорог + пробки\")\n",
    "print(\"   Моделирование: Блокировка 8% МКАД + 4% шоссе + пробки на других дорогах\")\n",
    "\n",
    "# Используем ДРУГИЕ дороги для пробок в этом сценарии\n",
    "jam_roads_scenario2 = select_random_roads(important_roads, 0.12)\n",
    "# Исключаем дороги, которые уже использовались в первом сценарии\n",
    "jam_roads_scenario2 = [road for road in jam_roads_scenario2 if road not in jam_roads_scenario1]\n",
    "\n",
    "blocked_mkad = select_random_roads(mkad_roads, 0.08)\n",
    "blocked_highways = select_random_roads(highway_roads, 0.04)\n",
    "all_blocked = blocked_mkad + blocked_highways\n",
    "\n",
    "# Сначала блокируем дороги, потом добавляем пробки на ДРУГИХ дорогах\n",
    "graph_blocked = block_roads(graph, all_blocked)\n",
    "graph_combined = add_traffic_jams(graph_blocked, jam_roads_scenario2, traffic_multiplier=8)\n",
    "\n",
    "# Визуализируем измененные дороги\n",
    "jammed_count2, blocked_count2 = visualize_modified_roads(graph, graph_combined,\n",
    "                                                         \"Блокировки и пробки\",\n",
    "                                                         warehouses_df, warehouse_nodes)\n",
    "\n",
    "# СЦЕНАРИЙ 3: Умеренные условия\n",
    "print(\"\\n3. СЦЕНАРИЙ 3: Умеренные условия\")\n",
    "print(\"   Моделирование: Легкие пробки и минимальные блокировки\")\n",
    "\n",
    "# УМЕРЕННЫЕ УСЛОВИЯ: меньше пробок и блокировок, меньший множитель\n",
    "moderate_jam_roads = select_random_roads(important_roads, 0.08)\n",
    "moderate_blocked_mkad = select_random_roads(mkad_roads, 0.03)\n",
    "moderate_blocked_highways = select_random_roads(highway_roads, 0.015)\n",
    "all_blocked_moderate = moderate_blocked_mkad + moderate_blocked_highways\n",
    "\n",
    "graph_moderate_blocked = block_roads(graph, all_blocked_moderate)\n",
    "graph_moderate = add_traffic_jams(graph_moderate_blocked, moderate_jam_roads, traffic_multiplier=4)\n",
    "\n",
    "# Визуализируем измененные дороги\n",
    "jammed_count3, blocked_count3 = visualize_modified_roads(graph, graph_moderate,\n",
    "                                                         \"Умеренные условия\",\n",
    "                                                         warehouses_df, warehouse_nodes)\n",
    "\n",
    "print(\"\\nСтатистиа измененных дорог по сценариям:\")\n",
    "print(f\"Сценарий 1 (Пробки): {jammed_count1} дорог с пробками, {blocked_count1} заблокированных\")\n",
    "print(f\"Сценарий 2 (Блокировки+Пробки): {jammed_count2} дорог с пробками, {blocked_count2} заблокированных\")\n",
    "print(f\"Сценарий 3 (Умеренные): {jammed_count3} дорог с пробками, {blocked_count3} заблокированных\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQBk3hGCZ9v6",
    "outputId": "734dc483-e63a-484e-e2d5-b19374ec6ad6"
   },
   "outputs": [],
   "source": [
    "# Проверяем, что склады достижимы и не перекрыты\n",
    "def check_warehouses_accessibility(graph, warehouses_df, warehouse_nodes):\n",
    "    \"\"\"\n",
    "    Проверяет, что все склады достижимы в графе (существует путь между всеми парами)\n",
    "    \"\"\"\n",
    "    n = len(warehouse_nodes)\n",
    "    unreachable_pairs = []\n",
    "\n",
    "    # Проверяем что все узлы существуют в графе\n",
    "    missing_nodes = []\n",
    "    for node in warehouse_nodes:\n",
    "        if node not in graph.nodes:\n",
    "            missing_nodes.append(node)\n",
    "\n",
    "    if missing_nodes:\n",
    "        print(f\"В графе отсутствуют узлы: {missing_nodes}\")\n",
    "        return False\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            try:\n",
    "                nx.shortest_path_length(graph, warehouse_nodes[i], warehouse_nodes[j], weight='length')\n",
    "            except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "                unreachable_pairs.append((i, j, warehouses_df.iloc[i]['name'], warehouses_df.iloc[j]['name']))\n",
    "\n",
    "    if unreachable_pairs:\n",
    "        print(f\"Найдено {len(unreachable_pairs)} недостижимых пар:\")\n",
    "        for i, j, name1, name2 in unreachable_pairs[:3]:\n",
    "            print(f\"   {name1} <-> {name2}\")\n",
    "        if len(unreachable_pairs) > 3:\n",
    "            print(f\"   ... и еще {len(unreachable_pairs) - 3} пар\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"Все склады достижимы!\")\n",
    "        return True\n",
    "\n",
    "# Создаем словарь сценариев для тестирования\n",
    "scenarios = {\n",
    "    \"Без шума\": graph,\n",
    "    \"Пробки\": graph_traffic_jam,\n",
    "    \"Блокировки+Пробки\": graph_combined,\n",
    "    \"Умеренные условия\": graph_moderate\n",
    "}\n",
    "\n",
    "# Проверяем доступность для всех сценариев\n",
    "accessibility_results = {}\n",
    "print(\"Проверка доступности складов по сценариям:\")\n",
    "for scenario_name, scenario_graph in scenarios.items():\n",
    "    print(f\"\\n{scenario_name}:\")\n",
    "    accessible = check_warehouses_accessibility(scenario_graph, warehouses_df, warehouse_nodes)\n",
    "    accessibility_results[scenario_name] = accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uo_4uQGKPtI4"
   },
   "outputs": [],
   "source": [
    "# Пересчёт матрицы расстояний для всех сценариев\n",
    "def calculate_distance_matrix_for_scenario(graph, warehouse_nodes, scenario_name):\n",
    "    \"\"\"\n",
    "    Вычисляет матрицу расстояний между складами для заданного сценария\n",
    "    Использует алгоритм Дейкстры для нахождения кратчайших путей\n",
    "    \"\"\"\n",
    "    print(f\"Вычисляем матрицу расстояний для '{scenario_name}'...\")\n",
    "\n",
    "    n = len(warehouse_nodes)\n",
    "    distance_matrix = np.zeros((n, n))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            try:\n",
    "                # Проверяем что узлы существуют в графе\n",
    "                if warehouse_nodes[i] not in graph.nodes or warehouse_nodes[j] not in graph.nodes:\n",
    "                    distance_matrix[i][j] = 1e9\n",
    "                    distance_matrix[j][i] = 1e9\n",
    "                    continue\n",
    "\n",
    "                # Вычисляем кратчайший путь по дорожной сети\n",
    "                path_length = nx.shortest_path_length(graph, warehouse_nodes[i], warehouse_nodes[j], weight='length')\n",
    "                distance_matrix[i][j] = path_length\n",
    "                distance_matrix[j][i] = path_length\n",
    "\n",
    "            except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "                # Если пути нет, ставим большое число\n",
    "                distance_matrix[i][j] = 1e9\n",
    "                distance_matrix[j][i] = 1e9\n",
    "\n",
    "    end_time = time.time()\n",
    "    computation_time = end_time - start_time\n",
    "\n",
    "    print(f\"   '{scenario_name}': {computation_time:.2f} сек\")\n",
    "\n",
    "    return distance_matrix, computation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ghq1LgJOd9Jx",
    "outputId": "0a53f763-b254-420b-8392-8531cd2a60d7"
   },
   "outputs": [],
   "source": [
    "# Вычисляем матрицы для всех сценариев\n",
    "distance_matrices = {}\n",
    "computation_times = {}\n",
    "\n",
    "for scenario_name, scenario_graph in scenarios.items():\n",
    "    if accessibility_results[scenario_name]:\n",
    "        dist_matrix, comp_time = calculate_distance_matrix_for_scenario(\n",
    "            scenario_graph, warehouse_nodes, scenario_name\n",
    "        )\n",
    "        distance_matrices[scenario_name] = dist_matrix\n",
    "        computation_times[scenario_name] = comp_time\n",
    "    else:\n",
    "        print(f\"   Пропускаем '{scenario_name}' - склады недостижимы\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6do1REMdbMdD"
   },
   "outputs": [],
   "source": [
    "# Вспомогательные функции для алгоритмов\n",
    "def calculate_route_distance(route, distance_matrix):\n",
    "    \"\"\"Вычисляет общее расстояние маршрута\"\"\"\n",
    "    total_distance = 0\n",
    "    for i in range(len(route) - 1):\n",
    "        from_node = route[i]\n",
    "        to_node = route[i+1]\n",
    "        total_distance += distance_matrix[from_node][to_node]\n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0tc7rqQFbXow",
    "outputId": "7edf1ad5-ce5b-4e6a-b514-39875a4efb87"
   },
   "outputs": [],
   "source": [
    "# Тестируем все алгоритмы на всех сценариях\n",
    "\n",
    "all_algorithm_results = []\n",
    "\n",
    "for scenario_name, dist_matrix in distance_matrices.items():\n",
    "    print(f\"\\nСЦЕНАРИЙ: {scenario_name}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    scenario_results = []\n",
    "\n",
    "    # 1. Жадный алгорит\n",
    "    print(\"1. Жадный алгоритм\")\n",
    "    greedy_route, greedy_distance, greedy_time = greedy_tsp(dist_matrix, START_WAREHOUSE)\n",
    "    scenario_results.append({\n",
    "        'algorithm': 'Жадный',\n",
    "        'route': greedy_route,\n",
    "        'distance_km': greedy_distance / 1000,\n",
    "        'time_sec': greedy_time,\n",
    "        'scenario': scenario_name\n",
    "    })\n",
    "    print(f\"   Результат: {greedy_distance/1000:.2f} км за {greedy_time:.6f} сек\")\n",
    "\n",
    "    # 2. 2-OPT (улучшение жадного)\n",
    "    print(\"2. 2-OPT\")\n",
    "    opt2_route, opt2_distance, opt2_time = tsp_2opt(greedy_route, dist_matrix)\n",
    "    scenario_results.append({\n",
    "        'algorithm': '2-OPT',\n",
    "        'route': opt2_route,\n",
    "        'distance_km': opt2_distance / 1000,\n",
    "        'time_sec': opt2_time,\n",
    "        'scenario': scenario_name,\n",
    "        'improvement': f\"{(greedy_distance - opt2_distance) / greedy_distance * 100:.1f}%\" if greedy_distance > 0 else \"0.0%\"\n",
    "    })\n",
    "    print(f\"   Результат: {opt2_distance/1000:.2f} км за {opt2_time:.6f} сек\")\n",
    "    if opt2_distance < greedy_distance:\n",
    "        print(f\"   Улучшение: {scenario_results[-1]['improvement']}\")\n",
    "\n",
    "    # 3. MST\n",
    "    print(\"3. MST\")\n",
    "    mst_route, mst_distance, mst_time = mst_tsp(dist_matrix, START_WAREHOUSE)\n",
    "    scenario_results.append({\n",
    "        'algorithm': 'MST',\n",
    "        'route': mst_route,\n",
    "        'distance_km': mst_distance / 1000,\n",
    "        'time_sec': mst_time,\n",
    "        'scenario': scenario_name\n",
    "    })\n",
    "    print(f\"   Результат: {mst_distance/1000:.2f} км за {mst_time:.6f} сек\")\n",
    "\n",
    "    # 4. Генетический алгоритм (3 запуска)\n",
    "    print(\"4. Генетический алгоритм (3 запуска)\")\n",
    "    ga_runs = []\n",
    "    for run in range(3):\n",
    "        print(f\"   Запуск {run+1}/3\", end=\" \")\n",
    "        ga_route, ga_distance, ga_time = genetic_algorithm_tsp(\n",
    "            dist_matrix, START_WAREHOUSE, pop_size=30, generations=50\n",
    "        )\n",
    "        ga_runs.append({\n",
    "            'route': ga_route,\n",
    "            'distance_km': ga_distance / 1000,\n",
    "            'time_sec': ga_time\n",
    "        })\n",
    "        print(f\"{ga_distance/1000:.2f} км\")\n",
    "\n",
    "    # Анализируем результаты генетического алгоритма\n",
    "    ga_distances = [run['distance_km'] for run in ga_runs]\n",
    "    best_ga_idx = np.argmin(ga_distances)\n",
    "    best_ga = ga_runs[best_ga_idx]\n",
    "\n",
    "    scenario_results.append({\n",
    "        'algorithm': 'Генетический',\n",
    "        'route': best_ga['route'],\n",
    "        'distance_km': best_ga['distance_km'],\n",
    "        'time_sec': np.mean([run['time_sec'] for run in ga_runs]),\n",
    "        'scenario': scenario_name,\n",
    "        'statistics': {\n",
    "            'best_km': best_ga['distance_km'],\n",
    "            'mean_km': np.mean(ga_distances),\n",
    "            'std_km': np.std(ga_distances),\n",
    "            'runs': len(ga_runs)\n",
    "        }\n",
    "    })\n",
    "    print(f\"   Лучший результат: {best_ga['distance_km']:.2f} км\")\n",
    "    print(f\"   Статистика: среднее {np.mean(ga_distances):.2f} ± {np.std(ga_distances):.2f} км\")\n",
    "\n",
    "    all_algorithm_results.extend(scenario_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "3a8w0KKLgsHu",
    "outputId": "cb548b6f-1211-4d5a-fc2c-8fb00da4189a"
   },
   "outputs": [],
   "source": [
    "# Анализ\n",
    "\n",
    "# Создаем DataFrame для анализа\n",
    "results_df = pd.DataFrame(all_algorithm_results)\n",
    "\n",
    "# Группируем по алгоритмам и сценариям\n",
    "algorithm_performance = results_df.groupby(['algorithm', 'scenario']).agg({\n",
    "    'distance_km': 'mean',\n",
    "    'time_sec': 'mean'\n",
    "}).round(5)\n",
    "\n",
    "algorithm_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "id": "cNxM8WE8bs7e",
    "outputId": "8e847d81-db14-47f9-a7df-cfdd53aaabcf"
   },
   "outputs": [],
   "source": [
    "# Визуализация результатов\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Сравнение алгоритмов по сценариям (расстояния)\n",
    "scenarios_list = list(distance_matrices.keys())\n",
    "algorithms = ['Жадный', '2-OPT', 'MST', 'Генетический']\n",
    "\n",
    "# Подготовка данных для группированного графика\n",
    "x = np.arange(len(scenarios_list))\n",
    "width = 0.2\n",
    "\n",
    "for i, algorithm in enumerate(algorithms):\n",
    "    algorithm_distances = []\n",
    "    for scenario in scenarios_list:\n",
    "        result = results_df[(results_df['algorithm'] == algorithm) & (results_df['scenario'] == scenario)]\n",
    "        if not result.empty:\n",
    "            algorithm_distances.append(result['distance_km'].values[0])\n",
    "        else:\n",
    "            algorithm_distances.append(0)\n",
    "\n",
    "    ax1.bar(x + i*width, algorithm_distances, width, label=algorithm, alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Сценарии')\n",
    "ax1.set_ylabel('Расстояние (км)')\n",
    "ax1.set_title('Сравнение алгоритмов по сценариям\\n(Длина маршрута)')\n",
    "ax1.set_xticks(x + width*1.5)\n",
    "ax1.set_xticklabels(scenarios_list, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Время выполнения по алгоритмам\n",
    "algorithm_times = results_df.groupby('algorithm')['time_sec'].mean()\n",
    "bars2 = ax2.bar(algorithm_times.index, algorithm_times.values, color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99'])\n",
    "ax2.set_ylabel('Среднее время (сек)')\n",
    "ax2.set_title('Среднее время выполнения алгоритмов')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, time_val in zip(bars2, algorithm_times.values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "            f'{time_val:.4f} сек', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Увеличение расстояния по сценариям (относительно базового)\n",
    "base_scenario = \"Без шума\"\n",
    "base_results = results_df[results_df['scenario'] == base_scenario]\n",
    "\n",
    "increase_data = []\n",
    "for scenario in scenarios_list[1:]:  # Исключаем базовый сценарий\n",
    "    for algorithm in algorithms:\n",
    "        base_result = base_results[base_results['algorithm'] == algorithm]\n",
    "        current_result = results_df[(results_df['algorithm'] == algorithm) & (results_df['scenario'] == scenario)]\n",
    "\n",
    "        if not base_result.empty and not current_result.empty:\n",
    "            base_distance = base_result['distance_km'].values[0]\n",
    "            current_distance = current_result['distance_km'].values[0]\n",
    "            increase = ((current_distance - base_distance) / base_distance) * 100\n",
    "            increase_data.append({\n",
    "                'scenario': scenario,\n",
    "                'algorithm': algorithm,\n",
    "                'increase_percent': increase\n",
    "            })\n",
    "\n",
    "if increase_data:\n",
    "    increase_df = pd.DataFrame(increase_data)\n",
    "\n",
    "    # Группированный график увеличения\n",
    "    scenarios_increase = increase_df['scenario'].unique()\n",
    "    x_increase = np.arange(len(scenarios_increase))\n",
    "\n",
    "    for i, algorithm in enumerate(algorithms):\n",
    "        algorithm_increases = []\n",
    "        for scenario in scenarios_increase:\n",
    "            result = increase_df[(increase_df['algorithm'] == algorithm) & (increase_df['scenario'] == scenario)]\n",
    "            if not result.empty:\n",
    "                algorithm_increases.append(result['increase_percent'].values[0])\n",
    "            else:\n",
    "                algorithm_increases.append(0)\n",
    "\n",
    "        ax3.bar(x_increase + i*width, algorithm_increases, width, label=algorithm, alpha=0.8)\n",
    "\n",
    "    ax3.set_xlabel('Сценарии')\n",
    "    ax3.set_ylabel('Увеличение расстояния (%)')\n",
    "    ax3.set_title('Увеличение длины маршрута по сценариям\\n(Относительно базового сценария)')\n",
    "    ax3.set_xticks(x_increase + width*1.5)\n",
    "    ax3.set_xticklabels(scenarios_increase, rotation=45)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Стабильность алгоритмов (стандартное отклонение по сценариям)\n",
    "stability_data = []\n",
    "for algorithm in algorithms:\n",
    "    algorithm_results = results_df[results_df['algorithm'] == algorithm]\n",
    "    if len(algorithm_results) > 0:\n",
    "        mean_distance = algorithm_results['distance_km'].mean()\n",
    "        std_distance = algorithm_results['distance_km'].std()\n",
    "        cv_percent = (std_distance / mean_distance) * 100 if mean_distance > 0 else 0\n",
    "        stability_data.append({\n",
    "            'algorithm': algorithm,\n",
    "            'mean_km': mean_distance,\n",
    "            'std_km': std_distance,\n",
    "            'cv_percent': cv_percent\n",
    "        })\n",
    "\n",
    "if stability_data:\n",
    "    stability_df = pd.DataFrame(stability_data)\n",
    "    bars4 = ax4.bar(stability_df['algorithm'], stability_df['cv_percent'],\n",
    "                    color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99'], alpha=0.7)\n",
    "    ax4.set_ylabel('Коэффициент вариации (%)')\n",
    "    ax4.set_title('Стабильность алгоритмов\\n(Меньше = стабильнее)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar, cv in zip(bars4, stability_df['cv_percent']):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{cv:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_algorithms_noise_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Визуализация сохранена: 'all_algorithms_noise_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmwroPS0xYKO"
   },
   "source": [
    "## Большее количество складов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_SmKx_rxfR9"
   },
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53zmPO6qx1A5",
    "outputId": "dfc19e02-583c-477c-fe27-11e25cb35334"
   },
   "outputs": [],
   "source": [
    "# Загрузка всех складов\n",
    "# Загрузка дорожной сети\n",
    "graph = ox.load_graphml(\"moscow_region_drive_network.graphml\")\n",
    "print(\"Дорожная сеть загружена\")\n",
    "\n",
    "# Загрузка всех складов из JSON файла\n",
    "print(\"Загружаем ВСЕ склады из all_warehouses_coordinates.json...\")\n",
    "with open('all_warehouses_coordinates.json', 'r', encoding='utf-8') as f:\n",
    "    all_warehouses_json = json.load(f)\n",
    "\n",
    "# Создаем DataFrame со всеми складами\n",
    "all_warehouses_data = []\n",
    "for warehouse in all_warehouses_json:\n",
    "    all_warehouses_data.append({\n",
    "        \"name\": warehouse['name'],\n",
    "        \"latitude\": warehouse['latitude'],\n",
    "        \"longitude\": warehouse['longitude'],\n",
    "        \"type\": \"РЦ/РФЦ\" if \"РЦ\" in warehouse['name'] or \"РФЦ\" in warehouse['name'] else \"СЦ\"\n",
    "    })\n",
    "\n",
    "all_warehouses_df = pd.DataFrame(all_warehouses_data)\n",
    "START_WAREHOUSE = 0  # ПАВЛО_СЛОБОДСКОЕ_РЦ как начальная точка\n",
    "\n",
    "print(f\"Загружено всего складов: {len(all_warehouses_df)}\")\n",
    "print(f\"Из них:\")\n",
    "print(f\"Начальная точка: {all_warehouses_df.iloc[START_WAREHOUSE]['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWy7s9sT1qzo",
    "outputId": "a0f84b51-110c-4a06-8aa6-b5eaadf872a5"
   },
   "outputs": [],
   "source": [
    "# Привязка всех складов к узлам графа\n",
    "all_warehouse_nodes = []\n",
    "connection_distances = []\n",
    "\n",
    "for idx, warehouse in all_warehouses_df.iterrows():\n",
    "    point = (warehouse['latitude'], warehouse['longitude'])\n",
    "    nearest_node = ox.distance.nearest_nodes(graph, point[1], point[0])\n",
    "    all_warehouse_nodes.append(nearest_node)\n",
    "\n",
    "    node_data = graph.nodes[nearest_node]\n",
    "    node_point = (node_data['y'], node_data['x'])\n",
    "    distance = calculate_geodesic_distance(point[0], point[1], node_point[0], node_point[1])\n",
    "    connection_distances.append(distance)\n",
    "\n",
    "    if idx < 5:  # Показываем только первые 5 для примера\n",
    "        print(f\"   {warehouse['name']:30} -> Узел {nearest_node} ({distance:.1f} м)\")\n",
    "\n",
    "all_warehouses_df['node_id'] = all_warehouse_nodes\n",
    "all_warehouses_df['distance_to_node'] = connection_distances\n",
    "\n",
    "# Анализ качества подключения всех складов\n",
    "print(f\"\\nАнализ качества подключения {len(all_warehouses_df)} складов:\")\n",
    "good_connections = sum(1 for d in connection_distances if d <= 200)\n",
    "medium_connections = sum(1 for d in connection_distances if 200 < d <= 500)\n",
    "poor_connections = sum(1 for d in connection_distances if d > 500)\n",
    "\n",
    "print(f\"  - Хорошие подключения (≤200 м): {good_connections} ({good_connections/len(connection_distances)*100:.1f}%)\")\n",
    "print(f\"  - Средние подключения (200-500 м): {medium_connections} ({medium_connections/len(connection_distances)*100:.1f}%)\")\n",
    "print(f\"  - Плохие подключения (>500 м): {poor_connections} ({poor_connections/len(connection_distances)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eHWcgoDv3zvO",
    "outputId": "436fe083-c9e4-4ae8-ec45-0c7bb11259a8"
   },
   "outputs": [],
   "source": [
    "distance_matrices = {}\n",
    "computation_times = {}\n",
    "accessibility_results = {}\n",
    "\n",
    "print(\"Проверка доступности и создание матриц расстояний для всех сценариев:\")\n",
    "for scenario_name, scenario_graph in scenarios.items():\n",
    "    print(f\"\\n{scenario_name}:\")\n",
    "    accessible = check_warehouses_accessibility(scenario_graph, all_warehouses_df, all_warehouse_nodes)\n",
    "    accessibility_results[scenario_name] = accessible\n",
    "\n",
    "    if accessible:\n",
    "        dist_matrix, comp_time = calculate_distance_matrix_for_scenario(\n",
    "            scenario_graph, all_warehouse_nodes, scenario_name\n",
    "        )\n",
    "        distance_matrices[scenario_name] = dist_matrix\n",
    "        computation_times[scenario_name] = comp_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CsyapDQN4EnT",
    "outputId": "635211c0-82d6-4c66-96c9-70b82035677e"
   },
   "outputs": [],
   "source": [
    "# запуск всех алгоритмов на всех сценариях\n",
    "all_results = []\n",
    "\n",
    "for scenario_name, dist_matrix in distance_matrices.items():\n",
    "    print(f\"\\nСЦЕНАРИЙ: {scenario_name}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    scenario_results = []\n",
    "\n",
    "    # 1. Жадный алгоритм\n",
    "    print(\"1. Жадный алгоритм\")\n",
    "    greedy_route, greedy_distance, greedy_time = greedy_tsp(dist_matrix, START_WAREHOUSE)\n",
    "    scenario_results.append({\n",
    "        'algorithm': 'Жадный',\n",
    "        'route': greedy_route,\n",
    "        'distance_km': greedy_distance / 1000,\n",
    "        'time_sec': greedy_time,\n",
    "        'scenario': scenario_name\n",
    "    })\n",
    "    print(f\"   Результат: {greedy_distance/1000:.2f} км за {greedy_time:.3f} сек\")\n",
    "\n",
    "    # 2. 2-OPT (улучшение жадного)\n",
    "    print(\"2. 2-OPT\")\n",
    "    opt2_route, opt2_distance, opt2_time = tsp_2opt(greedy_route, dist_matrix, max_iterations=500)\n",
    "    scenario_results.append({\n",
    "        'algorithm': '2-OPT',\n",
    "        'route': opt2_route,\n",
    "        'distance_km': opt2_distance / 1000,\n",
    "        'time_sec': opt2_time,\n",
    "        'scenario': scenario_name,\n",
    "        'improvement': f\"{(greedy_distance - opt2_distance) / greedy_distance * 100:.1f}%\" if greedy_distance > 0 else \"0.0%\"\n",
    "    })\n",
    "    print(f\"   Результат: {opt2_distance/1000:.2f} км за {opt2_time:.3f} сек\")\n",
    "    if opt2_distance < greedy_distance:\n",
    "        print(f\"   Улучшение: {scenario_results[-1]['improvement']}\")\n",
    "\n",
    "    # 3. MST\n",
    "    print(\"3. MST\")\n",
    "    mst_route, mst_distance, mst_time = mst_tsp(dist_matrix, START_WAREHOUSE)\n",
    "    scenario_results.append({\n",
    "        'algorithm': 'MST',\n",
    "        'route': mst_route,\n",
    "        'distance_km': mst_distance / 1000,\n",
    "        'time_sec': mst_time,\n",
    "        'scenario': scenario_name\n",
    "    })\n",
    "    print(f\"   Результат: {mst_distance/1000:.2f} км за {mst_time:.3f} сек\")\n",
    "\n",
    "    # 4. Генетический алгоритм (2 запуска для скорости)\n",
    "    print(\"4. Генетический алгоритм\")\n",
    "    print(\"   Запускаем 2 раза для надежности\")\n",
    "    ga_runs = []\n",
    "    for run in range(2):\n",
    "        print(f\"   Запуск {run+1}/2\", end=\" \")\n",
    "        ga_route, ga_distance, ga_time = genetic_algorithm_tsp(\n",
    "            dist_matrix, START_WAREHOUSE, pop_size=30, generations=50\n",
    "        )\n",
    "        ga_runs.append({\n",
    "            'route': ga_route,\n",
    "            'distance_km': ga_distance / 1000,\n",
    "            'time_sec': ga_time\n",
    "        })\n",
    "        print(f\"{ga_distance/1000:.2f} км\")\n",
    "\n",
    "    # Анализируем результаты генетического алгоритма\n",
    "    ga_distances = [run['distance_km'] for run in ga_runs]\n",
    "    best_ga_idx = np.argmin(ga_distances)\n",
    "    best_ga = ga_runs[best_ga_idx]\n",
    "\n",
    "    scenario_results.append({\n",
    "        'algorithm': 'Генетический',\n",
    "        'route': best_ga['route'],\n",
    "        'distance_km': best_ga['distance_km'],\n",
    "        'time_sec': np.mean([run['time_sec'] for run in ga_runs]),\n",
    "        'scenario': scenario_name,\n",
    "        'statistics': {\n",
    "            'best_km': best_ga['distance_km'],\n",
    "            'mean_km': np.mean(ga_distances),\n",
    "            'std_km': np.std(ga_distances),\n",
    "            'runs': len(ga_runs)\n",
    "        }\n",
    "    })\n",
    "    print(f\"   Лучший результат: {best_ga['distance_km']:.2f} км\")\n",
    "    print(f\"   Статистика: среднее {np.mean(ga_distances):.2f} ± {np.std(ga_distances):.2f} км\")\n",
    "\n",
    "    all_results.extend(scenario_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 828
    },
    "id": "yiWAP9Yj6024",
    "outputId": "a615ca66-0150-436f-ee59-577a10459360"
   },
   "outputs": [],
   "source": [
    "# Анализ результатов\n",
    "\n",
    "# Создаем DataFrame для анализа\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCVqrfFM6um_",
    "outputId": "5b592330-a028-497f-a11f-5a2432597635"
   },
   "outputs": [],
   "source": [
    "print(f\"Всего тестов: {len(results_df)}\")\n",
    "print(f\"Количество сценариев: {len(results_df['scenario'].unique())}\")\n",
    "print(f\"Количество алгоритмов: {len(results_df['algorithm'].unique())}\")\n",
    "\n",
    "# Сводная таблица по алгоритмам и сценариям\n",
    "pivot_distance = results_df.pivot_table(\n",
    "    index='scenario',\n",
    "    columns='algorithm',\n",
    "    values='distance_km',\n",
    "    aggfunc='mean'\n",
    ").round(2)\n",
    "\n",
    "pivot_time = results_df.pivot_table(\n",
    "    index='scenario',\n",
    "    columns='algorithm',\n",
    "    values='time_sec',\n",
    "    aggfunc='mean'\n",
    ").round(5)\n",
    "\n",
    "print(\"\\nСРЕДНЯЯ ДЛИНА МАРШРУТА ПО СЦЕНАРИЯМ (км):\")\n",
    "print(pivot_distance)\n",
    "\n",
    "print(\"\\nСРЕДНЕЕ ВРЕМЯ ВЫПОЛНЕНИЯ ПО СЦЕНАРИЯМ (сек):\")\n",
    "print(pivot_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "id": "Wh98hyDd7fCe",
    "outputId": "c0ac7d6a-2388-4c94-9104-b441f13d6df7"
   },
   "outputs": [],
   "source": [
    "# Создаем комплексные графики\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# График 1: Сравнение алгоритмов по сценариям (расстояния)\n",
    "scenarios_list = list(distance_matrices.keys())\n",
    "algorithms = ['Жадный', '2-OPT', 'MST', 'Генетический']\n",
    "\n",
    "# Подготовка данных для группированного графика\n",
    "x = np.arange(len(scenarios_list))\n",
    "width = 0.2\n",
    "\n",
    "for i, algorithm in enumerate(algorithms):\n",
    "    algorithm_distances = []\n",
    "    for scenario in scenarios_list:\n",
    "        result = results_df[(results_df['algorithm'] == algorithm) & (results_df['scenario'] == scenario)]\n",
    "        if not result.empty:\n",
    "            algorithm_distances.append(result['distance_km'].values[0])\n",
    "        else:\n",
    "            algorithm_distances.append(0)\n",
    "\n",
    "    ax1.bar(x + i*width, algorithm_distances, width, label=algorithm, alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Сценарии')\n",
    "ax1.set_ylabel('Расстояние (км)')\n",
    "ax1.set_title('Сравнение алгоритмов по сценариям\\n(Длина маршрута)', fontweight='bold')\n",
    "ax1.set_xticks(x + width*1.5)\n",
    "ax1.set_xticklabels(scenarios_list, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Добавляем значения на столбцы\n",
    "for i, scenario in enumerate(scenarios_list):\n",
    "    for j, algorithm in enumerate(algorithms):\n",
    "        result = results_df[(results_df['algorithm'] == algorithm) & (results_df['scenario'] == scenario)]\n",
    "        if not result.empty:\n",
    "            height = result['distance_km'].values[0]\n",
    "            ax1.text(x[i] + j*width, height + 5, f'{height:.1f}',\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# График 2: Время выполнения по алгоритмам\n",
    "algorithm_times = results_df.groupby('algorithm')['time_sec'].mean()\n",
    "bars2 = ax2.bar(algorithm_times.index, algorithm_times.values,\n",
    "                color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99'], alpha=0.8)\n",
    "ax2.set_ylabel('Среднее время (сек)')\n",
    "ax2.set_title('Среднее время выполнения алгоритмов\\n(Все сценарии)', fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, time_val in zip(bars2, algorithm_times.values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{time_val:.3f} сек', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# График 3: Увеличение расстояния по сценариям (относительно базового)\n",
    "base_scenario = \"Без шума\"\n",
    "base_results = results_df[results_df['scenario'] == base_scenario]\n",
    "\n",
    "increase_data = []\n",
    "for scenario in scenarios_list[1:]:  # Исключаем базовый сценарий\n",
    "    for algorithm in algorithms:\n",
    "        base_result = base_results[base_results['algorithm'] == algorithm]\n",
    "        current_result = results_df[(results_df['algorithm'] == algorithm) & (results_df['scenario'] == scenario)]\n",
    "\n",
    "        if not base_result.empty and not current_result.empty:\n",
    "            base_distance = base_result['distance_km'].values[0]\n",
    "            current_distance = current_result['distance_km'].values[0]\n",
    "            increase = ((current_distance - base_distance) / base_distance) * 100\n",
    "            increase_data.append({\n",
    "                'scenario': scenario,\n",
    "                'algorithm': algorithm,\n",
    "                'increase_percent': increase\n",
    "            })\n",
    "\n",
    "if increase_data:\n",
    "    increase_df = pd.DataFrame(increase_data)\n",
    "\n",
    "    # Группированный график увеличения\n",
    "    scenarios_increase = increase_df['scenario'].unique()\n",
    "    x_increase = np.arange(len(scenarios_increase))\n",
    "\n",
    "    for i, algorithm in enumerate(algorithms):\n",
    "        algorithm_increases = []\n",
    "        for scenario in scenarios_increase:\n",
    "            result = increase_df[(increase_df['algorithm'] == algorithm) & (increase_df['scenario'] == scenario)]\n",
    "            if not result.empty:\n",
    "                algorithm_increases.append(result['increase_percent'].values[0])\n",
    "            else:\n",
    "                algorithm_increases.append(0)\n",
    "\n",
    "        ax3.bar(x_increase + i*width, algorithm_increases, width, label=algorithm, alpha=0.8)\n",
    "\n",
    "    ax3.set_xlabel('Сценарии')\n",
    "    ax3.set_ylabel('Увеличение расстояния (%)')\n",
    "    ax3.set_title('Увеличение длины маршрута по сценариям\\n(Относительно базового сценария)', fontweight='bold')\n",
    "    ax3.set_xticks(x_increase + width*1.5)\n",
    "    ax3.set_xticklabels(scenarios_increase, rotation=45)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# График 4: Стабильность алгоритмов (стандартное отклонение по сценариям)\n",
    "stability_data = []\n",
    "for algorithm in algorithms:\n",
    "    algorithm_results = results_df[results_df['algorithm'] == algorithm]\n",
    "    if len(algorithm_results) > 0:\n",
    "        mean_distance = algorithm_results['distance_km'].mean()\n",
    "        std_distance = algorithm_results['distance_km'].std()\n",
    "        cv_percent = (std_distance / mean_distance) * 100 if mean_distance > 0 else 0\n",
    "        stability_data.append({\n",
    "            'algorithm': algorithm,\n",
    "            'mean_km': mean_distance,\n",
    "            'std_km': std_distance,\n",
    "            'cv_percent': cv_percent\n",
    "        })\n",
    "\n",
    "if stability_data:\n",
    "    stability_df = pd.DataFrame(stability_data)\n",
    "    bars4 = ax4.bar(stability_df['algorithm'], stability_df['cv_percent'],\n",
    "                    color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99'], alpha=0.7)\n",
    "    ax4.set_ylabel('Коэффициент вариации (%)')\n",
    "    ax4.set_title('Стабильность алгоритмов\\n(Меньше = стабильнее)', fontweight='bold')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar, cv in zip(bars4, stability_df['cv_percent']):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{cv:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comprehensive_algorithms_comparison_all_scenarios.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Комплексная визуализация сохранена: 'comprehensive_algorithms_comparison_all_scenarios.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7YpY-hKLS3B"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Создаем директорию для сохранения графиков\n",
    "os.makedirs('road_route_visualizations', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Z0zHaZ8LarU"
   },
   "outputs": [],
   "source": [
    "def get_path_between_warehouses(graph, warehouse1_node, warehouse2_node):\n",
    "    \"\"\"\n",
    "    Находит кратчайший путь по дорожной сети между двумя складами\n",
    "    Возвращает список координат (lat, lon) пути\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Находим кратчайший путь по узлам графа\n",
    "        path_nodes = nx.shortest_path(graph, warehouse1_node, warehouse2_node, weight='length')\n",
    "\n",
    "        # Собираем координаты всех узлов пути\n",
    "        path_coords = []\n",
    "        for node in path_nodes:\n",
    "            node_data = graph.nodes[node]\n",
    "            path_coords.append((node_data['y'], node_data['x']))  # (lat, lon)\n",
    "\n",
    "        return path_coords\n",
    "    except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "        # Если пути нет, возвращаем прямую линию\n",
    "        node1_data = graph.nodes[warehouse1_node]\n",
    "        node2_data = graph.nodes[warehouse2_node]\n",
    "        return [\n",
    "            (node1_data['y'], node1_data['x']),\n",
    "            (node2_data['y'], node2_data['x'])\n",
    "        ]\n",
    "\n",
    "def get_full_route_path(graph, route_indices, warehouse_nodes):\n",
    "    \"\"\"\n",
    "    Получает полный маршрут по дорожной сети для заданной последовательности складов\n",
    "\n",
    "    Args:\n",
    "        graph: дорожный граф\n",
    "        route_indices: список индексов складов в порядке посещения\n",
    "        warehouse_nodes: список узлов графа, соответствующих складам\n",
    "\n",
    "    Returns:\n",
    "        list: список координат (lat, lon) всего маршрута\n",
    "    \"\"\"\n",
    "    full_path = []\n",
    "\n",
    "    for i in range(len(route_indices) - 1):\n",
    "        # Получаем узлы графа для текущего и следующего склада\n",
    "        from_idx = route_indices[i]\n",
    "        to_idx = route_indices[i + 1]\n",
    "\n",
    "        from_node = warehouse_nodes[from_idx]\n",
    "        to_node = warehouse_nodes[to_idx]\n",
    "\n",
    "        # Получаем путь между этими складами\n",
    "        segment_path = get_path_between_warehouses(graph, from_node, to_node)\n",
    "\n",
    "        # Добавляем сегмент к общему пути (исключая последнюю точку, чтобы не дублировать)\n",
    "        if full_path:\n",
    "            full_path.extend(segment_path[1:])  # исключаем первую точку (она уже есть)\n",
    "        else:\n",
    "            full_path.extend(segment_path)\n",
    "\n",
    "    return full_path\n",
    "\n",
    "def visualize_route_on_roads(graph, route_indices, warehouse_nodes, warehouses_df,\n",
    "                           scenario_name, algorithm_name, distance_km=None):\n",
    "    \"\"\"\n",
    "    Визуализирует маршрут по реальным дорогам на карте\n",
    "\n",
    "    Args:\n",
    "        graph: дорожный граф\n",
    "        route_indices: список индексов складов\n",
    "        warehouse_nodes: список узлов графа для складов\n",
    "        warehouses_df: DataFrame со складами\n",
    "        scenario_name: название сценария\n",
    "        algorithm_name: название алгоритма\n",
    "        distance_km: расстояние маршрута (опционально)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "    # 1. Получаем полный путь по дорогам\n",
    "    route_path = get_full_route_path(graph, route_indices, warehouse_nodes)\n",
    "\n",
    "    # 2. Преобразуем координаты для отрисовки\n",
    "    route_lats = [coord[0] for coord in route_path]\n",
    "    route_lons = [coord[1] for coord in route_path]\n",
    "\n",
    "    # 3. Рисуем дорожную сеть (упрощенную версию)\n",
    "    # Получаем ограничивающую рамку для маршрута\n",
    "    min_lat, max_lat = min(route_lats), max(route_lats)\n",
    "    min_lon, max_lon = min(route_lons), max(route_lons)\n",
    "\n",
    "    # Добавляем отступ\n",
    "    lat_padding = (max_lat - min_lat) * 0.2\n",
    "    lon_padding = (max_lon - min_lon) * 0.2\n",
    "\n",
    "    bbox = (min_lat - lat_padding, max_lat + lat_padding,\n",
    "            min_lon - lon_padding, max_lon + lon_padding)\n",
    "\n",
    "    # Извлекаем подграф для этой области\n",
    "    nodes_in_bbox = []\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        if (bbox[0] <= data['y'] <= bbox[1] and\n",
    "            bbox[2] <= data['x'] <= bbox[3]):\n",
    "            nodes_in_bbox.append(node)\n",
    "\n",
    "    subgraph = graph.subgraph(nodes_in_bbox)\n",
    "\n",
    "    # Рисуем дороги (тонкие серые линии)\n",
    "    for u, v, data in subgraph.edges(data=True):\n",
    "        if 'geometry' in data:\n",
    "            # Если есть геометрия линии, рисуем ее\n",
    "            line_coords = list(data['geometry'].coords)\n",
    "            x_coords = [coord[0] for coord in line_coords]\n",
    "            y_coords = [coord[1] for coord in line_coords]\n",
    "            ax.plot(x_coords, y_coords, 'lightgray', linewidth=0.5, alpha=0.5, zorder=1)\n",
    "        else:\n",
    "            # Иначе рисуем прямую линию между узлами\n",
    "            u_data = subgraph.nodes[u]\n",
    "            v_data = subgraph.nodes[v]\n",
    "            ax.plot([u_data['x'], v_data['x']], [u_data['y'], v_data['y']],\n",
    "                   'lightgray', linewidth=0.5, alpha=0.5, zorder=1)\n",
    "\n",
    "    # 4. Рисуем маршрут (толстая цветная линия)\n",
    "    ax.plot(route_lons, route_lats, 'b-', linewidth=3, alpha=0.8, zorder=3, label='Маршрут')\n",
    "\n",
    "    # Добавляем стрелки направления каждые N точек\n",
    "    arrow_step = max(1, len(route_path) // 10)\n",
    "    for i in range(0, len(route_path) - 1, arrow_step):\n",
    "        if i + 1 < len(route_path):\n",
    "            start_lon, start_lat = route_lons[i], route_lats[i]\n",
    "            end_lon, end_lat = route_lons[i + 1], route_lats[i + 1]\n",
    "\n",
    "            # Пропускаем слишком короткие сегменты\n",
    "            if calculate_geodesic_distance(start_lat, start_lon, end_lat, end_lon) > 1000:\n",
    "                ax.annotate('', xy=(end_lon, end_lat), xytext=(start_lon, start_lat),\n",
    "                           arrowprops=dict(arrowstyle='->', color='red', lw=2, alpha=0.7))\n",
    "\n",
    "    # 5. Рисуем склады\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(route_indices)))\n",
    "\n",
    "    for i, idx in enumerate(route_indices):\n",
    "        if i == 0:  # Начальный склад\n",
    "            color = 'red'\n",
    "            size = 200\n",
    "            marker = '*'\n",
    "            label = 'Начальная точка'\n",
    "            zorder = 5\n",
    "        elif i == len(route_indices) - 1:  # Конечный склад (возврат)\n",
    "            continue  # Не рисуем, это тот же что и начальный\n",
    "        else:\n",
    "            color = colors[i % len(colors)]\n",
    "            size = 100\n",
    "            marker = 'o'\n",
    "            label = None\n",
    "            zorder = 4\n",
    "\n",
    "        warehouse = warehouses_df.iloc[idx]\n",
    "        ax.scatter(warehouse['longitude'], warehouse['latitude'],\n",
    "                  c=color, s=size, marker=marker, edgecolors='black',\n",
    "                  linewidth=2, zorder=zorder, label=label if i == 0 else None)\n",
    "\n",
    "        # Подписываем первые 10 складов\n",
    "        if i < 10:\n",
    "            ax.annotate(warehouse['name'],\n",
    "                       (warehouse['longitude'], warehouse['latitude']),\n",
    "                       xytext=(5, 5), textcoords='offset points',\n",
    "                       fontsize=9, fontweight='bold',\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\",\n",
    "                                facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    # 6. Настраиваем график\n",
    "    title = f'{algorithm_name} - {scenario_name}'\n",
    "    if distance_km:\n",
    "        title += f'\\nОбщее расстояние: {distance_km:.2f} км'\n",
    "\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Долгота')\n",
    "    ax.set_ylabel('Широта')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    # Устанавливаем равные масштабы по осям\n",
    "    ax.set_aspect('equal', adjustable='datalim')\n",
    "\n",
    "    # Сохраняем график\n",
    "    filename = f'road_route_{algorithm_name}_{scenario_name}'.replace(' ', '_').replace('+', '_')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'road_route_visualizations/{filename}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "id": "ULW0h9oYUIOr",
    "outputId": "298fe4d5-4a1d-4791-bbd1-b3097893e952"
   },
   "outputs": [],
   "source": [
    "print(\"Визуализация всех алгоритмов на всех сценариях (4x4 сетка по дорогам)\")\n",
    "\n",
    "# Создаем большую фигуру\n",
    "fig3, axes = plt.subplots(4, 4, figsize=(25, 20))\n",
    "\n",
    "scenarios_dict = {\n",
    "    'Без шума': graph,\n",
    "    'Пробки': graph_traffic_jam,\n",
    "    'Блокировки+Пробки': graph_combined,\n",
    "    'Умеренные условия': graph_moderate\n",
    "}\n",
    "\n",
    "algorithms = ['Жадный', '2-OPT', 'MST', 'Генетический']\n",
    "scenarios_list = list(scenarios_dict.keys())\n",
    "\n",
    "# Преобразуем all_results в DataFrame для удобства поиска\n",
    "all_results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Цвета для алгоритмов\n",
    "algorithm_colors = {\n",
    "    'Жадный': 'blue',\n",
    "    '2-OPT': 'orange',\n",
    "    'MST': 'green',\n",
    "    'Генетический': 'purple'\n",
    "}\n",
    "\n",
    "for algo_idx, algorithm in enumerate(algorithms):\n",
    "    for scenario_idx, scenario_name in enumerate(scenarios_list):\n",
    "        ax = axes[algo_idx, scenario_idx]\n",
    "        scenario_graph = scenarios_dict[scenario_name]\n",
    "\n",
    "        # Получаем результат - ИСПРАВЛЕННЫЙ СПОСОБ\n",
    "        result_filtered = all_results_df[\n",
    "            (all_results_df['algorithm'] == algorithm) &\n",
    "            (all_results_df['scenario'] == scenario_name)\n",
    "        ]\n",
    "\n",
    "        if not result_filtered.empty:\n",
    "            result_data = result_filtered.iloc[0]\n",
    "            route_indices = result_data['route']\n",
    "            distance_km = result_data['distance_km']\n",
    "\n",
    "            # Определяем, какие узлы использовать (в зависимости от того, какой набор складов)\n",
    "            # Проверяем, доступен ли all_warehouse_nodes\n",
    "            if 'all_warehouse_nodes' in locals() or 'all_warehouse_nodes' in globals():\n",
    "                current_nodes = all_warehouse_nodes\n",
    "                current_df = all_warehouses_df\n",
    "            else:\n",
    "                current_nodes = warehouse_nodes\n",
    "                current_df = warehouses_df\n",
    "\n",
    "            # Получаем путь по дорогам\n",
    "            route_path = get_full_route_path(scenario_graph, route_indices, current_nodes)\n",
    "\n",
    "            if route_path and len(route_path) > 1:\n",
    "                route_lats = [coord[0] for coord in route_path]\n",
    "                route_lons = [coord[1] for coord in route_path]\n",
    "\n",
    "                # Определяем bounding box\n",
    "                min_lat, max_lat = min(route_lats), max(route_lats)\n",
    "                min_lon, max_lon = min(route_lons), max(route_lons)\n",
    "\n",
    "                # Рисуем упрощенную дорожную сеть\n",
    "                lat_padding = (max_lat - min_lat) * 0.4\n",
    "                lon_padding = (max_lon - min_lon) * 0.4\n",
    "\n",
    "                bbox = (min_lat - lat_padding, max_lat + lat_padding,\n",
    "                        min_lon - lon_padding, max_lon + lon_padding)\n",
    "\n",
    "                # Отбираем и рисуем дороги в bounding box\n",
    "                roads_drawn = 0\n",
    "                for u, v, data in scenario_graph.edges(data=True):\n",
    "                    if roads_drawn > 500:  # Ограничиваем количество дорог для производительности\n",
    "                        break\n",
    "\n",
    "                    u_data = scenario_graph.nodes[u]\n",
    "                    v_data = scenario_graph.nodes[v]\n",
    "\n",
    "                    # Проверяем, попадает ли дорога в bounding box\n",
    "                    u_in_bbox = (bbox[0] <= u_data['y'] <= bbox[1] and\n",
    "                                bbox[2] <= u_data['x'] <= bbox[3])\n",
    "                    v_in_bbox = (bbox[0] <= v_data['y'] <= bbox[1] and\n",
    "                                bbox[2] <= v_data['x'] <= bbox[3])\n",
    "\n",
    "                    if u_in_bbox or v_in_bbox:\n",
    "                        if 'geometry' in data:\n",
    "                            line_coords = list(data['geometry'].coords)\n",
    "                            x_coords = [coord[0] for coord in line_coords]\n",
    "                            y_coords = [coord[1] for coord in line_coords]\n",
    "                            ax.plot(x_coords, y_coords, 'lightgray',\n",
    "                                   linewidth=0.2, alpha=0.2)\n",
    "                        else:\n",
    "                            ax.plot([u_data['x'], v_data['x']],\n",
    "                                   [u_data['y'], v_data['y']],\n",
    "                                   'lightgray', linewidth=0.2, alpha=0.2)\n",
    "\n",
    "                        roads_drawn += 1\n",
    "\n",
    "                # Рисуем маршрут\n",
    "                ax.plot(route_lons, route_lats,\n",
    "                       color=algorithm_colors.get(algorithm, 'blue'),\n",
    "                       linewidth=2, alpha=0.8)\n",
    "\n",
    "                # Рисуем начальный склад\n",
    "                if route_indices and len(route_indices) > 0 and route_indices[0] < len(current_df):\n",
    "                    start_idx = route_indices[0]\n",
    "                    start_warehouse = current_df.iloc[start_idx]\n",
    "                    ax.scatter(start_warehouse['longitude'], start_warehouse['latitude'],\n",
    "                              c='red', s=60, marker='*', edgecolors='black',\n",
    "                              linewidth=1.5, zorder=5)\n",
    "\n",
    "                # Рисуем несколько промежуточных складов (первые 5)\n",
    "                if len(route_indices) > 1:\n",
    "                    for i in range(1, min(6, len(route_indices) - 1)):\n",
    "                        idx = route_indices[i]\n",
    "                        if idx < len(current_df):\n",
    "                            warehouse = current_df.iloc[idx]\n",
    "                            ax.scatter(warehouse['longitude'], warehouse['latitude'],\n",
    "                                      c='green', s=40, marker='o', edgecolors='black',\n",
    "                                      linewidth=1, zorder=4)\n",
    "\n",
    "            ax.set_title(f'{algorithm}\\n{scenario_name}\\n{distance_km:.1f} км',\n",
    "                        fontsize=10, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.1)\n",
    "            ax.set_aspect('equal', adjustable='datalim')\n",
    "\n",
    "            # Убираем подписи осей для внутренних графиков\n",
    "            if algo_idx < 3:\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_xlabel('')\n",
    "            if scenario_idx > 0:\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_ylabel('')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Нет данных',\n",
    "                   ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title(f'{algorithm}: {scenario_name}', fontsize=10)\n",
    "\n",
    "# Добавляем общие метки\n",
    "fig3.text(0.5, 0.04, 'Сценарии', ha='center', va='center',\n",
    "         fontsize=14, fontweight='bold')\n",
    "fig3.text(0.04, 0.5, 'Алгоритмы', ha='center', va='center',\n",
    "         rotation='vertical', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Добавляем легенду\n",
    "legend_patches = [mpatches.Patch(color=color, label=algo)\n",
    "                 for algo, color in algorithm_colors.items()]\n",
    "legend_patches.append(mpatches.Patch(color='red', label='Начальная точка'))\n",
    "fig3.legend(handles=legend_patches, loc='upper right',\n",
    "           bbox_to_anchor=(0.98, 0.98), fontsize=11)\n",
    "\n",
    "plt.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])\n",
    "plt.savefig('road_route_visualizations/all_algorithms_all_scenarios_grid_roads.png',\n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymtRqg3QUkna",
    "outputId": "d50cb9e9-5578-435a-b1c8-97f230fee6ee"
   },
   "outputs": [],
   "source": [
    "print(\"Сводная таблица результатов всех алгоритмов:\")\n",
    "\n",
    "# Создаем таблицу\n",
    "summary_data = []\n",
    "for scenario in scenarios_list:\n",
    "    for algorithm in algorithms:\n",
    "        result_filtered = all_results_df[\n",
    "            (all_results_df['algorithm'] == algorithm) &\n",
    "            (all_results_df['scenario'] == scenario)\n",
    "        ]\n",
    "\n",
    "        if not result_filtered.empty:\n",
    "            result_data = result_filtered.iloc[0]\n",
    "            summary_data.append({\n",
    "                'Сценарий': scenario,\n",
    "                'Алгоритм': algorithm,\n",
    "                'Расстояние (км)': result_data['distance_km'],\n",
    "                'Время (сек)': result_data['time_sec']\n",
    "            })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Выводим таблицу\n",
    "for scenario in scenarios_list:\n",
    "    print(f\"\\n{scenario}:\")\n",
    "    print(\"-\" * 40)\n",
    "    scenario_data = summary_df[summary_df['Сценарий'] == scenario]\n",
    "\n",
    "    for _, row in scenario_data.iterrows():\n",
    "        print(f\"  {row['Алгоритм']:20} | {row['Расстояние (км)']:8.2f} км | {row['Время (сек)']:8.4f} сек\")\n",
    "\n",
    "# Находим лучшие результаты для каждого сценария\n",
    "print(\"Лучшие результаты по сценариям:\")\n",
    "print(\"=\"*80)\n",
    "for scenario in scenarios_list:\n",
    "    scenario_data = summary_df[summary_df['Сценарий'] == scenario]\n",
    "    if not scenario_data.empty:\n",
    "        best_row = scenario_data.loc[scenario_data['Расстояние (км)'].idxmin()]\n",
    "        print(f\"\\n{scenario}:\")\n",
    "        print(f\"  Лучший алгоритм: {best_row['Алгоритм']}\")\n",
    "        print(f\"  Расстояние: {best_row['Расстояние (км)']:.2f} км\")\n",
    "        print(f\"  Время: {best_row['Время (сек)']:.4f} сек\")\n",
    "\n",
    "# Сохраняем таблицу в файл\n",
    "summary_df.to_csv('road_route_visualizations/summary_results.csv', index=False)\n",
    "print(f\"\\nТаблица сохранена в файл: 'road_route_visualizations/summary_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PP7md03QeZ2J",
    "outputId": "be62de67-2f11-4e72-ed0a-f9811b90e7cf"
   },
   "outputs": [],
   "source": [
    "print(\"Визуализация лучшего маршрута для каждого сценария\")\n",
    "\n",
    "# Создаем фигуру 2x2 для лучших маршрутов\n",
    "fig4, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "def route_indices_to_nodes(route_indices, warehouse_nodes):\n",
    "    \"\"\"Преобразует индексы складов в узлы графа\"\"\"\n",
    "    return [warehouse_nodes[i] for i in route_indices]\n",
    "\n",
    "for idx, scenario_name in enumerate(scenarios_list):\n",
    "    ax = axes[idx]\n",
    "    scenario_graph = scenarios_dict[scenario_name]\n",
    "\n",
    "    # Находим лучший алгоритм для этого сценария (минимальное расстояние)\n",
    "    scenario_data = summary_df[summary_df['Сценарий'] == scenario_name]\n",
    "\n",
    "    if not scenario_data.empty:\n",
    "        best_row = scenario_data.loc[scenario_data['Расстояние (км)'].idxmin()]\n",
    "        best_algorithm = best_row['Алгоритм']\n",
    "        best_distance = best_row['Расстояние (км)']\n",
    "\n",
    "        # Получаем маршрут\n",
    "        result_filtered = all_results_df[\n",
    "            (all_results_df['algorithm'] == best_algorithm) &\n",
    "            (all_results_df['scenario'] == scenario_name)\n",
    "        ]\n",
    "\n",
    "        if not result_filtered.empty:\n",
    "            result_data = result_filtered.iloc[0]\n",
    "            route_indices = result_data['route']\n",
    "\n",
    "            # Определяем узлы графа\n",
    "            if 'all_warehouse_nodes' in locals() or 'all_warehouse_nodes' in globals():\n",
    "                current_nodes = all_warehouse_nodes\n",
    "                current_df = all_warehouses_df\n",
    "            else:\n",
    "                current_nodes = warehouse_nodes\n",
    "                current_df = warehouses_df\n",
    "\n",
    "            # Преобразуем индексы в узлы графа\n",
    "            route_nodes = route_indices_to_nodes(route_indices, current_nodes)\n",
    "\n",
    "            # Получаем полный путь по узлам графа\n",
    "            full_path = []\n",
    "            for i in range(len(route_nodes) - 1):\n",
    "                try:\n",
    "                    # Находим кратчайший путь между узлами\n",
    "                    segment_path = nx.shortest_path(scenario_graph, route_nodes[i], route_nodes[i+1], weight='length')\n",
    "                    # Добавляем все узлы сегмента, кроме последнего (чтобы не дублировать)\n",
    "                    full_path.extend(segment_path[:-1])\n",
    "                except (nx.NetworkXNoPath, nx.NodeNotFound) as e:\n",
    "                    print(f\"Предупреждение для {scenario_name}: не удалось найти путь между {route_nodes[i]} и {route_nodes[i+1]}: {e}\")\n",
    "                    # Если пути нет, добавляем только начальный и конечный узел\n",
    "                    if not full_path:\n",
    "                        full_path.append(route_nodes[i])\n",
    "                    full_path.append(route_nodes[i+1])\n",
    "\n",
    "            # Добавляем последний узел\n",
    "            if route_nodes:\n",
    "                full_path.append(route_nodes[-1])\n",
    "\n",
    "            # Визуализируем\n",
    "            if full_path:\n",
    "                # Рисуем дорожную сеть\n",
    "                ox.plot_graph(\n",
    "                    scenario_graph,\n",
    "                    ax=ax,\n",
    "                    node_size=0,\n",
    "                    edge_linewidth=0.3,\n",
    "                    edge_color='lightgray',\n",
    "                    bgcolor='white',\n",
    "                    show=False,\n",
    "                    close=False\n",
    "                )\n",
    "\n",
    "                # Рисуем маршрут\n",
    "                ox.plot_graph_route(\n",
    "                    scenario_graph,\n",
    "                    full_path,\n",
    "                    route_linewidth=4,\n",
    "                    route_color='red',\n",
    "                    route_alpha=0.8,\n",
    "                    orig_dest_size=0,\n",
    "                    ax=ax,\n",
    "                    show=False,\n",
    "                    close=False\n",
    "                )\n",
    "\n",
    "                # Рисуем склады\n",
    "                for i, (idx_point, warehouse) in enumerate(current_df.iterrows()):\n",
    "                    # Проверяем, что индекс в пределах маршрута\n",
    "                    color = 'blue'\n",
    "                    size = 150\n",
    "                    alpha = 0.9\n",
    "\n",
    "                    # Выделяем склады, которые есть в маршруте\n",
    "                    if idx_point in route_indices:\n",
    "                        position_in_route = route_indices.index(idx_point)\n",
    "                        if position_in_route == 0:  # Начальный склад\n",
    "                            color = 'red'\n",
    "                            size = 200\n",
    "                        else:\n",
    "                            color = 'green'\n",
    "                            size = 180\n",
    "\n",
    "                    ax.scatter(\n",
    "                        warehouse['longitude'],\n",
    "                        warehouse['latitude'],\n",
    "                        c=color,\n",
    "                        s=size,\n",
    "                        alpha=alpha,\n",
    "                        edgecolors='black',\n",
    "                        linewidth=1.5,\n",
    "                        zorder=5\n",
    "                    )\n",
    "\n",
    "                # Подписываем начальный склад\n",
    "                if route_indices and route_indices[0] < len(current_df):\n",
    "                    start_warehouse = current_df.iloc[route_indices[0]]\n",
    "                    ax.annotate(\n",
    "                        start_warehouse['name'][:15] + '...' if len(start_warehouse['name']) > 15 else start_warehouse['name'],\n",
    "                        (start_warehouse['longitude'], start_warehouse['latitude']),\n",
    "                        xytext=(8, 8),\n",
    "                        textcoords='offset points',\n",
    "                        fontsize=9,\n",
    "                        fontweight='bold',\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='white', alpha=0.8)\n",
    "                    )\n",
    "\n",
    "        # Настраиваем график\n",
    "        ax.set_title(f'{scenario_name}\\nЛучший алгоритм: {best_algorithm}\\nРасстояние: {best_distance:.1f} км',\n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Долгота')\n",
    "        ax.set_ylabel('Широта')\n",
    "\n",
    "        # Добавляем легенду только для первого графика\n",
    "        if idx == 0:\n",
    "            from matplotlib.lines import Line2D\n",
    "            legend_elements = [\n",
    "                Line2D([0], [0], color='lightgray', lw=2, label='Дороги'),\n",
    "                Line2D([0], [0], color='red', lw=4, label='Маршрут'),\n",
    "                Line2D([0], [0], marker='o', color='w', label='Начальный склад',\n",
    "                      markerfacecolor='red', markersize=10, markeredgecolor='black'),\n",
    "                Line2D([0], [0], marker='o', color='w', label='Склады в маршруте',\n",
    "                      markerfacecolor='green', markersize=10, markeredgecolor='black'),\n",
    "                Line2D([0], [0], marker='o', color='w', label='Остальные склады',\n",
    "                      markerfacecolor='blue', markersize=10, markeredgecolor='black')\n",
    "            ]\n",
    "            ax.legend(handles=legend_elements, loc='upper right', fontsize=9)\n",
    "\n",
    "plt.suptitle('Лучшие маршруты для каждого сценария',\n",
    "            fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('road_route_visualizations/best_routes_all_scenarios.png',\n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "T17rEM_Bebvx",
    "outputId": "7e2fcf03-1fcb-40af-bf4d-6d8e6132f58b"
   },
   "outputs": [],
   "source": [
    "print(\"\\nВизуализация каждого алгоритма по отдельности\")\n",
    "\n",
    "algorithms = ['Жадный', '2-OPT', 'MST', 'Генетический']\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    print(f\"  Алгоритм: {algorithm}\")\n",
    "\n",
    "    # Создаем фигуру для этого алгоритма на всех сценариях\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, (scenario_name, scenario_graph) in enumerate(scenarios_dict.items()):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        # Получаем результат\n",
    "        result_filtered = all_results_df[\n",
    "            (all_results_df['algorithm'] == algorithm) &\n",
    "            (all_results_df['scenario'] == scenario_name)\n",
    "        ]\n",
    "\n",
    "        if not result_filtered.empty:\n",
    "            result_data = result_filtered.iloc[0]\n",
    "            route_indices = result_data['route']\n",
    "            distance_km = result_data['distance_km']\n",
    "\n",
    "            # Определяем узлы\n",
    "            if 'all_warehouse_nodes' in locals() or 'all_warehouse_nodes' in globals():\n",
    "                current_nodes = all_warehouse_nodes\n",
    "                current_df = all_warehouses_df\n",
    "            else:\n",
    "                current_nodes = warehouse_nodes\n",
    "                current_df = warehouses_df\n",
    "\n",
    "            # Преобразуем индексы в узлы\n",
    "            route_nodes = route_indices_to_nodes(route_indices, current_nodes)\n",
    "\n",
    "            # Получаем полный путь\n",
    "            full_path = []\n",
    "            for i in range(len(route_nodes) - 1):\n",
    "                try:\n",
    "                    segment_path = nx.shortest_path(scenario_graph, route_nodes[i], route_nodes[i+1], weight='length')\n",
    "                    full_path.extend(segment_path[:-1])\n",
    "                except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "                    if not full_path:\n",
    "                        full_path.append(route_nodes[i])\n",
    "                    full_path.append(route_nodes[i+1])\n",
    "\n",
    "            if route_nodes:\n",
    "                full_path.append(route_nodes[-1])\n",
    "\n",
    "            # Визуализируем\n",
    "            if full_path:\n",
    "                # Дорожная сеть\n",
    "                ox.plot_graph(\n",
    "                    scenario_graph,\n",
    "                    ax=ax,\n",
    "                    node_size=0,\n",
    "                    edge_linewidth=0.3,\n",
    "                    edge_color='lightgray',\n",
    "                    bgcolor='white',\n",
    "                    show=False,\n",
    "                    close=False\n",
    "                )\n",
    "\n",
    "                # Маршрут\n",
    "                ox.plot_graph_route(\n",
    "                    scenario_graph,\n",
    "                    full_path,\n",
    "                    route_linewidth=4,\n",
    "                    route_color='blue',\n",
    "                    route_alpha=0.8,\n",
    "                    orig_dest_size=0,\n",
    "                    ax=ax,\n",
    "                    show=False,\n",
    "                    close=False\n",
    "                )\n",
    "\n",
    "                # Склады\n",
    "                for i, (idx_point, warehouse) in enumerate(current_df.iterrows()):\n",
    "                    color = 'gray'\n",
    "                    size = 100\n",
    "\n",
    "                    if idx_point in route_indices:\n",
    "                        position = route_indices.index(idx_point)\n",
    "                        if position == 0:\n",
    "                            color = 'red'\n",
    "                            size = 200\n",
    "                        elif position == len(route_indices) - 1:\n",
    "                            color = 'darkred'\n",
    "                            size = 150\n",
    "                        else:\n",
    "                            color = 'green'\n",
    "                            size = 120\n",
    "\n",
    "                    ax.scatter(\n",
    "                        warehouse['longitude'],\n",
    "                        warehouse['latitude'],\n",
    "                        c=color,\n",
    "                        s=size,\n",
    "                        alpha=0.9,\n",
    "                        edgecolors='black',\n",
    "                        linewidth=1,\n",
    "                        zorder=5\n",
    "                    )\n",
    "\n",
    "                # Подпись начального склада\n",
    "                if route_indices and route_indices[0] < len(current_df):\n",
    "                    start_warehouse = current_df.iloc[route_indices[0]]\n",
    "                    ax.annotate(\n",
    "                        'Начало',\n",
    "                        (start_warehouse['longitude'], start_warehouse['latitude']),\n",
    "                        xytext=(5, 5),\n",
    "                        textcoords='offset points',\n",
    "                        fontsize=8,\n",
    "                        fontweight='bold',\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='white', alpha=0.8)\n",
    "                    )\n",
    "\n",
    "        ax.set_title(f'{scenario_name}\\n{distance_km:.1f} км',\n",
    "                    fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "\n",
    "    plt.suptitle(f'Алгоритм: {algorithm}\\nМаршруты на разных сценариях',\n",
    "                fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = f'road_route_{algorithm}_all_scenarios'.replace(' ', '_')\n",
    "    plt.savefig(f'road_route_visualizations/{filename}.png',\n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ker2PRSRdoiP"
   },
   "source": [
    "# RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxgF5rDwBxyB"
   },
   "source": [
    "## Базовые методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBw5PD4mOP02"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAiUzy8PQBvD"
   },
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "def load_data():\n",
    "    # Загружаем дорожную сеть\n",
    "    graph = ox.load_graphml(\"moscow_region_drive_network.graphml\")\n",
    "\n",
    "    # Загружаем склады из JSON\n",
    "    with open('warehouses_rc_rfc_coordinates.json', 'r', encoding='utf-8') as f:\n",
    "        warehouses_json = json.load(f)\n",
    "\n",
    "    # Создаем DataFrame\n",
    "    warehouses_data = []\n",
    "    for warehouse in warehouses_json:\n",
    "        warehouses_data.append({\n",
    "            \"name\": warehouse['name'],\n",
    "            \"latitude\": warehouse['latitude'],\n",
    "            \"longitude\": warehouse['longitude']\n",
    "        })\n",
    "\n",
    "    warehouses_df = pd.DataFrame(warehouses_data)\n",
    "\n",
    "    # Привязываем склады к узлам графа\n",
    "    warehouse_nodes = []\n",
    "    for idx, warehouse in warehouses_df.iterrows():\n",
    "        point = (warehouse['latitude'], warehouse['longitude'])\n",
    "        nearest_node = ox.distance.nearest_nodes(graph, point[1], point[0])\n",
    "        warehouse_nodes.append(nearest_node)\n",
    "\n",
    "    warehouses_df['node_id'] = warehouse_nodes\n",
    "\n",
    "    # Создаем матрицу расстояний между складами\n",
    "    n_warehouses = len(warehouse_nodes)\n",
    "    distance_matrix = np.zeros((n_warehouses, n_warehouses))\n",
    "\n",
    "    for i in range(n_warehouses):\n",
    "        for j in range(i+1, n_warehouses):\n",
    "            try:\n",
    "                distance = nx.shortest_path_length(graph, warehouse_nodes[i], warehouse_nodes[j], weight='length')\n",
    "                distance_matrix[i][j] = distance\n",
    "                distance_matrix[j][i] = distance\n",
    "            except nx.NetworkXNoPath:\n",
    "                distance_matrix[i][j] = 1e9\n",
    "                distance_matrix[j][i] = 1e9\n",
    "\n",
    "    # Конвертируем в float32 для PyTorch\n",
    "    distance_matrix = distance_matrix.astype(np.float32)\n",
    "\n",
    "    return distance_matrix, warehouses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGMYQIoUulpM"
   },
   "outputs": [],
   "source": [
    "# Базовая инфраструктура\n",
    "class TSPEnvironment:\n",
    "    \"\"\"\n",
    "    Класс среды\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, distance_matrix, warehouses_df):\n",
    "        self.distance_matrix = distance_matrix # матрица попарных расстояний между складами (в метрах)\n",
    "        self.warehouses_df = warehouses_df     # DataFrame с информацией о складах (координаты, названия)\n",
    "        self.n_warehouses = len(warehouses_df)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Сброс среды в начальное состояние\n",
    "        \"\"\"\n",
    "        self.current_warehouse = 0  # Начальный склад\n",
    "        self.visited = set([0])     # Уже посетили начальный склад\n",
    "        self.route = [0]            # Маршрут начинается с начального склада\n",
    "        self.total_distance = 0     # Общая пройденная дистанция\n",
    "        self.done = False           # Флаг завершения эпизода\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        Кодирование состояния в формат для нейросети\n",
    "        \"\"\"\n",
    "        visited_mask = np.zeros(self.n_warehouses, dtype=np.float32)\n",
    "        for i in range(self.n_warehouses):\n",
    "            visited_mask[i] = 1.0 if i in self.visited else 0.0\n",
    "\n",
    "        return {\n",
    "            'current': self.current_warehouse,  # индекс текущего склада\n",
    "            'visited_mask': visited_mask        #бинарный вектор длины n_warehouses (1 = склад посещен, 0 = не посещен)\n",
    "        }\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Выполнение одного шага в среде\n",
    "\n",
    "        Правила:\n",
    "        1. Если действие - посещение уже посещенного склада (кроме случая завершения) -> штраф -1000\n",
    "        2. Иначе: награда = отрицательное расстояние до следующего склада\n",
    "           (чем дальше едем, тем хуже)\n",
    "        3. При завершении маршрута (все склады посещены):\n",
    "           - добавляем расстояние возврата в начальную точку\n",
    "           - даем бонус обратно пропорциональный общей дистанции\n",
    "             1000/(total_distance+1e-6) - поощряем короткие маршруты\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            return self._get_state(), 0.0, True, {}\n",
    "\n",
    "        if action in self.visited and len(self.visited) < self.n_warehouses:\n",
    "            # Штраф за повторное посещение незавершенного маршрута\n",
    "            reward = -1000.0\n",
    "            self.done = True\n",
    "        else:\n",
    "            # Основная награда: отрицательное пройденное расстояние\n",
    "            reward = -float(self.distance_matrix[self.current_warehouse][action])\n",
    "            self.total_distance += float(self.distance_matrix[self.current_warehouse][action])\n",
    "\n",
    "            # Обновление состояния\n",
    "            self.current_warehouse = action\n",
    "            self.visited.add(action)\n",
    "            self.route.append(action)\n",
    "\n",
    "            # Проверка завершения (все склады посещены)\n",
    "            self.done = len(self.visited) == self.n_warehouses\n",
    "\n",
    "            if self.done:\n",
    "                # Добавляем расстояние возврата в начальную точку\n",
    "                return_distance = float(self.distance_matrix[action][0])\n",
    "                reward -= return_distance\n",
    "                self.total_distance += return_distance\n",
    "                self.route.append(0)\n",
    "\n",
    "                # Бонус за завершение: поощряем короткие маршруты\n",
    "                # 1e-6 для избежания деления на 0\n",
    "                reward += 1000.0 / (self.total_distance + 1e-6)\n",
    "\n",
    "        return self._get_state(), float(reward), self.done, {}\n",
    "\n",
    "    def get_route_info(self):\n",
    "        \"\"\"Возвращает информацию о текущем маршруте\"\"\"\n",
    "        return {\n",
    "            'route': self.route,\n",
    "            'total_distance': self.total_distance,\n",
    "            'distance_km': self.total_distance / 1000.0  # Конвертация в км\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSiRidnUSwiG"
   },
   "outputs": [],
   "source": [
    "# Нейросетевые архитектуры\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    сеть для Policy Gradient методов\n",
    "\n",
    "    Архитектура: простая полносвязная сеть с 2 скрытыми слоями\n",
    "    Размеры: вход = n_warehouses * 2, выход = n_warehouses\n",
    "\n",
    "    Входные данные:\n",
    "    - One-hot encoding текущего склада (n_warehouses)\n",
    "    - Маска посещенных складов (n_warehouses)\n",
    "    Всего: 2 * n_warehouses признаков\n",
    "\n",
    "    Параметры:\n",
    "    - hidden_size=128: стандартный размер для простых задач\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_warehouses, hidden_size=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.n_warehouses = n_warehouses\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_warehouses * 2, hidden_size),  # Входной слой\n",
    "            nn.ReLU(),  # Активация для нелинейности\n",
    "            nn.Linear(hidden_size, hidden_size),  # Скрытый слой\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_warehouses)  # Выходной слой\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Прямой проход через сеть\"\"\"\n",
    "        current = state['current']\n",
    "        visited_mask = state['visited_mask']\n",
    "\n",
    "        # One-hot кодирование текущего положения\n",
    "        current_onehot = torch.zeros(current.shape[0], self.n_warehouses, dtype=torch.float32)\n",
    "        for i, c in enumerate(current):\n",
    "            current_onehot[i, c] = 1.0\n",
    "\n",
    "        # Конкатенация one-hot вектора и маски посещенных\n",
    "        x = torch.cat([current_onehot, visited_mask], dim=1)\n",
    "\n",
    "        # Проход через сеть\n",
    "        logits = self.network(x)\n",
    "\n",
    "        # Маскирование уже посещенных складов (большое отрицательное число)\n",
    "        mask = visited_mask.bool()\n",
    "        logits[mask] = -1e9  # Делаем их невыбираемыми\n",
    "\n",
    "        # Softmax для получения вероятностей действий\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Q-сеть для DQN методов\n",
    "    - Выход: Q-значения для каждого действия\n",
    "    - Используется для оценки \"ценности\" действий\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_warehouses, hidden_size=128):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.n_warehouses = n_warehouses\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_warehouses * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_warehouses)  # Q-значения для каждого склада\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Возвращает Q-значения для всех возможных действий\"\"\"\n",
    "        current = state['current']\n",
    "        visited_mask = state['visited_mask']\n",
    "\n",
    "        current_onehot = torch.zeros(current.shape[0], self.n_warehouses, dtype=torch.float32)\n",
    "        for i, c in enumerate(current):\n",
    "            current_onehot[i, c] = 1.0\n",
    "\n",
    "        x = torch.cat([current_onehot, visited_mask], dim=1)\n",
    "\n",
    "        q_values = self.network(x)\n",
    "        q_values[visited_mask.bool()] = -1e9  # Маскирование посещенных\n",
    "\n",
    "        return q_values\n",
    "\n",
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Сеть для Actor-Critic методов (A2C, PPO)\n",
    "    Содержит:\n",
    "    - Общий энкодер для извлечения признаков\n",
    "    - Два \"головы\":\n",
    "      1. Actor: вероятности действий\n",
    "      2. Critic: оценка ценности состояния\n",
    "\n",
    "    Архитектура разделена для стабильности обучения\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_warehouses, hidden_size=128):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.n_warehouses = n_warehouses\n",
    "\n",
    "        # Общий энкодер (shared feature extractor)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_warehouses * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Actor head: политика (распределение действий)\n",
    "        self.actor = nn.Linear(hidden_size, n_warehouses)\n",
    "\n",
    "        # Critic head: ценность состояния (скаляр)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Возвращает и политику, и ценность состояния\"\"\"\n",
    "        current = state['current']\n",
    "        visited_mask = state['visited_mask']\n",
    "\n",
    "        # Подготовка входных данных\n",
    "        current_onehot = torch.zeros(current.shape[0], self.n_warehouses, dtype=torch.float32)\n",
    "        for i, c in enumerate(current):\n",
    "            current_onehot[i, c] = 1.0\n",
    "\n",
    "        x = torch.cat([current_onehot, visited_mask], dim=1)\n",
    "        features = self.encoder(x)  # Общие признаки\n",
    "\n",
    "        # Actor: вероятности действий\n",
    "        logits = self.actor(features)\n",
    "        logits[visited_mask.bool()] = -1e9  # Маскирование\n",
    "        action_probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Critic: ценность состояния\n",
    "        state_value = self.critic(features)\n",
    "\n",
    "        return action_probs, state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ho6SPGLlw58C"
   },
   "source": [
    "Агенты RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfYL_Tu7xKWo"
   },
   "outputs": [],
   "source": [
    "class PolicyGradientAgent:\n",
    "    \"\"\"\n",
    "    Агент Policy Gradient\n",
    "\n",
    "    Алгоритм:\n",
    "    1. Собирает траектории (действия, логи вероятностей, награды)\n",
    "    2. Вычисляет return (сумму наград с discount factor)\n",
    "    3. Обновляет политику в направлении увеличения награды\n",
    "\n",
    "    Гиперпараметры:\n",
    "    - learning_rate=0.001: стандартное значение для Adam\n",
    "    - discount_factor=0.99: учитывает будущие награды\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_warehouses, learning_rate=0.001):\n",
    "        self.policy_network = PolicyNetwork(n_warehouses)\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "        self.saved_log_probs = []  # Логи вероятностей выбранных действий\n",
    "        self.rewards = []  # Награды за шаги\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Выбор действия на основе текущей политики\"\"\"\n",
    "        # Конвертация состояния в тензоры\n",
    "        state_tensor = {\n",
    "            'current': torch.tensor([state['current']], dtype=torch.long),\n",
    "            'visited_mask': torch.tensor([state['visited_mask']], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "        # Получение вероятностей действий\n",
    "        probs = self.policy_network(state_tensor)\n",
    "\n",
    "        # Сэмплирование действия из распределения\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        # Сохранение лога вероятности для обновления\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Обновление политики после завершения эпизода\"\"\"\n",
    "        if not self.rewards:\n",
    "            return\n",
    "\n",
    "        # Вычисление дисконтированных returns (G_t)\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "\n",
    "        # Обратный проход по наградам для расчета returns\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + 0.99 * R  # discount factor γ=0.99\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        # Нормализация returns для стабильности\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "        # Вычисление потерь: -log(π(a|s)) * G_t\n",
    "        for log_prob, R in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        # Градиентный спуск\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Очистка буферов для следующего эпизода\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ir77Z4U_xsf5"
   },
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    \"\"\"\n",
    "    Advantage Actor-Critic агент\n",
    "\n",
    "    Использует Critic для оценки advantage (A = Q - V)\n",
    "    Одновременно обучение Actor и Critic\n",
    "\n",
    "    Гиперпараметры:\n",
    "    - learning_rate=0.001\n",
    "    - gamma=0.99: коэффициент дисконтирования\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_warehouses, learning_rate=0.001, gamma=0.99):\n",
    "        self.actor_critic = ActorCriticNetwork(n_warehouses)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.saved_log_probs = []  # Логи вероятностей действий\n",
    "        self.rewards = []  # Награды\n",
    "        self.state_values = []  # Оценки Critic\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Выбор действия с использованием Actor-Critic\"\"\"\n",
    "        state_tensor = {\n",
    "            'current': torch.tensor([state['current']], dtype=torch.long),\n",
    "            'visited_mask': torch.tensor([state['visited_mask']], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "        # Получение вероятностей действий и оценки состояния\n",
    "        probs, state_value = self.actor_critic(state_tensor)\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        # Сохранение для обновления\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        self.state_values.append(state_value)\n",
    "        return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Обновление Actor и Critic\"\"\"\n",
    "        if not self.rewards:\n",
    "            return\n",
    "\n",
    "        # Вычисление дисконтированных returns\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        # Нормализация returns\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "        # Объединение оценок состояний\n",
    "        state_values = torch.cat(self.state_values)\n",
    "\n",
    "        # Advantage = Returns - State Values\n",
    "        advantages = returns - state_values.squeeze()\n",
    "\n",
    "        # Потери Actor: -log(π(a|s)) * A\n",
    "        for log_prob, advantage in zip(self.saved_log_probs, advantages):\n",
    "            policy_loss.append(-log_prob * advantage.detach())\n",
    "\n",
    "        # Потери Critic: MSE между предсказаниями и returns\n",
    "        value_loss = nn.MSELoss()(state_values.squeeze(), returns)\n",
    "\n",
    "        # Совместное обновление\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.stack(policy_loss).sum() + value_loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Очистка буферов\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZXaT6FpyAf3"
   },
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization агент\n",
    "\n",
    "    Особенности:\n",
    "    - Clipping для стабильности обучения\n",
    "    - Несколько эпох обновления на одном наборе данных\n",
    "    - Энтропийный бонус для exploration\n",
    "\n",
    "    Гиперпараметры:\n",
    "    - learning_rate=0.001\n",
    "    - gamma=0.99: дисконтирование\n",
    "    - clip_epsilon=0.2: параметр clipping\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_warehouses, learning_rate=0.001, gamma=0.99, clip_epsilon=0.2):\n",
    "        self.actor_critic = ActorCriticNetwork(n_warehouses)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.memory = []  # Буфер для хранения траектории\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Выбор действия с сохранением в память\"\"\"\n",
    "        state_tensor = {\n",
    "            'current': torch.tensor([state['current']], dtype=torch.long),\n",
    "            'visited_mask': torch.tensor([state['visited_mask']], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs, state_value = self.actor_critic(state_tensor)\n",
    "            m = torch.distributions.Categorical(probs)\n",
    "            action = m.sample()\n",
    "\n",
    "        # Сохранение всей информации для PPO updates\n",
    "        self.memory.append({\n",
    "            'state': state_tensor,\n",
    "            'action': action,\n",
    "            'log_prob': m.log_prob(action),\n",
    "            'value': state_value,\n",
    "            'reward': 0.0,  # Заполняется позже\n",
    "            'done': False\n",
    "        })\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, final_reward=0.0):\n",
    "        \"\"\"Обновление политики с PPO clipping\"\"\"\n",
    "        if not self.memory:\n",
    "            return\n",
    "\n",
    "        # Распределение финальной награды по всем шагам\n",
    "        for i in range(len(self.memory)):\n",
    "            self.memory[i]['reward'] = final_reward / len(self.memory)\n",
    "\n",
    "        # Извлечение данных из памяти\n",
    "        states = [m['state'] for m in self.memory]\n",
    "        actions = torch.stack([m['action'] for m in self.memory])\n",
    "        old_log_probs = torch.stack([m['log_prob'] for m in self.memory])\n",
    "        old_values = torch.cat([m['value'] for m in self.memory])\n",
    "        rewards = torch.tensor([m['reward'] for m in self.memory], dtype=torch.float32)\n",
    "\n",
    "        # Вычисление дисконтированных returns\n",
    "        returns = []\n",
    "        R = 0.0\n",
    "        for r in rewards.flip(0):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        # Advantage с нормализацией\n",
    "        advantages = returns - old_values.squeeze()\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Несколько эпох PPO\n",
    "        for _ in range(3):\n",
    "            new_log_probs = []\n",
    "            new_values = []\n",
    "            new_entropy = []\n",
    "\n",
    "            # Пересчет для новых параметров\n",
    "            for i, state in enumerate(states):\n",
    "                probs, value = self.actor_critic(state)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                new_log_probs.append(dist.log_prob(actions[i]))\n",
    "                new_values.append(value)\n",
    "                new_entropy.append(dist.entropy())\n",
    "\n",
    "            new_log_probs = torch.stack(new_log_probs)\n",
    "            new_values = torch.cat(new_values)\n",
    "            new_entropy = torch.stack(new_entropy)\n",
    "\n",
    "            # PPO ratio и clipping\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "\n",
    "            # Потери\n",
    "            value_loss = nn.MSELoss()(new_values.squeeze(), returns)\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()  # Берем минимум для clipping\n",
    "            entropy_bonus = -0.01 * new_entropy.mean()  # Поощрение exploration\n",
    "\n",
    "            total_loss = policy_loss + 0.5 * value_loss + entropy_bonus\n",
    "\n",
    "            # Обновление\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Очистка памяти\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PM-G9kt4zGhg"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Deep Q-Network агент\n",
    "\n",
    "    Особенности:\n",
    "    - Experience replay для декорреляции данных\n",
    "    - Target network для стабильности\n",
    "    - ε-greedy exploration\n",
    "\n",
    "    Гиперпараметры:\n",
    "    - learning_rate=0.001\n",
    "    - gamma=0.99\n",
    "    - epsilon=1.0: начальная вероятность случайного действия\n",
    "    - epsilon_decay=0.995: скорость уменьшения ε\n",
    "    - memory_size=10000: размер replay buffer\n",
    "    - batch_size=32: размер мини-батча\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_warehouses, learning_rate=0.001, gamma=0.99,\n",
    "                 epsilon=1.0, epsilon_decay=0.995):\n",
    "        self.q_network = QNetwork(n_warehouses)\n",
    "        self.target_network = QNetwork(n_warehouses)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon  # Для ε-greedy\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=10000)  # Replay buffer\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"ε-greedy выбор действия\"\"\"\n",
    "        # Случайное действие с вероятностью ε\n",
    "        if random.random() < self.epsilon:\n",
    "            visited_mask = state['visited_mask']\n",
    "            valid_actions = [i for i in range(len(visited_mask)) if visited_mask[i] == 0.0]\n",
    "            return random.choice(valid_actions) if valid_actions else 0\n",
    "\n",
    "        # Жадное действие на основе Q-значений\n",
    "        state_tensor = {\n",
    "            'current': torch.tensor([state['current']], dtype=torch.long),\n",
    "            'visited_mask': torch.tensor([state['visited_mask']], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Сохранение опыта в replay buffer\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Обновление Q-сети на мини-батче из replay buffer\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Сэмплирование случайного мини-батча\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Подготовка тензоров\n",
    "        state_current = torch.tensor([s['current'] for s in states], dtype=torch.long)\n",
    "        state_visited = torch.tensor([s['visited_mask'] for s in states], dtype=torch.float32)\n",
    "\n",
    "        next_state_current = torch.tensor([s['current'] for s in next_states], dtype=torch.long)\n",
    "        next_state_visited = torch.tensor([s['visited_mask'] for s in next_states], dtype=torch.float32)\n",
    "\n",
    "        state_tensors = {'current': state_current, 'visited_mask': state_visited}\n",
    "        next_state_tensors = {'current': next_state_current, 'visited_mask': next_state_visited}\n",
    "\n",
    "        # Вычисление target Q-значений с target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_state_tensors).max(1)[0]\n",
    "\n",
    "        # Функция Беллмана: Q-target = r + γ * max Q(s', a')\n",
    "        targets = torch.tensor(rewards, dtype=torch.float32) + \\\n",
    "                  self.gamma * next_q_values * (1 - torch.tensor(dones, dtype=torch.float32))\n",
    "\n",
    "        # Текущие Q-значения\n",
    "        current_q_values = self.q_network(state_tensors)\n",
    "        current_q_values = current_q_values[torch.arange(self.batch_size), actions]\n",
    "\n",
    "        # MSE loss между current и target Q-значениями\n",
    "        loss = nn.MSELoss()(current_q_values, targets)\n",
    "\n",
    "        # Градиентный спуск\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay ε для уменьшения exploration со временем\n",
    "        self.epsilon = max(0.01, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        # Периодическое обновление target network\n",
    "        if random.random() < 0.01:  # С вероятностью 1%\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXk9kJwSzUS-"
   },
   "outputs": [],
   "source": [
    "class DoubleDQNAgent(DQNAgent):\n",
    "    \"\"\"\n",
    "    Double DQN агент - улучшение DQN\n",
    "\n",
    "    Решает проблему переоценки (overestimation) Q-значений:\n",
    "    - Использует online network для выбора действий\n",
    "    - Использует target network для их оценки\n",
    "\n",
    "    Наследует все от DQNAgent, переопределяет только update()\n",
    "    \"\"\"\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Обновление с Double DQN логикой\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Подготовка тензоров\n",
    "        state_current = torch.tensor([s['current'] for s in states], dtype=torch.long)\n",
    "        state_visited = torch.tensor([s['visited_mask'] for s in states], dtype=torch.float32)\n",
    "\n",
    "        next_state_current = torch.tensor([s['current'] for s in next_states], dtype=torch.long)\n",
    "        next_state_visited = torch.tensor([s['visited_mask'] for s in next_states], dtype=torch.float32)\n",
    "\n",
    "        state_tensors = {'current': state_current, 'visited_mask': state_visited}\n",
    "        next_state_tensors = {'current': next_state_current, 'visited_mask': next_state_visited}\n",
    "\n",
    "        # Double DQN: online network выбирает, target network оценивает\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.q_network(next_state_tensors).argmax(1)  # Online network\n",
    "            next_q_values = self.target_network(next_state_tensors)      # Target network\n",
    "            next_q_values = next_q_values[torch.arange(self.batch_size), next_actions]\n",
    "\n",
    "        # Функция Беллмана\n",
    "        targets = torch.tensor(rewards, dtype=torch.float32) + \\\n",
    "                  self.gamma * next_q_values * (1 - torch.tensor(dones, dtype=torch.float32))\n",
    "\n",
    "        # Текущие Q-значения\n",
    "        current_q_values = self.q_network(state_tensors)\n",
    "        current_q_values = current_q_values[torch.arange(self.batch_size), actions]\n",
    "\n",
    "        # Loss и обновление\n",
    "        loss = nn.MSELoss()(current_q_values, targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay ε\n",
    "        self.epsilon = max(0.01, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        # Периодическое обновление target network\n",
    "        if random.random() < 0.01:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVmyq01dw5a_"
   },
   "outputs": [],
   "source": [
    "class SACAgent:\n",
    "    \"\"\"\n",
    "    Soft Actor-Critic агент (для дискретных действий)\n",
    "\n",
    "    Особенности:\n",
    "    - Maximum entropy RL: баланс exploration и exploitation\n",
    "    - Два критика для уменьшения переоценки\n",
    "    - Автоматическая настройка temperature parameter (α)\n",
    "\n",
    "    Гиперпараметры:\n",
    "    - learning_rate=0.001\n",
    "    - gamma=0.99\n",
    "    - alpha=0.2: параметр энтропии (temperature)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_warehouses, learning_rate=0.001, gamma=0.99, alpha=0.2):\n",
    "        self.actor = PolicyNetwork(n_warehouses)\n",
    "        self.critic1 = QNetwork(n_warehouses)  # Первый критик\n",
    "        self.critic2 = QNetwork(n_warehouses)  # Второй критик\n",
    "\n",
    "        # Отдельные оптимизаторы для каждой сети\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=learning_rate)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # Temperature parameter\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Выбор действия с учетом энтропии\"\"\"\n",
    "        state_tensor = {\n",
    "            'current': torch.tensor([state['current']], dtype=torch.long),\n",
    "            'visited_mask': torch.tensor([state['visited_mask']], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = self.actor(state_tensor)\n",
    "            m = torch.distributions.Categorical(probs)\n",
    "            action = m.sample()\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Сохранение опыта в replay buffer\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Обновление актора и критиков\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Сэмплирование мини-батча\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Подготовка тензоров\n",
    "        state_current = torch.tensor([s['current'] for s in states], dtype=torch.long)\n",
    "        state_visited = torch.tensor([s['visited_mask'] for s in states], dtype=torch.float32)\n",
    "\n",
    "        next_state_current = torch.tensor([s['current'] for s in next_states], dtype=torch.long)\n",
    "        next_state_visited = torch.tensor([s['visited_mask'] for s in next_states], dtype=torch.float32)\n",
    "\n",
    "        state_tensors = {'current': state_current, 'visited_mask': state_visited}\n",
    "        next_state_tensors = {'current': next_state_current, 'visited_mask': next_state_visited}\n",
    "\n",
    "        actions_tensor = torch.tensor(actions)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Обновление критиков\n",
    "        with torch.no_grad():\n",
    "            # Распределение политики для следующего состояния\n",
    "            next_probs = self.actor(next_state_tensors)\n",
    "            next_log_probs = torch.log(next_probs + 1e-8)\n",
    "\n",
    "            # Q-значения от двух критиков\n",
    "            next_q1 = self.critic1(next_state_tensors)\n",
    "            next_q2 = self.critic2(next_state_tensors)\n",
    "            next_q = torch.min(next_q1, next_q2)  # Берем минимум для консервативности\n",
    "\n",
    "            # Soft value: ∑π(a|s) * (Q(s,a) - α * logπ(a|s))\n",
    "            next_value = (next_probs * (next_q - self.alpha * next_log_probs)).sum(dim=1)\n",
    "            targets = rewards_tensor + self.gamma * next_value * (1 - dones_tensor)\n",
    "\n",
    "        # Потери для критиков\n",
    "        current_q1 = self.critic1(state_tensors)\n",
    "        current_q2 = self.critic2(state_tensors)\n",
    "\n",
    "        current_q1 = current_q1[torch.arange(self.batch_size), actions_tensor]\n",
    "        current_q2 = current_q2[torch.arange(self.batch_size), actions_tensor]\n",
    "\n",
    "        critic1_loss = nn.MSELoss()(current_q1, targets)\n",
    "        critic2_loss = nn.MSELoss()(current_q2, targets)\n",
    "\n",
    "        # Обновление критиков\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        self.critic1_optimizer.step()\n",
    "\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "        # Обновление актора\n",
    "        probs = self.actor(state_tensors)\n",
    "        log_probs = torch.log(probs + 1e-8)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q1 = self.critic1(state_tensors)\n",
    "            q2 = self.critic2(state_tensors)\n",
    "            q = torch.min(q1, q2)\n",
    "\n",
    "        # Потери актора: максимизация (Q(s,a) - α * logπ(a|s))\n",
    "        actor_loss = (probs * (self.alpha * log_probs - q)).sum(dim=1).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBpU7rOx7Y5C"
   },
   "outputs": [],
   "source": [
    "# Обучение и сравнение\n",
    "def train_rl_agents(distance_matrix, warehouses_df, num_episodes=500):\n",
    "    \"\"\"\n",
    "    Обучение всех RL агентов\n",
    "\n",
    "    Параметры:\n",
    "    - distance_matrix: матрица расстояний\n",
    "    - warehouses_df: информация о складах\n",
    "    - num_episodes=500: количество эпизодов обучения\n",
    "      Источник: эмпирически выбран для начального обучения\n",
    "    \"\"\"\n",
    "    env = TSPEnvironment(distance_matrix, warehouses_df)\n",
    "    n_warehouses = len(warehouses_df)\n",
    "\n",
    "    # Инициализация всех агентов\n",
    "    agents = {\n",
    "        'Policy Gradient': PolicyGradientAgent(n_warehouses),\n",
    "        'A2C': A2CAgent(n_warehouses),\n",
    "        'PPO': PPOAgent(n_warehouses),\n",
    "        'DQN': DQNAgent(n_warehouses),\n",
    "        'Double DQN': DoubleDQNAgent(n_warehouses),\n",
    "        'SAC': SACAgent(n_warehouses)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    print(\"Обучение\")\n",
    "    print(f\"Складов: {n_warehouses}\")\n",
    "    print(f\"Эпизодов: {num_episodes}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "\n",
    "    for agent_name, agent in agents.items():\n",
    "        print(f\"\\n{agent_name}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        episode_rewards = []\n",
    "        episode_distances = []\n",
    "        best_route = None\n",
    "        best_distance = float('inf')\n",
    "\n",
    "        # Основной цикл обучения\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            total_reward = 0.0\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            max_steps = n_warehouses * 2 # Лимит шагов для предотвращения бесконечных циклов\n",
    "\n",
    "            # Эпизод\n",
    "            while not done and step_count < max_steps:\n",
    "                action = agent.select_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                 # Сохранение опыта для off-policy методов\n",
    "                if hasattr(agent, 'store_experience'):\n",
    "                    agent.store_experience(state, action, reward, next_state, done)\n",
    "\n",
    "                # Сохранение наград для on-policy методов\n",
    "                if hasattr(agent, 'rewards'):\n",
    "                    agent.rewards.append(reward)\n",
    "\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "\n",
    "            # Периодическое обновление\n",
    "            if episode % 10 == 0 or done:\n",
    "                if agent_name == 'PPO':        # PPO использует финальную награду\n",
    "                    agent.update(total_reward)\n",
    "                elif hasattr(agent, 'update'):\n",
    "                    agent.update()\n",
    "\n",
    "            # Информация о маршруте\n",
    "            route_info = env.get_route_info()\n",
    "            current_distance = route_info['distance_km']\n",
    "\n",
    "            # Сохранение статистики\n",
    "            episode_rewards.append(total_reward)\n",
    "            episode_distances.append(current_distance)\n",
    "\n",
    "            # Обновление лучшего результата\n",
    "            if current_distance < best_distance and len(env.visited) == n_warehouses:\n",
    "                best_distance = current_distance\n",
    "                best_route = route_info['route']\n",
    "\n",
    "            # Логирование прогресса\n",
    "            if episode % 100 == 0:\n",
    "                visited_count = len(env.visited)\n",
    "                print(f\"  Эпизод {episode:4d}: {current_distance:7.2f} км, \"\n",
    "                      f\"Посещено: {visited_count:2d}/{n_warehouses}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        avg_time_per_episode = total_time / num_episodes\n",
    "\n",
    "        # Сохраняем результаты\n",
    "        results[agent_name] = {\n",
    "            'rewards': episode_rewards,\n",
    "            'distances': episode_distances,\n",
    "            'best_route': best_route,\n",
    "            'best_distance': best_distance,\n",
    "            'avg_last_100': np.mean(episode_distances[-100:]),  # Среднее последних 100\n",
    "            'std_last_100': np.std(episode_distances[-100:]),   # Стандартное отклонение\n",
    "            'total_time': total_time,                           # Общее время\n",
    "            'avg_time_per_episode': avg_time_per_episode        # Среднее время на эпизод\n",
    "        }\n",
    "\n",
    "        print(f\"   Лучшее: {best_distance:.2f} км\")\n",
    "        print(f\"   Среднее (последние 100): {results[agent_name]['avg_last_100']:.2f} км\")\n",
    "        print(f\"   Время: {total_time:.1f} сек ({avg_time_per_episode:.3f} сек/эпизод)\")\n",
    "\n",
    "    return results, agents\n",
    "\n",
    "\n",
    "def print_rl_results_table(results):\n",
    "    \"\"\"\n",
    "    Выводит таблицу сравнения RL агентов\n",
    "\n",
    "    Что считаем:\n",
    "    1. Лучший результат (км) - минимальная длина маршрута за все эпизоды\n",
    "    2. Среднее последние 100 (км) - средняя длина последних 100 маршрутов\n",
    "    3. Станд. отклонение - станд. отклонение последних 100 маршрутов\n",
    "    4. Время обучения (сек) - общее время обучения агента\n",
    "    5. Время на эпизод (сек) - среднее время на один эпизод\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Свобдная таблицы\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Создаем список для данных таблицы\n",
    "    table_data = []\n",
    "\n",
    "    for agent_name, result in results.items():\n",
    "        table_data.append([\n",
    "            agent_name,\n",
    "            f\"{result['best_distance']:.2f}\",\n",
    "            f\"{result['avg_last_100']:.2f}\",\n",
    "            f\"{result['std_last_100']:.2f}\",\n",
    "            f\"{result['total_time']:.1f}\",\n",
    "            f\"{result['avg_time_per_episode']:.3f}\"\n",
    "        ])\n",
    "\n",
    "    # Сортируем по лучшему результату (по возрастанию)\n",
    "    table_data.sort(key=lambda x: float(x[1]))\n",
    "\n",
    "    # Заголовки таблицы\n",
    "    headers = [\n",
    "        \"Метод\",\n",
    "        \"Лучший результат (км)\",\n",
    "        \"Среднее последние 100 (км)\",\n",
    "        \"Станд. отклонение\",\n",
    "        \"Время обучения (сек)\",\n",
    "        \"Время на эпизод (сек)\"\n",
    "    ]\n",
    "\n",
    "    # Выводим таблицу\n",
    "    print(f\"{'Метод':<20} | {'Лучший':>10} | {'Среднее':>10} | {'Ст.откл':>8} | {'Время':>8} | {'Время/эп':>8}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for row in table_data:\n",
    "        print(f\"{row[0]:<20} | {row[1]:>10} | {row[2]:>10} | {row[3]:>8} | {row[4]:>8} | {row[5]:>8}\")\n",
    "\n",
    "    return table_data\n",
    "\n",
    "def plot_results(results):\n",
    "    \"\"\"\n",
    "    Визуализация результатов обучения RL агентов\n",
    "\n",
    "    Выводит 4 графика:\n",
    "    1. Сглаженные награды по эпизодам\n",
    "    2. Сглаженные расстояния маршрутов\n",
    "    3. Лучшие результаты каждого алгоритма\n",
    "    4. Общее время обучения каждого алгоритма\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # 1. График наград (сглаженный)\n",
    "    for agent_name, result in results.items():\n",
    "        rewards_smooth = pd.Series(result['rewards']).rolling(20, min_periods=1).mean()\n",
    "        ax1.plot(rewards_smooth, label=agent_name, alpha=0.7)\n",
    "    ax1.set_title('Episode Rewards (Smoothed)')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. График расстояний (сглаженный)\n",
    "    for agent_name, result in results.items():\n",
    "        distances_smooth = pd.Series(result['distances']).rolling(20, min_periods=1).mean()\n",
    "        ax2.plot(distances_smooth, label=agent_name, alpha=0.7)\n",
    "    ax2.set_title('Route Distances (Smoothed)')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Distance (km)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Лучшие результаты (столбчатая диаграмма)\n",
    "    best_distances = {name: result['best_distance'] for name, result in results.items()}\n",
    "    names = list(best_distances.keys())\n",
    "    distances = list(best_distances.values())\n",
    "\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0', '#ffb3e6']\n",
    "    bars1 = ax3.bar(names, distances, color=colors[:len(names)])\n",
    "    ax3.set_title('Best Route Distances by Algorithm')\n",
    "    ax3.set_ylabel('Distance (km)')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    for bar, distance in zip(bars1, distances):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + max(distances)*0.02,\n",
    "                f'{distance:.1f} км', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # 4. Общее время обучения (сек)\n",
    "    total_times = {name: result['total_time'] for name, result in results.items()}\n",
    "    names_time = list(total_times.keys())\n",
    "    times = list(total_times.values())\n",
    "\n",
    "    bars2 = ax4.bar(names_time, times, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD'])\n",
    "    ax4.set_title('Total Training Time by Algorithm')\n",
    "    ax4.set_ylabel('Time (seconds)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Добавляем значения времени на столбцы\n",
    "    for bar, time_val in zip(bars2, times):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + max(times)*0.02,\n",
    "                f'{time_val:.1f} сек', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rl_algorithms_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WE9E7oQjlR3h"
   },
   "outputs": [],
   "source": [
    "distance_matrix, warehouses_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bGlQyMLFkTNf",
    "outputId": "c411d06e-0286-4d65-ab77-af5968ba7329"
   },
   "outputs": [],
   "source": [
    "print(f\"Загружено: {len(warehouses_df)} складов\")\n",
    "print(f\"Размер матрицы: {distance_matrix.shape}\")\n",
    "print(f\"Тип данных: {distance_matrix.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VsTpVJ7Z6Vlc",
    "outputId": "48dd38e3-c318-445a-bf9e-760f18cd6fbc"
   },
   "outputs": [],
   "source": [
    "print(f\"Загружено: {len(warehouses_df)} складов\")\n",
    "print(f\"Размер матрицы: {distance_matrix.shape}\")\n",
    "print(f\"Тип данных: {distance_matrix.dtype}\")\n",
    "\n",
    "# Запускаем обучение RL агентов\n",
    "print(\"\\nОбучение RL агентов\")\n",
    "results, agents = train_rl_agents(distance_matrix, warehouses_df, num_episodes=500)\n",
    "\n",
    "# Выводим таблицу сравнения\n",
    "print_rl_results_table(results)\n",
    "\n",
    "# Визуализируем результаты\n",
    "plot_results(results)\n",
    "\n",
    "# Сохраняем результаты\n",
    "results_summary = {}\n",
    "for agent_name, result in results.items():\n",
    "    results_summary[agent_name] = {\n",
    "        'best_distance_km': result['best_distance'],\n",
    "        'best_route': result['best_route'],\n",
    "        'avg_last_100_km': result['avg_last_100'],\n",
    "        'std_last_100_km': result['std_last_100'],\n",
    "        'total_time_sec': result['total_time'],\n",
    "        'avg_time_per_episode_sec': result['avg_time_per_episode']\n",
    "    }\n",
    "\n",
    "with open('rl_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"\\nРезультаты сохранены в 'rl_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99BeSsvVsAZ0"
   },
   "source": [
    "## RL оптимизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gphtGBZSAPHb"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgwGEoNOG0v-"
   },
   "outputs": [],
   "source": [
    "# Оптимизируем архитектуры\n",
    "class EnhancedPolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Улучшения для Policy Gradient методов:\n",
    "    1. Добавляем LayerNorm - нормализуем выходы слоев для стабильности обучения\n",
    "    2. Используем Dropout - предотвращаем переобучение (отключаем часть нейронов при обучении)\n",
    "    3. Xavier инициализация - инициализируем веса для быстрой сходимости\n",
    "    4. Больше скрытых слоев - увеличиваем емкость сети для сложных паттернов\n",
    "    \"\"\"\n",
    "    def __init__(self, n_warehouses, hidden_size=256):\n",
    "        super(EnhancedPolicyNetwork, self).__init__()\n",
    "        self.n_warehouses = n_warehouses\n",
    "\n",
    "        # Строим последовательность слоев с регуляризацией\n",
    "        self.network = nn.Sequential(\n",
    "            # Входной слой: one-hot текущего + маска посещенных (2*n_warehouses признаков)\n",
    "            nn.Linear(n_warehouses * 2, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),  # нормализация: стабилизирует обучение\n",
    "            nn.ReLU(),  # Нелинейность для сложных зависимостей\n",
    "            nn.Dropout(0.1),  # регуляризация: отключаем 10% нейронов при обучении\n",
    "\n",
    "            # Скрытый слой\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),  # Еще одна нормализация\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),  # Еще немного регуляризации\n",
    "\n",
    "            # Выходной слой: вероятности выбора каждого склада\n",
    "            nn.Linear(hidden_size, n_warehouses)\n",
    "        )\n",
    "\n",
    "        # инициализация Xavier для равномерного распределения градиентов\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Инициализируем веса по методу Xavier для быстрой сходимости\"\"\"\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)  # Xavier инициализация весов\n",
    "                nn.init.constant_(layer.bias, 0.01)  # Маленькое положительное смещение\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Прямой проход через сеть\"\"\"\n",
    "        # Получаем текущий склад и маску посещенных\n",
    "        current = state['current']\n",
    "        visited_mask = state['visited_mask']\n",
    "\n",
    "        # One-hot кодирование текущего склада\n",
    "        current_onehot = torch.zeros(current.shape[0], self.n_warehouses, dtype=torch.float32)\n",
    "        for i, c in enumerate(current):\n",
    "            current_onehot[i, c] = 1.0\n",
    "\n",
    "        # Объединяем one-hot вектора и маску посещенных\n",
    "        x = torch.cat([current_onehot, visited_mask], dim=1)\n",
    "\n",
    "        # Проходим через все слои сети\n",
    "        logits = self.network(x)\n",
    "\n",
    "        # маскируем уже посещенные склады\n",
    "        mask = visited_mask.bool()\n",
    "        logits[mask] = -1e8  # Очень большое отрицательное число = очень низкая вероятность\n",
    "\n",
    "        # Превращаем logits в вероятности с помощью softmax\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "class EnhancedActorCriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Улучшения Actor-Critic сети:\n",
    "    - LayerNorm в энкодере\n",
    "    - Раздельные головы Actor и Critic\n",
    "    - Xavier инициализация\n",
    "\n",
    "    Actor-Critic:\n",
    "    1. Actor (Актор) - предсказывает вероятности действий\n",
    "    2. Critic (Критик) - оценивает \"ценность\" текущего состояния\n",
    "    3. Общий энкодер - извлекает общие признаки для обоих голов\n",
    "    Преимущество: Critic помогает Actor понимать, какие действия действительно хороши\n",
    "    \"\"\"\n",
    "    def __init__(self, n_warehouses, hidden_size=256):\n",
    "        super(EnhancedActorCriticNetwork, self).__init__()\n",
    "        self.n_warehouses = n_warehouses\n",
    "\n",
    "        # Общий энкодер: извлекает признаки для Actor и Critic\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_warehouses * 2, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Actor: предсказывает вероятности действий\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, n_warehouses)\n",
    "        )\n",
    "\n",
    "        # Critic: оценивает ценность состояния (скалярное значение)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "        # Инициализируем веса\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Инициализация весов для всех слоев\"\"\"\n",
    "        for module in [self.encoder, self.actor, self.critic]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.constant_(layer.bias, 0.01)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Возвращает и политику Actor, и оценку Critic\"\"\"\n",
    "        current = state['current']\n",
    "        visited_mask = state['visited_mask']\n",
    "\n",
    "        # One-hot кодирование текущего склада\n",
    "        current_onehot = torch.zeros(current.shape[0], self.n_warehouses, dtype=torch.float32)\n",
    "        for i, c in enumerate(current):\n",
    "            current_onehot[i, c] = 1.0\n",
    "\n",
    "        # Объединяем входные данные\n",
    "        x = torch.cat([current_onehot, visited_mask], dim=1)\n",
    "\n",
    "        # Извлекаем общие признаки через энкодер\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Actor: вероятности действий\n",
    "        logits = self.actor(features)\n",
    "        logits[visited_mask.bool()] = -1e8  # Маскируем посещенные\n",
    "        action_probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Critic: оценка ценности состояния\n",
    "        state_value = self.critic(features)\n",
    "\n",
    "        return action_probs, state_value\n",
    "\n",
    "class EnhancedSACNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Улучшение сети для SAC (Soft Actor-Critic):\n",
    "    1. Dueling Architecture для лучшей оценки value и advantage\n",
    "    2. LayerNorm для стабильности обучения\n",
    "    3. Dropout для регуляризации\n",
    "    4. Xavier инициализация весов\n",
    "\n",
    "    Потенциальные плюсы:\n",
    "    - Value stream оценивает общую ценность состояния\n",
    "    - Advantage stream оценивает преимущество каждого действия\n",
    "    - Это помогает в задачах с большим пространством действий\n",
    "    \"\"\"\n",
    "    def __init__(self, n_warehouses, hidden_size=256):\n",
    "        super(EnhancedSACNetwork, self).__init__()\n",
    "        self.n_warehouses = n_warehouses\n",
    "\n",
    "        # Общий энкодер\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_warehouses * 2, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        # Value stream: оценивает ценность состояния\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "        # Advantage stream: оценивает преимущество действий\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, n_warehouses)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Инициализация весов по методу Xavier\"\"\"\n",
    "        for module in [self.encoder, self.value_stream, self.advantage_stream]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.constant_(layer.bias, 0.01)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Возвращает Q-значения с использованием Dueling Architecture\"\"\"\n",
    "        current = state['current']\n",
    "        visited_mask = state['visited_mask']\n",
    "\n",
    "        # One-hot кодирование текущего склада\n",
    "        current_onehot = torch.zeros(current.shape[0], self.n_warehouses, dtype=torch.float32)\n",
    "        for i, c in enumerate(current):\n",
    "            current_onehot[i, c] = 1.0\n",
    "\n",
    "        # Объединяем входные данные\n",
    "        x = torch.cat([current_onehot, visited_mask], dim=1)\n",
    "\n",
    "        # Извлекаем признаки через энкодер\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Вычисляем value и advantage\n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "\n",
    "        # Dueling формула: Q(s,a) = V(s) + A(s,a) - mean(A(s,a))\n",
    "        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "\n",
    "        # Маскируем посещенные склады\n",
    "        q_values[visited_mask.bool()] = -1e8\n",
    "\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XicSVEJiFTV7"
   },
   "outputs": [],
   "source": [
    "#Оптимизированные RL aгенты\n",
    "class EnhancedPolicyGradientAgent:\n",
    "    \"\"\"\n",
    "    Улучшение базового Policy Gradient агента\n",
    "\n",
    "    Улучшения:\n",
    "    1. Энтропийная регуляризация: поощряем exploration\n",
    "    2. AdamW оптимизатор: более стабильный Adam с L2 регуляризацией\n",
    "    3. Cosine Annealing: автоматически уменьшаем learning rate\n",
    "    4. Gradient Clipping: предотвращаем взрыв градиентов\n",
    "\n",
    "    Policy Gradient:\n",
    "    1. Собираем траекторию (действия, награды)\n",
    "    2. Вычисляем advantage (насколько действия лучше среднего)\n",
    "    3. Увеличиваем вероятность хороших действий, уменьшаем плохих\n",
    "    \"\"\"\n",
    "    def __init__(self, n_warehouses, learning_rate=0.001, gamma=0.99, entropy_coef=0.01):\n",
    "        # Инициализируем нейросеть\n",
    "        self.policy_network = EnhancedPolicyNetwork(n_warehouses)\n",
    "\n",
    "        # AdamW (Adam с weight decay)\n",
    "        self.optimizer = optim.AdamW(self.policy_network.parameters(),\n",
    "                                     lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "        # Cosine Annealing: автоматически настраивает learning rate\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=1500)\n",
    "\n",
    "        # Гиперпараметры\n",
    "        self.gamma = gamma  # Коэффициент дисконтирования будущих наград\n",
    "        self.entropy_coef = entropy_coef  # Сила энтропийной регуляризации\n",
    "\n",
    "        # Буферы для хранения данных эпизода\n",
    "        self.saved_log_probs = []  # Логарифмы вероятностей выбранных действий\n",
    "        self.rewards = []  # Полученные награды\n",
    "        self.entropies = []  # Энтропия распределения (мера неопределенности)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Выбор действия на основе текущей политики\"\"\"\n",
    "        # Преобразуем состояние в тензор\n",
    "        state_tensor = {\n",
    "            'current': torch.tensor([state['current']], dtype=torch.long),\n",
    "            'visited_mask': torch.tensor([state['visited_mask']], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "        # Получаем вероятности действий из сети\n",
    "        probs = self.policy_network(state_tensor)\n",
    "\n",
    "        # Создаем категориальное распределение\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        action = m.sample()  # Выбираем действие случайно согласно распределению\n",
    "        entropy = m.entropy()  # Вычисляем энтропию (понадобится для регуляризации)\n",
    "\n",
    "        # Сохраняем для обучения\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        self.entropies.append(entropy)\n",
    "\n",
    "        return action.item()  # Возвращаем индекс склада\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Обновление политики после завершения эпизода\"\"\"\n",
    "        if not self.rewards:  # Если нет данных, ничего не делаем\n",
    "            return\n",
    "\n",
    "        # Вычисляем дисконтированные returns (G_t)\n",
    "        R = 0  # Накопленная дисконтированная награда\n",
    "        returns = []  # Список returns для каждого шага\n",
    "\n",
    "        # Идем с конца эпизода к началу\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R  # Дисконтируем и суммируем\n",
    "            returns.insert(0, R)  # Вставляем в начало (чтобы сохранить порядок)\n",
    "\n",
    "        # Преобразуем в тензор и нормализуем для стабильности\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        # Вычисляем loss для Policy Gradient\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(self.saved_log_probs, returns):\n",
    "            # Основная формула Policy Gradient: -log(π(a|s)) * G_t\n",
    "            # Минимизируем это = максимизируем вероятность хороших действий\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        # Энтропийная регуляризация\n",
    "        # Суммируем энтропии за все шаги эпизода\n",
    "        entropy = torch.stack(self.entropies).sum()\n",
    "\n",
    "        # Общий loss: policy_loss - коэффициент * энтропия\n",
    "        # Энтропийная регуляризация поощряет exploration\n",
    "        loss = torch.stack(policy_loss).sum() - self.entropy_coef * entropy\n",
    "\n",
    "        # Градиентный спуск\n",
    "        self.optimizer.zero_grad()  # Обнуляем градиенты\n",
    "        loss.backward()  # Вычисляем градиенты\n",
    "\n",
    "        # Gradient clipping: обрезаем большие градиенты для стабильности\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=1.0)\n",
    "\n",
    "        self.optimizer.step()  # Обновляем веса\n",
    "        self.scheduler.step()  # Обновляем learning rate\n",
    "\n",
    "        # Очищаем буферы для следующего эпизода\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "\n",
    "class EnhancedA2CAgent:\n",
    "    \"\"\"\n",
    "    Улучшенный A2C (Advantage Actor-Critic) агент.\n",
    "\n",
    "    Улучшения:\n",
    "    - Энтропийная регуляризация для Actor\n",
    "    - Gradient clipping\n",
    "    - Cosine annealing scheduler\n",
    "\n",
    "    A2C:\n",
    "    1. Actor выбирает действия\n",
    "    2. Critic оценивает состояния\n",
    "    3. Advantage = Returns - Value (насколько действие лучше среднего)\n",
    "    Critic помогает Actor быстрее учиться, оценивая состояния\n",
    "    \"\"\"\n",
    "    def __init__(self, n_warehouses, learning_rate=0.001, gamma=0.99, entropy_coef=0.01):\n",
    "        # Инициализируем Actor-Critic сеть\n",
    "        self.actor_critic = EnhancedActorCriticNetwork(n_warehouses)\n",
    "\n",
    "        # Оптимизатор и scheduler\n",
    "        self.optimizer = optim.AdamW(self.actor_critic.parameters(),\n",
    "                                     lr=learning_rate, weight_decay=1e-4)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=1500)\n",
    "\n",
    "        # Гиперпараметры\n",
    "        self.gamma = gamma\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "        # Буферы\n",
    "        self.saved_log_probs = []  # Логи вероятностей Actor\n",
    "        self.rewards = []  # Награды\n",
    "        self.state_values = []  # Оценки Critic\n",
    "        self.entropies = []  # Энтропия\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Выбор действия с использованием Actor-Critic\"\"\"\n",
    "        state_tensor = {\n",
    "            'current': torch.tensor([state['current']], dtype=torch.long),\n",
    "            'visited_mask': torch.tensor([state['visited_mask']], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "        # Получаем вероятности действий (Actor) и оценку состояния (Critic)\n",
    "        probs, state_value = self.actor_critic(state_tensor)\n",
    "\n",
    "        # Создаем распределение и выбираем действие\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        action = m.sample()\n",
    "        entropy = m.entropy()\n",
    "\n",
    "        # Сохраняем для обучения\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        self.state_values.append(state_value)\n",
    "        self.entropies.append(entropy)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Обновление Actor и Critic после эпизода\"\"\"\n",
    "        if not self.rewards:\n",
    "            return\n",
    "\n",
    "        # Вычисляем дисконтированные returns\n",
    "        R = 0\n",
    "        returns = []\n",
    "\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        # Вычисляем Advantage\n",
    "        state_values = torch.cat(self.state_values)  # Объединяем оценки Critic\n",
    "        # Advantage = Returns - Value\n",
    "        # Показывает, насколько returns лучше оценки Critic\n",
    "        advantages = returns - state_values.squeeze()\n",
    "\n",
    "        # Actor loss (Policy Gradient с advantage)\n",
    "        policy_loss = []\n",
    "        for log_prob, advantage in zip(self.saved_log_probs, advantages):\n",
    "            # Учитываем advantage вместо простого return\n",
    "            policy_loss.append(-log_prob * advantage.detach())\n",
    "\n",
    "        # Critic loss (MSE между предсказаниями и реальными returns)\n",
    "        value_loss = nn.MSELoss()(state_values.squeeze(), returns)\n",
    "\n",
    "        # Энтропийная регуляризация\n",
    "        entropy = torch.stack(self.entropies).sum()\n",
    "\n",
    "        # Общий loss: Actor loss + Critic loss + Энтропия\n",
    "        loss = torch.stack(policy_loss).sum() + 0.5 * value_loss - self.entropy_coef * entropy\n",
    "\n",
    "        # Градиентный спуск\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "\n",
    "        # Очищаем буферы\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.entropies = []\n",
    "\n",
    "class EnhancedPPOAgent:\n",
    "    \"\"\"\n",
    "    Улучшенный PPO (Proximal Policy Optimization) агент.\n",
    "\n",
    "    Улучшения:\n",
    "    - Меньший clip epsilon (0.1 вместо 0.2)\n",
    "    - Больше эпох (4 вместо 3)\n",
    "    - AdamW с weight decay\n",
    "    - Cosine annealing\n",
    "\n",
    "    PPO:\n",
    "    1. Clipped surrogate objective - предотвращает слишком большие изменения политики\n",
    "    2. Несколько эпох обучения на одних данных - эффективное использование данных\n",
    "    3. Энтропийная регуляризация\n",
    "    Стабильное обучение, минимизирует риск испортить политику\n",
    "    \"\"\"\n",
    "    def __init__(self, n_warehouses, learning_rate=0.0003, gamma=0.99,\n",
    "                 clip_epsilon=0.1, entropy_coef=0.02):\n",
    "        # Инициализируем сеть\n",
    "        self.actor_critic = EnhancedActorCriticNetwork(n_warehouses)\n",
    "\n",
    "        # Оптимизатор и scheduler\n",
    "        self.optimizer = optim.AdamW(self.actor_critic.parameters(),\n",
    "                                     lr=learning_rate, weight_decay=1e-4)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=1500)\n",
    "\n",
    "        # Гиперпараметры PPO\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon  # Параметр clipping (меньше = осторожнее)\n",
    "        self.entropy_coef = entropy_coef  # Сила энтропийной регуляризации\n",
    "\n",
    "        # Буфер для хранения траектории\n",
    "        self.memory = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Выбор действия с сохранением в memory для PPO\"\"\"\n",
    "        state_tensor = {\n",
    "            'current': torch.tensor([state['current']], dtype=torch.long),\n",
    "            'visited_mask': torch.tensor([state['visited_mask']], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "        # В режиме без градиентов (для скорости)\n",
    "        with torch.no_grad():\n",
    "            probs, state_value = self.actor_critic(state_tensor)\n",
    "            m = torch.distributions.Categorical(probs)\n",
    "            action = m.sample()\n",
    "            entropy = m.entropy()\n",
    "\n",
    "        # Сохраняем все данные шага для PPO updates\n",
    "        self.memory.append({\n",
    "            'state': state_tensor,\n",
    "            'action': action,\n",
    "            'log_prob': m.log_prob(action),\n",
    "            'value': state_value,\n",
    "            'entropy': entropy\n",
    "        })\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, episode_reward=0):\n",
    "        \"\"\"\n",
    "        Обновление политики PPO\n",
    "\n",
    "        PPO:\n",
    "        - Использует данные всего эпизода\n",
    "        - Делает несколько эпох обновления\n",
    "        - Применяет clipping для стабильности\n",
    "        \"\"\"\n",
    "        if not self.memory:\n",
    "            return\n",
    "\n",
    "        # Извлекаем данные из memory\n",
    "        states = [m['state'] for m in self.memory]\n",
    "        actions = torch.stack([m['action'] for m in self.memory])\n",
    "        old_log_probs = torch.stack([m['log_prob'] for m in self.memory])\n",
    "        old_values = torch.cat([m['value'] for m in self.memory])\n",
    "        old_entropies = torch.stack([m['entropy'] for m in self.memory])\n",
    "\n",
    "        # используем одну награду за эпизод\n",
    "        # Распределяем финальную награду по всем шагам\n",
    "        rewards = torch.tensor([episode_reward / len(self.memory)] * len(self.memory),\n",
    "                              dtype=torch.float32)\n",
    "\n",
    "        # Вычисляем дисконтированные returns\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in rewards.flip(0):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        # Вычисляем advantage\n",
    "        advantages = returns - old_values.squeeze()\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # несколько эпох обучения на одних данных\n",
    "        for _ in range(4):  # 4 эпохи вместо обычных 3\n",
    "            new_log_probs = []\n",
    "            new_values = []\n",
    "            new_entropies = []\n",
    "\n",
    "            # Пересчитываем для новых параметров\n",
    "            for i, state in enumerate(states):\n",
    "                probs, value = self.actor_critic(state)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                new_log_probs.append(dist.log_prob(actions[i]))\n",
    "                new_values.append(value)\n",
    "                new_entropies.append(dist.entropy())\n",
    "\n",
    "            new_log_probs = torch.stack(new_log_probs)\n",
    "            new_values = torch.cat(new_values)\n",
    "            new_entropies = torch.stack(new_entropies)\n",
    "\n",
    "            # Формула PPO: Clipped Surrogate Objective\n",
    "            ratio = (new_log_probs - old_log_probs).exp()  # Отношение новых и старых вероятностей\n",
    "            surr1 = ratio * advantages  # Обычный Policy Gradient\n",
    "            # Clipped вариант: ограничиваем изменение вероятностей\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "\n",
    "            # Берем минимум для предотвращения слишком больших шагов\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            # Critic loss (MSE)\n",
    "            value_loss = nn.MSELoss()(new_values.squeeze(), returns)\n",
    "\n",
    "            # Энтропийная регуляризация\n",
    "            entropy_bonus = -self.entropy_coef * new_entropies.mean()\n",
    "\n",
    "            # Общий loss\n",
    "            total_loss = policy_loss + 0.5 * value_loss + entropy_bonus\n",
    "\n",
    "            # Градиентный спуск\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Обновляем learning rate\n",
    "        self.scheduler.step()\n",
    "\n",
    "        # Очищаем memory\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbS8jegKPUVG"
   },
   "outputs": [],
   "source": [
    "class EnhancedSACAgent:\n",
    "    \"\"\"\n",
    "    Улучшение SAC (Soft Actor-Critic) агента\n",
    "\n",
    "    Улучшения:\n",
    "    1. Automatic entropy tuning (автоматическая настройка α):\n",
    "       - α адаптируется во время обучения\n",
    "       - Балансирует exploration и exploitation\n",
    "\n",
    "    2. Double critics (два критика):\n",
    "       - Две Q-сети для уменьшения переоценки\n",
    "       - Берем минимум для консервативных оценок\n",
    "\n",
    "    3. Target networks (таргет сети):\n",
    "       - Плавное обновление для стабильности\n",
    "       - Polyak averaging\n",
    "\n",
    "    4. Prioratized experience replay:\n",
    "       - Учимся быстрее на важных примерах\n",
    "       - Исправляем смещение sampling\n",
    "\n",
    "    5. Cosine anneling LR scheduler:\n",
    "       - Автоматическая настройка learning rate\n",
    "\n",
    "    6. Gradient clipping:\n",
    "       - Предотвращаем взрыв градиентов\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_warehouses, learning_rate=0.0003, gamma=0.99,\n",
    "                 tau=0.005, alpha=0.2, target_entropy=None,\n",
    "                 use_prioritized_replay=True):\n",
    "\n",
    "        # инициализация сетей\n",
    "        # Актор (политика)\n",
    "        self.actor = EnhancedPolicyNetwork(n_warehouses)\n",
    "\n",
    "        # Два критика (Q-сети)\n",
    "        self.critic1 = EnhancedSACNetwork(n_warehouses)\n",
    "        self.critic2 = EnhancedSACNetwork(n_warehouses)\n",
    "\n",
    "        # Target сети для стабильности\n",
    "        self.critic1_target = EnhancedSACNetwork(n_warehouses)\n",
    "        self.critic2_target = EnhancedSACNetwork(n_warehouses)\n",
    "\n",
    "        # Копируем веса в target сети\n",
    "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
    "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        # оптимизаторы\n",
    "        # AdamW с weight decay для лучшего обобщения\n",
    "        self.actor_optimizer = optim.AdamW(self.actor.parameters(),\n",
    "                                          lr=learning_rate, weight_decay=1e-4)\n",
    "        self.critic1_optimizer = optim.AdamW(self.critic1.parameters(),\n",
    "                                            lr=learning_rate, weight_decay=1e-4)\n",
    "        self.critic2_optimizer = optim.AdamW(self.critic2.parameters(),\n",
    "                                            lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "        # Cosine anneling scheduler\n",
    "        self.actor_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.actor_optimizer, T_max=1500\n",
    "        )\n",
    "        self.critic1_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.critic1_optimizer, T_max=1500\n",
    "        )\n",
    "        self.critic2_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.critic2_optimizer, T_max=1500\n",
    "        )\n",
    "\n",
    "        # Automatic entropy tuning\n",
    "        # Инициализируем log_alpha для автоматической настройки\n",
    "        self.target_entropy = target_entropy or -np.log(1.0 / n_warehouses) * 0.98\n",
    "        self.log_alpha = torch.tensor(np.log(alpha), requires_grad=True)\n",
    "        self.alpha_optimizer = optim.AdamW([self.log_alpha], lr=learning_rate)\n",
    "\n",
    "        # гиперпараметры\n",
    "        self.gamma = gamma  # Коэффициент дисконтирования\n",
    "        self.tau = tau      # Параметр плавного обновления target сетей\n",
    "        self.n_warehouses = n_warehouses\n",
    "\n",
    "        # replay buffer\n",
    "        self.use_prioritized_replay = use_prioritized_replay\n",
    "\n",
    "        if use_prioritized_replay:\n",
    "            # Приоритизированный replay buffer\n",
    "            self.memory = PrioritizedReplayBuffer(capacity=10000, alpha=0.6)\n",
    "            self.beta = 0.4  # Параметр для корректировки смещения\n",
    "            self.beta_increment = 0.001  # Увеличение beta со временем\n",
    "        else:\n",
    "            # Обычный replay buffer\n",
    "            self.memory = deque(maxlen=10000)\n",
    "\n",
    "        self.batch_size = 256  # Больше batch size для стабильности\n",
    "\n",
    "        # статистика\n",
    "        self.update_step = 0\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.alpha_values = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Выбор действия с использованием стохастической политики:\n",
    "        1. Получаем вероятности действий из актора\n",
    "        2. Добавляем шум через энтропийную регуляризацию\n",
    "        3. Выбираем действие согласно распределению\n",
    "        \"\"\"\n",
    "        state_tensor = {\n",
    "            'current': torch.tensor([state['current']], dtype=torch.long),\n",
    "            'visited_mask': torch.tensor([state['visited_mask']], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "        # Получаем вероятности действий\n",
    "        probs = self.actor(state_tensor)\n",
    "\n",
    "        # Создаем категориальное распределение\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        # Также вычисляем log вероятности для обучения\n",
    "        log_prob = m.log_prob(action)\n",
    "\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Сохранение опыта в Replay buffer\n",
    "        Для приоритизированного replay buffer храним также TD ошибку как приоритет\n",
    "        \"\"\"\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "\n",
    "        if self.use_prioritized_replay:\n",
    "            # Инициализируем с максимальным приоритетом\n",
    "            max_priority = 1.0 if len(self.memory) == 0 else self.memory.max_priority()\n",
    "            self.memory.add(experience, max_priority)\n",
    "        else:\n",
    "            self.memory.append(experience)\n",
    "\n",
    "    def _soft_update(self, target, source):\n",
    "        \"\"\"\n",
    "        Плавное обновление target сетей\n",
    "\n",
    "        Формула: θ_target = τ * θ_source + (1 - τ) * θ_target\n",
    "\n",
    "        Почему это важно:\n",
    "        - Предотвращает резкие изменения в target значениях\n",
    "        - Стабилизирует обучение Q-сетей\n",
    "        \"\"\"\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Обновление SAC\n",
    "        \"\"\"\n",
    "\n",
    "        # Проверяем, достаточно ли данных в buffer\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sampling из Replay Buffer\n",
    "        if self.use_prioritized_replay:\n",
    "            batch, indices, weights = self.memory.sample(self.batch_size, self.beta)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "            # Увеличиваем beta\n",
    "            self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "        else:\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            weights = torch.ones(self.batch_size)\n",
    "\n",
    "        # Подготовка тензоров\n",
    "        state_current = torch.tensor([s['current'] for s in states], dtype=torch.long)\n",
    "        state_visited = torch.tensor([s['visited_mask'] for s in states], dtype=torch.float32)\n",
    "\n",
    "        next_state_current = torch.tensor([s['current'] for s in next_states], dtype=torch.long)\n",
    "        next_state_visited = torch.tensor([s['visited_mask'] for s in next_states], dtype=torch.float32)\n",
    "\n",
    "        state_tensors = {'current': state_current, 'visited_mask': state_visited}\n",
    "        next_state_tensors = {'current': next_state_current, 'visited_mask': next_state_visited}\n",
    "\n",
    "        actions_tensor = torch.tensor(actions)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Обновлеине критика\n",
    "        with torch.no_grad():\n",
    "            # Получаем политику для следующего состояния\n",
    "            next_probs = self.actor(next_state_tensors)\n",
    "            next_log_probs = torch.log(next_probs + 1e-8)  # Добавляем epsilon для стабильности\n",
    "\n",
    "            # Получаем Q-значения от target критиков\n",
    "            next_q1_target = self.critic1_target(next_state_tensors)\n",
    "            next_q2_target = self.critic2_target(next_state_tensors)\n",
    "            next_q_target = torch.min(next_q1_target, next_q2_target)\n",
    "\n",
    "            # Вычисляем значение следующего состояния с учетом энтропии\n",
    "            alpha = self.log_alpha.exp()\n",
    "            next_value = (next_probs * (next_q_target - alpha * next_log_probs)).sum(dim=1)\n",
    "\n",
    "            # Target Q-значения: r + γ * (1 - done) * V(s')\n",
    "            target_q = rewards_tensor + self.gamma * next_value * (1 - dones_tensor)\n",
    "\n",
    "        # Текущие Q-значения\n",
    "        current_q1 = self.critic1(state_tensors)\n",
    "        current_q2 = self.critic2(state_tensors)\n",
    "\n",
    "        current_q1 = current_q1[torch.arange(self.batch_size), actions_tensor]\n",
    "        current_q2 = current_q2[torch.arange(self.batch_size), actions_tensor]\n",
    "\n",
    "        # Потери для критиков с учетом весов\n",
    "        critic1_loss = (weights * (current_q1 - target_q).pow(2)).mean()\n",
    "        critic2_loss = (weights * (current_q2 - target_q).pow(2)).mean()\n",
    "\n",
    "        # Обновляем критиков\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), max_norm=1.0)\n",
    "        self.critic1_optimizer.step()\n",
    "\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), max_norm=1.0)\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "        # Обновление актора\n",
    "        # Пересчитываем Q-значения с обновленными критикаим\n",
    "        with torch.no_grad():\n",
    "            q1 = self.critic1(state_tensors)\n",
    "            q2 = self.critic2(state_tensors)\n",
    "            q = torch.min(q1, q2)  # Берем минимум для консервативности\n",
    "\n",
    "        # Получаем текущие вероятности действий\n",
    "        probs = self.actor(state_tensors)\n",
    "        log_probs = torch.log(probs + 1e-8)\n",
    "\n",
    "        # Вычисляем потерю актора\n",
    "        alpha = self.log_alpha.exp().detach()\n",
    "        actor_loss = (probs * (alpha * log_probs - q)).sum(dim=1).mean()\n",
    "\n",
    "        # Обновляем актора\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Обновление температуры параметра α\n",
    "        # Вычисляем потерю для α\n",
    "        with torch.no_grad():\n",
    "            # Текущая энтропия политики\n",
    "            current_entropy = -(probs * log_probs).sum(dim=1).mean()\n",
    "\n",
    "        # Целевая функция для α\n",
    "        alpha_loss = -(self.log_alpha * (current_entropy - self.target_entropy).detach()).mean()\n",
    "\n",
    "        # Обновляем α\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "\n",
    "        # Обновление target сетей\n",
    "        self._soft_update(self.critic1_target, self.critic1)\n",
    "        self._soft_update(self.critic2_target, self.critic2)\n",
    "\n",
    "        # Обновление приоритетов (если используется PER)\n",
    "        if self.use_prioritized_replay:\n",
    "            # Вычисляем TD ошибки для обновления приоритетов\n",
    "            with torch.no_grad():\n",
    "                td_errors = torch.abs(target_q - current_q1).detach().numpy()\n",
    "            self.memory.update_priorities(indices, td_errors)\n",
    "\n",
    "        # Обновлеине learning rate schedulre\n",
    "        self.actor_scheduler.step()\n",
    "        self.critic1_scheduler.step()\n",
    "        self.critic2_scheduler.step()\n",
    "\n",
    "        # Сохранение статистики\n",
    "        self.update_step += 1\n",
    "        self.actor_losses.append(actor_loss.item())\n",
    "        self.critic_losses.append((critic1_loss.item() + critic2_loss.item()) / 2)\n",
    "        self.alpha_values.append(self.log_alpha.exp().item())\n",
    "\n",
    "        return {\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'critic_loss': (critic1_loss.item() + critic2_loss.item()) / 2,\n",
    "            'alpha': self.log_alpha.exp().item(),\n",
    "            'entropy': current_entropy.item()\n",
    "        }\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"\n",
    "    Приоритизированный Replay Buffer:\n",
    "    1. Хранит опыт с приоритетами (TD ошибками)\n",
    "    2. Выбирает важные примеры для обучения чаще\n",
    "    3. Корректирует смещение через importance sampling weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity=10000, alpha=0.6, beta=0.4, beta_increment=0.001):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # Параметр приоритета (0 = равномерный, 1 = только приоритеты)\n",
    "        self.beta = beta    # Параметр importance sampling\n",
    "        self.beta_increment = beta_increment\n",
    "\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "\n",
    "        # Минимальный приоритет для новых примеров\n",
    "        self.min_priority = 1.0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Возвращает текущий размер buffer\"\"\"\n",
    "        return self.size\n",
    "\n",
    "    def add(self, experience, priority=None):\n",
    "        \"\"\"\n",
    "        Добавление опыта в buffer\n",
    "        \"\"\"\n",
    "        if priority is None:\n",
    "            priority = self.max_priority()\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.position] = experience\n",
    "\n",
    "        # Сохраняем приоритет\n",
    "        self.priorities[self.position] = priority ** self.alpha\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size, beta=None):\n",
    "        \"\"\"\n",
    "        Выборка batch с учетом приоритетов\n",
    "        \"\"\"\n",
    "        if beta is None:\n",
    "            beta = self.beta\n",
    "\n",
    "        if self.size == 0:\n",
    "            return [], [], []\n",
    "\n",
    "        # Нормализуем приоритеты для получения вероятностей\n",
    "        probs = self.priorities[:self.size] / (self.priorities[:self.size].sum() + 1e-8)\n",
    "\n",
    "        # Выбираем индексы согласно вероятностям\n",
    "        indices = np.random.choice(self.size, min(batch_size, self.size), p=probs, replace=False)\n",
    "\n",
    "        # Вычисляем importance sampling weights\n",
    "        weights = (self.size * probs[indices]) ** (-beta)\n",
    "        weights = weights / (weights.max() + 1e-8)  # Нормализуем\n",
    "\n",
    "        # Собираем batch\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "\n",
    "        return batch, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"\n",
    "        Обновление приоритетов для выбранных примеров\n",
    "        \"\"\"\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = (priority + 1e-5) ** self.alpha  # Добавляем epsilon\n",
    "\n",
    "    def max_priority(self):\n",
    "        \"\"\"\n",
    "        Возвращает максимальный приоритет в buffer\n",
    "        \"\"\"\n",
    "        if self.size == 0:\n",
    "            return 1.0\n",
    "        return self.priorities[:self.size].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmDE4fgYil_e"
   },
   "outputs": [],
   "source": [
    "class DuelingQNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling DQN сеть:\n",
    "    1. Разделяет оценку на Value и Advantage потоки\n",
    "    2. Value stream: оценивает общую ценность состояния\n",
    "    3. Advantage stream: оценивает преимущество каждого действия\n",
    "\n",
    "    Формула: Q(s,a) = V(s) + A(s,a) - mean(A(s,a))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_warehouses, hidden_size=256):\n",
    "        super(DuelingQNetwork, self).__init__()\n",
    "        self.n_warehouses = n_warehouses\n",
    "\n",
    "        # Общий энкодер\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_warehouses * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Value stream: оценивает общую ценность состояния\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)  # Одно значение на состояние\n",
    "        )\n",
    "\n",
    "        # Advantage stream: оценивает преимущество каждого действия\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, n_warehouses)  # Одно значение на каждое действие\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Инициализация весов\"\"\"\n",
    "        for module in [self.encoder, self.value_stream, self.advantage_stream]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.constant_(layer.bias, 0.01)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Прямой проход через Dueling сеть\"\"\"\n",
    "        current = state['current']\n",
    "        visited_mask = state['visited_mask']\n",
    "\n",
    "        # One-hot кодирование текущего склада\n",
    "        current_onehot = torch.zeros(current.shape[0], self.n_warehouses, dtype=torch.float32)\n",
    "        for i, c in enumerate(current):\n",
    "            current_onehot[i, c] = 1.0\n",
    "\n",
    "        # Объединяем входные данные\n",
    "        x = torch.cat([current_onehot, visited_mask], dim=1)\n",
    "\n",
    "        # Извлекаем признаки через энкодер\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Вычисляем value и advantage\n",
    "        value = self.value_stream(features)  # [batch_size, 1]\n",
    "        advantage = self.advantage_stream(features)  # [batch_size, n_actions]\n",
    "\n",
    "        #  Dueling формула: Q(s,a) = V(s) + A(s,a) - mean(A(s,a))\n",
    "        # Это помогает стабилизировать обучение\n",
    "        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "\n",
    "        # Маскируем посещенные склады\n",
    "        q_values[visited_mask.bool()] = -1e8\n",
    "\n",
    "        return q_values\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Noisy linear слой:\n",
    "    1. Добавляет параметризованный шум к весам и смещениям\n",
    "    2. Заменяет ε-greedy exploration\n",
    "    3. Позволяет сети исследовать самостоятельно\n",
    "\n",
    "    Используется в Noisy DQN для улучшенного exploration\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, std_init=0.4):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        # Весовые параметры\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.Tensor(out_features, in_features))\n",
    "\n",
    "        # Смещения\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.Tensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Инициализация параметров\"\"\"\n",
    "        mu_range = 1 / np.sqrt(self.in_features)\n",
    "\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / np.sqrt(self.in_features))\n",
    "\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / np.sqrt(self.out_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Сброс шума\"\"\"\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        \"\"\"Генерация шума\"\"\"\n",
    "        x = torch.randn(size)\n",
    "        return x.sign().mul(x.abs().sqrt())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Прямой проход с шумом\"\"\"\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "\n",
    "        return nn.functional.linear(x, weight, bias)\n",
    "\n",
    "class NoisyDuelingQNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    NoisyDuealing DQN сеть\n",
    "\n",
    "    Особенности:\n",
    "    1. Dueling Architecture (Value + Advantage streams)\n",
    "    2. Noisy Layers для exploration\n",
    "    3. Не требует ε-greedy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_warehouses, hidden_size=256):\n",
    "        super(NoisyDuelingQNetwork, self).__init__()\n",
    "        self.n_warehouses = n_warehouses\n",
    "\n",
    "        # Энкодер С noisy layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            NoisyLinear(n_warehouses * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Value stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            NoisyLinear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "        # Advantage stream\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            NoisyLinear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(hidden_size // 2, n_warehouses)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        current = state['current']\n",
    "        visited_mask = state['visited_mask']\n",
    "\n",
    "        current_onehot = torch.zeros(current.shape[0], self.n_warehouses, dtype=torch.float32)\n",
    "        for i, c in enumerate(current):\n",
    "            current_onehot[i, c] = 1.0\n",
    "\n",
    "        x = torch.cat([current_onehot, visited_mask], dim=1)\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "\n",
    "        # Dueling формула\n",
    "        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "\n",
    "        # Маскируем посещенные\n",
    "        q_values[visited_mask.bool()] = -1e8\n",
    "\n",
    "        return q_values\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Сброс шума во всех Noisy слоях\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, NoisyLinear):\n",
    "                module.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nQ_apSJoxi5"
   },
   "outputs": [],
   "source": [
    "class ImprovedDQNAgent:\n",
    "    \"\"\"\n",
    "    Улучшенный DQN агент\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_warehouses, learning_rate=0.0003, gamma=0.99,\n",
    "                 tau=0.005, use_per=True, use_noisy=True, n_step=3):\n",
    "        \"\"\"\n",
    "        Инициализация улучшенного DQN\n",
    "        \"\"\"\n",
    "\n",
    "        # Выбираем тип сети\n",
    "        if use_noisy:\n",
    "            self.q_network = NoisyDuelingQNetwork(n_warehouses)\n",
    "            self.target_network = NoisyDuelingQNetwork(n_warehouses)\n",
    "        else:\n",
    "            self.q_network = DuelingQNetwork(n_warehouses)\n",
    "            self.target_network = DuelingQNetwork(n_warehouses)\n",
    "\n",
    "        # Копируем веса в target сеть\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        # Оптимизатор И Scheduler\n",
    "        self.optimizer = optim.AdamW(self.q_network.parameters(),\n",
    "                                    lr=learning_rate, weight_decay=1e-4)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=1500)\n",
    "\n",
    "        # Гиперпараметры\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.n_step = n_step\n",
    "        self.use_noisy = use_noisy\n",
    "        self.n_warehouses = n_warehouses\n",
    "\n",
    "        # Replay buffer\n",
    "        self.use_per = use_per\n",
    "        if use_per:\n",
    "            self.memory = PrioritizedReplayBuffer(capacity=10000, alpha=0.6)\n",
    "            self.beta = 0.4\n",
    "            self.beta_increment = 0.001\n",
    "        else:\n",
    "            self.memory = deque(maxlen=10000)\n",
    "            self.n_step_buffer = deque(maxlen=n_step)  # Для N-step returns\n",
    "\n",
    "        self.batch_size = 128\n",
    "\n",
    "        # Статистика\n",
    "        self.update_step = 0\n",
    "        self.losses = []\n",
    "\n",
    "        # Для Noisy Networks не нужен ε-greedy\n",
    "        if use_noisy:\n",
    "            self.epsilon = 0.0\n",
    "        else:\n",
    "            self.epsilon = 1.0\n",
    "            self.epsilon_decay = 0.995\n",
    "            self.epsilon_min = 0.01\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Выбор действия\n",
    "\n",
    "        Если используем Noisy Networks:\n",
    "        - Шум уже встроен в веса сети\n",
    "        - Не нужен ε-greedy\n",
    "\n",
    "        Если используем обычный DQN:\n",
    "        - Используем ε-greedy для exploration\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.use_noisy and random.random() < self.epsilon:\n",
    "            # ε-greedy: случайное действие\n",
    "            visited_mask = state['visited_mask']\n",
    "            valid_actions = [i for i in range(len(visited_mask)) if visited_mask[i] == 0.0]\n",
    "            return random.choice(valid_actions) if valid_actions else 0\n",
    "\n",
    "        # Жадное действие на основе Q-значений\n",
    "        state_tensor = {\n",
    "            'current': torch.tensor([state['current']], dtype=torch.long),\n",
    "            'visited_mask': torch.tensor([state['visited_mask']], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Сохранение опыта в Replay buffer\n",
    "        \"\"\"\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "\n",
    "        if self.use_per:\n",
    "            # Для PER вычисляем TD ошибку для приоритета\n",
    "            state_tensor = {\n",
    "                'current': torch.tensor([state['current']], dtype=torch.long),\n",
    "                'visited_mask': torch.tensor([state['visited_mask']], dtype=torch.float32)\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                current_q = self.q_network(state_tensor)[0, action]\n",
    "\n",
    "                next_state_tensor = {\n",
    "                    'current': torch.tensor([next_state['current']], dtype=torch.long),\n",
    "                    'visited_mask': torch.tensor([next_state['visited_mask']], dtype=torch.float32)\n",
    "                }\n",
    "\n",
    "                # Double DQN: online сеть выбирает, target сеть оценивает\n",
    "                next_action = self.q_network(next_state_tensor).argmax().item()\n",
    "                next_q = self.target_network(next_state_tensor)[0, next_action]\n",
    "\n",
    "                target_q = reward + self.gamma * next_q * (1 - done)\n",
    "                td_error = abs(target_q - current_q).item()\n",
    "\n",
    "            # Сохраняем с приоритетом\n",
    "            self.memory.add(experience, td_error)\n",
    "        else:\n",
    "            # Обычное сохранение\n",
    "            self.memory.append(experience)\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Обновление сети\n",
    "        \"\"\"\n",
    "\n",
    "        # Проверяем, достаточно ли данных в buffer\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sampling\n",
    "        if self.use_per:\n",
    "            batch, indices, weights = self.memory.sample(self.batch_size, self.beta)\n",
    "\n",
    "            # Проверяем, что получили достаточное количество примеров\n",
    "            if len(batch) < self.batch_size:\n",
    "                return\n",
    "\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "            # Увеличиваем beta\n",
    "            self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "        else:\n",
    "            batch = random.sample(list(self.memory), min(self.batch_size, len(self.memory)))\n",
    "\n",
    "            if len(batch) < self.batch_size:\n",
    "                return\n",
    "\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            weights = torch.ones(len(batch))\n",
    "\n",
    "        # Подготовка тензоров\n",
    "        state_current = torch.tensor([s['current'] for s in states], dtype=torch.long)\n",
    "        state_visited = torch.tensor([s['visited_mask'] for s in states], dtype=torch.float32)\n",
    "\n",
    "        next_state_current = torch.tensor([s['current'] for s in next_states], dtype=torch.long)\n",
    "        next_state_visited = torch.tensor([s['visited_mask'] for s in next_states], dtype=torch.float32)\n",
    "\n",
    "        state_tensors = {'current': state_current, 'visited_mask': state_visited}\n",
    "        next_state_tensors = {'current': next_state_current, 'visited_mask': next_state_visited}\n",
    "\n",
    "        actions_tensor = torch.tensor(actions)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Вычисление target Q-значений (Double DQN)\n",
    "        with torch.no_grad():\n",
    "            # Online сеть выбирает действия\n",
    "            next_actions = self.q_network(next_state_tensors).argmax(1)\n",
    "\n",
    "            # Target сеть оценивает выбранные действия\n",
    "            next_q_values = self.target_network(next_state_tensors)\n",
    "            next_q_values = next_q_values[torch.arange(len(batch)), next_actions]\n",
    "\n",
    "            # Вычисляем target Q-значения\n",
    "            target_q = rewards_tensor + self.gamma * next_q_values * (1 - dones_tensor)\n",
    "\n",
    "        # Вычисление текущих Q-значений\n",
    "        current_q_values = self.q_network(state_tensors)\n",
    "        current_q_values = current_q_values[torch.arange(len(batch)), actions_tensor]\n",
    "\n",
    "        # Вычисление потери\n",
    "        td_errors = target_q - current_q_values\n",
    "\n",
    "        if self.use_per:\n",
    "            # Для PER: взвешенная MSE\n",
    "            loss = (weights * td_errors.pow(2)).mean()\n",
    "        else:\n",
    "            # Обычная MSE\n",
    "            loss = td_errors.pow(2).mean()\n",
    "\n",
    "        # Градиентный спуск\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping для стабильности\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "\n",
    "        # Обновление Target Сети (Soft update)\n",
    "        self._soft_update_target_network()\n",
    "\n",
    "        # Обновление приоритетов (для PER)\n",
    "        if self.use_per:\n",
    "            td_errors_np = td_errors.detach().abs().numpy()\n",
    "            self.memory.update_priorities(indices, td_errors_np)\n",
    "\n",
    "        # Сброс шума (для noisy network)\n",
    "        if self.use_noisy and hasattr(self.q_network, 'reset_noise'):\n",
    "            self.q_network.reset_noise()\n",
    "            self.target_network.reset_noise()\n",
    "\n",
    "        # Decay ε (для обыченого DQN)\n",
    "        if not self.use_noisy:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        # Сохраниение статистики\n",
    "        self.update_step += 1\n",
    "        self.losses.append(loss.item())\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def _soft_update_target_network(self):\n",
    "        \"\"\"\n",
    "        Плавное обновление target сети\n",
    "        θ_target = τ * θ_online + (1 - τ) * θ_target\n",
    "        \"\"\"\n",
    "        for target_param, param in zip(self.target_network.parameters(),\n",
    "                                      self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BP6WxWdTpW1B"
   },
   "outputs": [],
   "source": [
    "class ImprovedDoubleDQNAgent(ImprovedDQNAgent):\n",
    "    \"\"\"\n",
    "    Улучшенный DOUBLE DQN aгент\n",
    "\n",
    "    Наследует от ImprovedDQNAgent, но переопределяет:\n",
    "    1. Более консервативный target update\n",
    "    2. Дополнительные техники для стабильности\n",
    "\n",
    "    Отличие от ImprovedDQNAgent:\n",
    "    - Более частая инициализация target сети\n",
    "    - Больший batch size\n",
    "    - Дополнительная регуляризация\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_warehouses, learning_rate=0.0003, gamma=0.99,\n",
    "                 tau=0.001, use_per=True, use_noisy=True, n_step=3):\n",
    "        super().__init__(n_warehouses, learning_rate, gamma, tau, use_per, use_noisy, n_step)\n",
    "\n",
    "        # Больше regularization для DOUBLE DQN\n",
    "        self.batch_size = 256  # Больше batch size для стабильности\n",
    "        self.tau = tau  # Меньше tau для более медленного обновления\n",
    "\n",
    "        # дополнительная regularization\n",
    "        self.gradient_clip = 0.5  # Меньше clipping\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Обновление DOUBLE DQN С дополнительными мерами:\n",
    "        1. Более частый hard update target сети\n",
    "        2. Дополнительная регуляризация\n",
    "        3. Более консервативное обучение\n",
    "        \"\"\"\n",
    "\n",
    "        loss = super().update()\n",
    "\n",
    "        # Переодически hard update\n",
    "        # Каждые 1000 шагов полностью копируем веса\n",
    "        if self.update_step % 1000 == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7aoiZavRm2W"
   },
   "outputs": [],
   "source": [
    "def train_all_rl_methods(distance_matrix, warehouses_df, num_episodes=1500):\n",
    "    \"\"\"\n",
    "    Обучение RL методов\n",
    "    \"\"\"\n",
    "\n",
    "    env = TSPEnvironment(distance_matrix, warehouses_df)\n",
    "    n_warehouses = len(warehouses_df)\n",
    "\n",
    "    agents = {\n",
    "        # Policy Gradient методы\n",
    "        'Policy Gradient (базовый)': PolicyGradientAgent(n_warehouses),\n",
    "        'Policy Gradient (улучшенный)': EnhancedPolicyGradientAgent(n_warehouses),\n",
    "\n",
    "        # Actor-Critic методы\n",
    "        'A2C (базовый)': A2CAgent(n_warehouses),\n",
    "        'A2C (улучшенный)': EnhancedA2CAgent(n_warehouses),\n",
    "\n",
    "        # PPO методы\n",
    "        'PPO (базовый)': PPOAgent(n_warehouses),\n",
    "        'PPO (улучшенный)': EnhancedPPOAgent(n_warehouses),\n",
    "\n",
    "        # DQN методы\n",
    "        'DQN (базовый)': DQNAgent(n_warehouses),\n",
    "        'Double DQN (базовый)': DoubleDQNAgent(n_warehouses),\n",
    "        'DQN (улучшенный)': ImprovedDQNAgent(n_warehouses, use_per=True, use_noisy=False),\n",
    "        'Double DQN (улучшенный)': ImprovedDoubleDQNAgent(n_warehouses, use_per=True, use_noisy=False),\n",
    "\n",
    "        # SAC методы\n",
    "        'SAC (базовый)': SACAgent(n_warehouses),\n",
    "        'SAC (улучшенный)': EnhancedSACAgent(n_warehouses),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    print(f\"Начинаем обучение {len(agents)} RL методов\")\n",
    "    print(f\"Количество складов: {n_warehouses}\")\n",
    "    print(f\"Количество эпизодов на метод: {num_episodes}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Обучаем каждого агента\n",
    "    for agent_name, agent in agents.items():\n",
    "        print(f\"\\nОбучение: {agent_name}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        episode_rewards = []\n",
    "        episode_distances = []\n",
    "        best_route = None\n",
    "        best_distance = float('inf')\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0.0\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            max_steps = n_warehouses * 2\n",
    "\n",
    "            while not done and step_count < max_steps:\n",
    "                # Выбор действия\n",
    "                if 'SAC' in agent_name:\n",
    "                    # SAC возвращает и действие, и log_prob\n",
    "                    action_result = agent.select_action(state)\n",
    "                    if isinstance(action_result, tuple):\n",
    "                        action = action_result[0]  # Берем только действие\n",
    "                    else:\n",
    "                        action = action_result\n",
    "                else:\n",
    "                    action = agent.select_action(state)\n",
    "\n",
    "                # Шаг в среде\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "                # Сохраняем опыт (для методов с replay buffer)\n",
    "                if hasattr(agent, 'store_experience'):\n",
    "                    agent.store_experience(state, action, reward, next_state, done)\n",
    "\n",
    "                # Сохраняем награды (для on-policy методов)\n",
    "                if hasattr(agent, 'rewards'):\n",
    "                    agent.rewards.append(reward)\n",
    "\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "\n",
    "            # Обновление политики\n",
    "            # Для PPO обновляем с финальной наградой\n",
    "            if 'PPO' in agent_name:\n",
    "                agent.update(episode_reward)\n",
    "            # Для остальных on-policy методов обновляем каждые 10 эпизодов\n",
    "            elif hasattr(agent, 'update') and not hasattr(agent, 'store_experience'):\n",
    "                if episode % 10 == 0 or done:\n",
    "                    agent.update()\n",
    "            # Для off-policy методов обновляем всегда\n",
    "            elif hasattr(agent, 'update'):\n",
    "                agent.update()\n",
    "\n",
    "            # Статистика\n",
    "            route_info = env.get_route_info()\n",
    "            current_distance = route_info['distance_km']\n",
    "\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_distances.append(current_distance)\n",
    "\n",
    "            if current_distance < best_distance and len(env.visited) == n_warehouses:\n",
    "                best_distance = current_distance\n",
    "                best_route = route_info['route']\n",
    "\n",
    "            # Прогресс\n",
    "            if episode % 100 == 0:\n",
    "                visited_count = len(env.visited)\n",
    "                epsilon = getattr(agent, 'epsilon', 0)\n",
    "                print(f\"  Эпизод {episode:4d}: {current_distance:7.2f} км, \"\n",
    "                      f\"ε={epsilon:.3f}, Посещено: {visited_count}/{n_warehouses}\")\n",
    "\n",
    "        # Результаты\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "\n",
    "        results[agent_name] = {\n",
    "            'best_distance': best_distance,\n",
    "            'best_route': best_route,\n",
    "            'avg_last_100': np.mean(episode_distances[-100:]) if episode_distances else float('inf'),\n",
    "            'std_last_100': np.std(episode_distances[-100:]) if len(episode_distances) >= 100 else 0,\n",
    "            'training_time_seconds': training_time,\n",
    "            'training_time_minutes': training_time / 60,\n",
    "            'episode_distances': episode_distances,\n",
    "            'episode_rewards': episode_rewards\n",
    "        }\n",
    "\n",
    "        print(f\"\\n{agent_name} завершен\")\n",
    "        print(f\"   Лучший результат: {best_distance:.2f} км\")\n",
    "        print(f\"   Среднее (последние 100): {results[agent_name]['avg_last_100']:.2f} км\")\n",
    "        print(f\"   Время обучения: {training_time:.2f} секунд\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "De074fTpDK59"
   },
   "outputs": [],
   "source": [
    "distance_matrix, warehouses_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yOFzDisSl4j5",
    "outputId": "d1e77891-7159-4928-e4a5-ce90a1c94ad8"
   },
   "outputs": [],
   "source": [
    "print(f\"Загружено складов: {len(warehouses_df)}\")\n",
    "print(f\"Размер матрицы расстояний: {distance_matrix.shape}\")\n",
    "\n",
    "print(\"Количество эпизодов на метод: 1500\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Запускаем обучение всех методов\n",
    "all_results = train_all_rl_methods(distance_matrix, warehouses_df, num_episodes=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "JF2AIfKdmkW7",
    "outputId": "1e598b0b-2000-4948-ab71-d8ec2a82c794"
   },
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "for agent_name, result in all_results.items():\n",
    "    summary_data.append({\n",
    "        'Метод': agent_name,\n",
    "        'Лучший результат (км)': f\"{result['best_distance']:.2f}\",\n",
    "        'Среднее последние 100 (км)': f\"{result['avg_last_100']:.2f}\",\n",
    "        'Станд. отклонение': f\"{result['std_last_100']:.2f}\",\n",
    "        'Время обучения (мин)': f\"{result['training_time_minutes']:.2f}\",\n",
    "        'Время на эпизод (сек)': f\"{result['training_time_seconds']/1500:.3f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Сортируем по лучшему результату\n",
    "summary_df = summary_df.sort_values(by='Лучший результат (км)', key=lambda x: x.str.replace(' км', '').astype(float))\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "id": "bwVeLjl9G8V2",
    "outputId": "cc883064-2138-4ad3-91e4-671db4a1ec8d"
   },
   "outputs": [],
   "source": [
    "def plot_comprehensive_results(results):\n",
    "    \"\"\"Комплексная визуализация всех результатов\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "    # График 1: Лучшие результаты (горизонтальные столбцы)\n",
    "    methods = list(results.keys())\n",
    "    best_distances = [results[name]['best_distance'] for name in methods]\n",
    "\n",
    "    colors = []\n",
    "    for name in methods:\n",
    "        if 'улучшенный' in name:\n",
    "            colors.append('green')\n",
    "        elif 'базовый' in name:\n",
    "            colors.append('red')\n",
    "        else:\n",
    "            colors.append('blue')\n",
    "\n",
    "    bars = ax1.barh(methods, best_distances, color=colors, alpha=0.7)\n",
    "    ax1.set_xlabel('Лучшее расстояние (км)')\n",
    "    ax1.set_title('Лучшие результаты всех методов\\n(чем меньше, тем лучше)', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    for bar, distance in zip(bars, best_distances):\n",
    "        width = bar.get_width()\n",
    "        ax1.text(width + max(best_distances)*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{distance:.1f} км', ha='left', va='center', fontsize=9)\n",
    "\n",
    "    # График 2: Время обучения в секундах\n",
    "    training_times = [results[name]['training_time_seconds'] for name in methods]\n",
    "    bars2 = ax2.bar(methods, training_times, color='orange', alpha=0.7)\n",
    "    ax2.set_ylabel('Время обучения (секунды)', fontsize=12)\n",
    "    ax2.set_title('Время обучения методов', fontsize=14, fontweight='bold')\n",
    "    ax2.tick_params(axis='x', rotation=90, labelsize=9)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Добавляем значения на столбцы\n",
    "    for bar, time_val in zip(bars2, training_times):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + max(training_times)*0.01,\n",
    "                f'{time_val:.1f} сек', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    # График 3: Скорость сходимости (время до лучшего результата)\n",
    "    # Считаем эпизод, на котором был достигнут лучший результат\n",
    "    convergence_data = []\n",
    "    for name in methods:\n",
    "        episode_distances = results[name]['episode_distances']\n",
    "        if episode_distances:\n",
    "            best_dist = results[name]['best_distance']\n",
    "            # Находим первый эпизод с таким результатом\n",
    "            convergence_episode = None\n",
    "            for i, dist in enumerate(episode_distances):\n",
    "                if abs(dist - best_dist) < 0.1:  # Учитываем погрешность\n",
    "                    convergence_episode = i\n",
    "                    break\n",
    "            if convergence_episode is not None:\n",
    "                convergence_data.append(convergence_episode)\n",
    "            else:\n",
    "                convergence_data.append(len(episode_distances))\n",
    "        else:\n",
    "            convergence_data.append(0)\n",
    "\n",
    "    bars3 = ax3.bar(methods, convergence_data, color='purple', alpha=0.7)\n",
    "    ax3.set_ylabel('Эпизод сходимости', fontsize=12)\n",
    "    ax3.set_title('Скорость сходимости методов\\n(чем меньше, тем быстрее)', fontsize=14, fontweight='bold')\n",
    "    ax3.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Добавляем значения\n",
    "    for bar, conv_ep in zip(bars3, convergence_data):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + max(convergence_data)*0.01,\n",
    "                f'{conv_ep}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    # График 4: Сравнение базовых и улучшенных версий\n",
    "    base_methods = [name for name in methods if 'базовый' in name]\n",
    "\n",
    "    base_distances = []\n",
    "    enhanced_distances = []\n",
    "    comparison_labels = []\n",
    "\n",
    "    for base_name in base_methods:\n",
    "        enhanced_name = base_name.replace(' (базовый)', ' (улучшенный)')\n",
    "        if enhanced_name in results:\n",
    "            base_distances.append(results[base_name]['best_distance'])\n",
    "            enhanced_distances.append(results[enhanced_name]['best_distance'])\n",
    "            comparison_labels.append(base_name.replace(' (базовый)', ''))\n",
    "\n",
    "    if base_distances:\n",
    "        x = np.arange(len(comparison_labels))\n",
    "        width = 0.35\n",
    "\n",
    "        bars_base = ax4.bar(x - width/2, base_distances, width,\n",
    "                           label='Базовая версия', alpha=0.7, color='red')\n",
    "        bars_enhanced = ax4.bar(x + width/2, enhanced_distances, width,\n",
    "                               label='Улучшенная версия', alpha=0.7, color='green')\n",
    "\n",
    "        ax4.set_ylabel('Лучший результат (км)', fontsize=12)\n",
    "        ax4.set_title('Сравнение базовых и улучшенных версий', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xticks(x)\n",
    "        ax4.set_xticklabels(comparison_labels, rotation=45, ha='right', fontsize=9)\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # Добавляем процент улучшения\n",
    "        for i, (base_dist, enhanced_dist) in enumerate(zip(base_distances, enhanced_distances)):\n",
    "            improvement = ((base_dist - enhanced_dist) / base_dist) * 100\n",
    "            color = 'green' if improvement > 0 else 'red'\n",
    "            ax4.text(i, max(base_dist, enhanced_dist) + max(base_distances)*0.05,\n",
    "                    f'{improvement:+.1f}%', ha='center', va='bottom',\n",
    "                    color=color, fontweight='bold', fontsize=9)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Нет данных для сравнения\\nбазовых и улучшенных версий',\n",
    "                ha='center', va='center', transform=ax4.transAxes, fontsize=12)\n",
    "        ax4.set_title('Сравнение недоступно', fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rl_methods_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nВизуализация сохранена: 'rl_methods_comparison.png'\")\n",
    "\n",
    "# Визуализация\n",
    "plot_comprehensive_results(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "He_UTuISHxMc",
    "outputId": "d277ddfa-2b8f-42a6-8910-72ae9faef763"
   },
   "outputs": [],
   "source": [
    "improvement_data = []\n",
    "\n",
    "for agent_name in all_results.keys():\n",
    "    if ' (улучшенный)' in agent_name:\n",
    "        base_name = agent_name.replace(' (улучшенный)', ' (базовый)')\n",
    "\n",
    "        if base_name in all_results:\n",
    "            base_result = all_results[base_name]['best_distance']\n",
    "            enhanced_result = all_results[agent_name]['best_distance']\n",
    "\n",
    "            improvement = ((base_result - enhanced_result) / base_result) * 100\n",
    "\n",
    "            base_time = all_results[base_name]['training_time_seconds']\n",
    "            enhanced_time = all_results[agent_name]['training_time_seconds']\n",
    "            time_change = ((enhanced_time - base_time) / base_time) * 100\n",
    "\n",
    "            improvement_data.append({\n",
    "                'Метод': agent_name.replace(' (улучшенный)', ''),\n",
    "                'Базовый (км)': f\"{base_result:.2f}\",\n",
    "                'Улучшенный (км)': f\"{enhanced_result:.2f}\",\n",
    "                'Улучшение (%)': f\"{improvement:+.2f}\",\n",
    "                'Время базовый (сек)': f\"{base_time:.1f}\",\n",
    "                'Время улучшенный (сек)': f\"{enhanced_time:.1f}\",\n",
    "                'Изменение времени (%)': f\"{time_change:+.1f}\",\n",
    "            })\n",
    "\n",
    "if improvement_data:\n",
    "    improvement_df = pd.DataFrame(improvement_data)\n",
    "\n",
    "    # Сортируем по улучшению результата (от большего к меньшему)\n",
    "    improvement_df['Улучшение числ'] = improvement_df['Улучшение (%)'].str.replace('%', '').astype(float)\n",
    "    improvement_df = improvement_df.sort_values('Улучшение числ', ascending=False)\n",
    "    improvement_df = improvement_df.drop('Улучшение числ', axis=1)\n",
    "\n",
    "    print(\"\\nСравнение базовых и улучшенных версий:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(improvement_df.to_string(index=False))\n",
    "\n",
    "else:\n",
    "    print(\"Нет данных для сравнения базовых и улучшенных версий\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuNJ8ulpU6pN"
   },
   "source": [
    "## RL c шумом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KgC3BfNFL5R"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7RI1NRrIpht"
   },
   "outputs": [],
   "source": [
    "def train_and_test_on_noise_scenarios(num_episodes=1500):\n",
    "    \"\"\"\n",
    "    Обучение и тестирование RL агентов на сценариях с шумом\n",
    "\n",
    "    Что делаем:\n",
    "    1. Загружаем 4 сценария: без шума, пробки, блокировки+пробки, умеренные условия\n",
    "    2. Для каждого сценария вычисляем матрицу расстояний между складами\n",
    "    3. Обучаем выбранные RL агенты на каждом сценарии (1500 эпизодов)\n",
    "    4. Анализируем устойчивость методов к разным типам шума\n",
    "    5. Сравниваем результаты и выявляем лучшие методы для каждого сценария\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Количество эпизодов на метод: {num_episodes}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Загружаем сценарии с шумом (такие же как и в классическом)\n",
    "    try:\n",
    "        scenarios = {\n",
    "            \"Без шума\": graph,\n",
    "            \"Пробки\": graph_traffic_jam,\n",
    "            \"Блокировки+Пробки\": graph_combined,\n",
    "            \"Умеренные условия\": graph_moderate\n",
    "        }\n",
    "        print(\"Сценарии с шумом успешно загружены\")\n",
    "    except NameError:\n",
    "        print(\"Ошибка: сценарии с шумом не определены.\")\n",
    "        return None, None\n",
    "\n",
    "    # Загружаем данные складов\n",
    "    print(\"\\nЗагружаем данные складов\")\n",
    "    with open('warehouses_rc_rfc_coordinates.json', 'r', encoding='utf-8') as f:\n",
    "        warehouses_json = json.load(f)\n",
    "\n",
    "    warehouses_data = []\n",
    "    for warehouse in warehouses_json:\n",
    "        warehouses_data.append({\n",
    "            \"name\": warehouse['name'],\n",
    "            \"latitude\": warehouse['latitude'],\n",
    "            \"longitude\": warehouse['longitude']\n",
    "        })\n",
    "\n",
    "    warehouses_df = pd.DataFrame(warehouses_data)\n",
    "    n_warehouses = len(warehouses_df)\n",
    "\n",
    "    # Привязываем склады к узлам графа\n",
    "    print(f\"Привязываем {n_warehouses} складов к дорожной сети\")\n",
    "    warehouse_nodes = []\n",
    "    for idx, warehouse in warehouses_df.iterrows():\n",
    "        point = (warehouse['latitude'], warehouse['longitude'])\n",
    "        nearest_node = ox.distance.nearest_nodes(graph, point[1], point[0])\n",
    "        warehouse_nodes.append(nearest_node)\n",
    "\n",
    "    warehouses_df['node_id'] = warehouse_nodes\n",
    "\n",
    "    # Создаем матрицы расстояний для каждого сценария\n",
    "    print(\"\\nВычисляем матрицы расстояний для всех сценариев\")\n",
    "    scenario_matrices = {}\n",
    "\n",
    "    for scenario_name, scenario_graph in scenarios.items():\n",
    "        print(f\"  Сценарий: {scenario_name}\")\n",
    "\n",
    "        n_warehouses = len(warehouse_nodes)\n",
    "        distance_matrix = np.zeros((n_warehouses, n_warehouses))\n",
    "\n",
    "        for i in range(n_warehouses):\n",
    "            for j in range(i+1, n_warehouses):\n",
    "                try:\n",
    "                    # Проверяем, что узлы существуют в графе\n",
    "                    if warehouse_nodes[i] not in scenario_graph.nodes or warehouse_nodes[j] not in scenario_graph.nodes:\n",
    "                        distance_matrix[i][j] = 1e9\n",
    "                        distance_matrix[j][i] = 1e9\n",
    "                        continue\n",
    "\n",
    "                    distance = nx.shortest_path_length(scenario_graph, warehouse_nodes[i], warehouse_nodes[j], weight='length')\n",
    "                    distance_matrix[i][j] = distance\n",
    "                    distance_matrix[j][i] = distance\n",
    "                except nx.NetworkXNoPath:\n",
    "                    distance_matrix[i][j] = 1e9\n",
    "                    distance_matrix[j][i] = 1e9\n",
    "\n",
    "        scenario_matrices[scenario_name] = distance_matrix.astype(np.float32)\n",
    "        print(f\"    Завершено\")\n",
    "\n",
    "    # Оставленные методы\n",
    "    selected_algorithms = {\n",
    "        'SAC (улучшенный)': lambda n: EnhancedSACAgent(n),\n",
    "        'DQN (улучшенный)': lambda n: ImprovedDQNAgent(n, use_per=True, use_noisy=False),\n",
    "        'Double DQN (улучшенный)': lambda n: ImprovedDoubleDQNAgent(n, use_per=True, use_noisy=False),\n",
    "        'PPO (базовый)': lambda n: PPOAgent(n),\n",
    "        'A2C (улучшенный)': lambda n: EnhancedA2CAgent(n),\n",
    "        'Policy Gradient (базовый)': lambda n: PolicyGradientAgent(n)\n",
    "    }\n",
    "\n",
    "    # Словарь для хранения результатов\n",
    "    all_scenario_results = {}\n",
    "    training_times = {}\n",
    "\n",
    "    # Обучаем и тестируем каждый алгоритм на каждом сценарии\n",
    "    for scenario_name, distance_matrix in scenario_matrices.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Сценарий: {scenario_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        scenario_results = {}\n",
    "\n",
    "        for agent_name, agent_constructor in selected_algorithms.items():\n",
    "            print(f\"\\n  Обучаем: {agent_name}\")\n",
    "            print(f\"  {'-'*60}\")\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Создаем агента и среду\n",
    "            agent = agent_constructor(n_warehouses)\n",
    "            env = TSPEnvironment(distance_matrix, warehouses_df)\n",
    "\n",
    "            # Обучение\n",
    "            episode_distances = []\n",
    "            episode_rewards = []\n",
    "            best_distance = float('inf')\n",
    "            best_route = None\n",
    "\n",
    "            for episode in range(num_episodes):\n",
    "                state = env.reset()\n",
    "                episode_reward = 0.0\n",
    "                done = False\n",
    "\n",
    "                step_count = 0\n",
    "                max_steps = n_warehouses * 2\n",
    "\n",
    "                while not done and step_count < max_steps:\n",
    "                    # Выбор действия (особенность SAC)\n",
    "                    if 'SAC' in agent_name:\n",
    "                        action_result = agent.select_action(state)\n",
    "                        if isinstance(action_result, tuple):\n",
    "                            action = action_result[0]  # SAC возвращает (action, log_prob)\n",
    "                        else:\n",
    "                            action = action_result\n",
    "                    else:\n",
    "                        action = agent.select_action(state)\n",
    "\n",
    "                    # Шаг в среде\n",
    "                    next_state, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "\n",
    "                    # Сохраняем опыт для off-policy методов\n",
    "                    if hasattr(agent, 'store_experience'):\n",
    "                        agent.store_experience(state, action, reward, next_state, done)\n",
    "\n",
    "                    # Сохраняем награды для on-policy методов\n",
    "                    if hasattr(agent, 'rewards'):\n",
    "                        agent.rewards.append(reward)\n",
    "\n",
    "                    state = next_state\n",
    "                    step_count += 1\n",
    "\n",
    "                # Обновление политики\n",
    "                # Для PPO передаем финальную награду\n",
    "                if 'PPO' in agent_name:\n",
    "                    agent.update(episode_reward)\n",
    "                # Для остальных on-policy методов обновляем каждые 10 эпизодов\n",
    "                elif hasattr(agent, 'update') and not hasattr(agent, 'store_experience'):\n",
    "                    if episode % 10 == 0 or done:\n",
    "                        agent.update()\n",
    "                # Для off-policy методов обновляем всегда\n",
    "                elif hasattr(agent, 'update'):\n",
    "                    agent.update()\n",
    "\n",
    "                # Сохраняем результаты\n",
    "                route_info = env.get_route_info()\n",
    "                current_distance = route_info['distance_km']\n",
    "\n",
    "                episode_distances.append(current_distance)\n",
    "                episode_rewards.append(episode_reward)\n",
    "\n",
    "                if current_distance < best_distance and len(env.visited) == n_warehouses:\n",
    "                    best_distance = current_distance\n",
    "                    best_route = route_info['route']\n",
    "\n",
    "                # Прогресс каждые 150 эпизодов\n",
    "                if episode % 150 == 0:\n",
    "                    visited_count = len(env.visited)\n",
    "                    epsilon = getattr(agent, 'epsilon', 0)\n",
    "                    print(f\"    Эпизод {episode:4d}: {current_distance:7.2f} км, \"\n",
    "                          f\"ε={epsilon:.3f}, Посещено: {visited_count}/{n_warehouses}\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            training_time = end_time - start_time\n",
    "\n",
    "            # Сохраняем результаты для этого агента\n",
    "            scenario_results[agent_name] = {\n",
    "                'best_distance': best_distance,\n",
    "                'best_route': best_route,\n",
    "                'avg_last_100': np.mean(episode_distances[-100:]) if len(episode_distances) >= 100 else float('inf'),\n",
    "                'std_last_100': np.std(episode_distances[-100:]) if len(episode_distances) >= 100 else 0,\n",
    "                'avg_all': np.mean(episode_distances),\n",
    "                'std_all': np.std(episode_distances),\n",
    "                'training_time_seconds': training_time,\n",
    "                'episode_distances': episode_distances,\n",
    "                'episode_rewards': episode_rewards\n",
    "            }\n",
    "\n",
    "            training_times[(scenario_name, agent_name)] = training_time\n",
    "\n",
    "            print(f\"    Завершено: {best_distance:.2f} км за {training_time:.1f} сек\")\n",
    "\n",
    "        all_scenario_results[scenario_name] = scenario_results\n",
    "\n",
    "    return all_scenario_results, scenario_matrices, training_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2v16WZITHTQ"
   },
   "outputs": [],
   "source": [
    "def create_summary_tables(all_scenario_results):\n",
    "    \"\"\"\n",
    "    Создает сводные таблицы результатов по всем сценариям\n",
    "\n",
    "    Возвращает:\n",
    "    - summary_df: полная таблица\n",
    "    - pivot_best: сводная таблица лучших результатов\n",
    "    - pivot_avg: сводная таблица средних результатов\n",
    "    - pivot_time: сводная таблица времени обучения\n",
    "    \"\"\"\n",
    "\n",
    "    scenario_names = list(all_scenario_results.keys())\n",
    "    algorithm_names = list(all_scenario_results[scenario_names[0]].keys())\n",
    "\n",
    "    print(\"\\nСводные таблицы\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "    # Создаем полную таблицу\n",
    "    summary_data = []\n",
    "    for scenario_name in scenario_names:\n",
    "        for algorithm_name in algorithm_names:\n",
    "            result = all_scenario_results[scenario_name][algorithm_name]\n",
    "            summary_data.append({\n",
    "                'Сценарий': scenario_name,\n",
    "                'Метод': algorithm_name,\n",
    "                'Лучший результат (км)': f\"{result['best_distance']:.2f}\",\n",
    "                'Среднее последние 100 (км)': f\"{result['avg_last_100']:.2f}\",\n",
    "                'Станд. отклонение': f\"{result['std_last_100']:.2f}\",\n",
    "                'Время обучения (сек)': f\"{result['training_time_seconds']:.2f}\",\n",
    "                'Время на эпизод (мс)': f\"{result['training_time_seconds']*1000/1500:.1f}\"\n",
    "            })\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "    # Создаем pivot таблицы\n",
    "    pivot_best = summary_df.pivot(index='Метод', columns='Сценарий', values='Лучший результат (км)')\n",
    "    pivot_avg = summary_df.pivot(index='Метод', columns='Сценарий', values='Среднее последние 100 (км)')\n",
    "    pivot_time = summary_df.pivot(index='Метод', columns='Сценарий', values='Время обучения (сек)')\n",
    "\n",
    "    # Выводим таблицы\n",
    "    print(\"\\nЛучшие результаты:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(pivot_best)\n",
    "\n",
    "    print(\"\\nСредние результаты (последние 100 эпизодов):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(pivot_avg)\n",
    "\n",
    "    print(\"\\nВремя обучения (секунды):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(pivot_time)\n",
    "\n",
    "    return summary_df, pivot_best, pivot_avg, pivot_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqKBr3CwTFBT"
   },
   "outputs": [],
   "source": [
    "def plot_results_by_scenario(all_scenario_results):\n",
    "    \"\"\"\n",
    "    Создает графики с результатами по сценариям\n",
    "    Ось X: сценарии\n",
    "    Столбцы: методы\n",
    "    \"\"\"\n",
    "\n",
    "    scenario_names = list(all_scenario_results.keys())\n",
    "    algorithm_names = list(all_scenario_results[scenario_names[0]].keys())\n",
    "\n",
    "    # Сокращенные имена методов для подписей\n",
    "    short_names = []\n",
    "    for name in algorithm_names:\n",
    "        short = name.replace(' (базовый)', ' (б.)').replace(' (улучшенный)', ' (у.)')\n",
    "        short_names.append(short)\n",
    "\n",
    "    # Создаем комплексные графики\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "    # 1. Лучшие результаты по сценариям\n",
    "    # Ось X: сценарии, столбцы: методы\n",
    "\n",
    "    # Подготовка данных\n",
    "    best_results = {}\n",
    "    for scenario_name in scenario_names:\n",
    "        best_results[scenario_name] = [\n",
    "            all_scenario_results[scenario_name][algo]['best_distance']\n",
    "            for algo in algorithm_names\n",
    "        ]\n",
    "\n",
    "    # Создаем данные для группированных столбцов\n",
    "    x = np.arange(len(scenario_names))  # позиции по оси X для сценариев\n",
    "    width = 0.1  # ширина столбца для каждого метода\n",
    "\n",
    "    # Разные цвета для разных категорий методов\n",
    "    color_palette = plt.cm.Set3(np.linspace(0, 1, len(algorithm_names)))\n",
    "\n",
    "    for i, (algo_name, short_name) in enumerate(zip(algorithm_names, short_names)):\n",
    "        algo_values = [best_results[scenario][i] for scenario in scenario_names]\n",
    "        ax1.bar(x + i*width - width*(len(algorithm_names)-1)/2,\n",
    "               algo_values, width, label=short_name, color=color_palette[i], alpha=0.8)\n",
    "\n",
    "    ax1.set_xlabel('Сценарии', fontsize=12)\n",
    "    ax1.set_ylabel('Лучшее расстояние (км)', fontsize=12)\n",
    "    ax1.set_title('Лучшие результаты методов RL по сценариям\\n(чем меньше, тем лучше)',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(scenario_names, rotation=90, fontsize=11)\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.tick_params(axis='x', rotation=90)\n",
    "\n",
    "    # Добавляем значения на столбцы (только минимальные для каждого сценария)\n",
    "    for j, scenario in enumerate(scenario_names):\n",
    "        min_value = min(best_results[scenario])\n",
    "        min_index = best_results[scenario].index(min_value)\n",
    "        ax1.text(j + min_index*width - width*(len(algorithm_names)-1)/2,\n",
    "                min_value - max(min_value*0.02, 1),\n",
    "                f'{min_value:.1f}', ha='center', va='top', fontsize=8,\n",
    "                fontweight='bold', color='red')\n",
    "\n",
    "    # 2. Средние результаты по сценариям\n",
    "    avg_results = {}\n",
    "    for scenario_name in scenario_names:\n",
    "        avg_results[scenario_name] = [\n",
    "            all_scenario_results[scenario_name][algo]['avg_last_100']\n",
    "            for algo in algorithm_names\n",
    "        ]\n",
    "\n",
    "    for i, (algo_name, short_name) in enumerate(zip(algorithm_names, short_names)):\n",
    "        algo_values = [avg_results[scenario][i] for scenario in scenario_names]\n",
    "        ax2.bar(x + i*width - width*(len(algorithm_names)-1)/2,\n",
    "               algo_values, width, label=short_name, color=color_palette[i], alpha=0.8)\n",
    "\n",
    "    ax2.set_xlabel('Сценарии', fontsize=12)\n",
    "    ax2.set_ylabel('Среднее расстояние (км)', fontsize=12)\n",
    "    ax2.set_title('Средние результаты методов RL (последние 100 эпизодов)',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(scenario_names, rotation=90, fontsize=11)\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.tick_params(axis='x', rotation=90)\n",
    "\n",
    "    # График 3. Ухудшение для сценариев\n",
    "    base_scenario = \"Без шума\"\n",
    "    performance_degradation = {}\n",
    "\n",
    "    # Рассчитываем ухудшение для каждого сценария с шумом\n",
    "    for scenario_name in scenario_names:\n",
    "        if scenario_name == base_scenario:\n",
    "            continue\n",
    "\n",
    "        degradation = []\n",
    "        for algo_name in algorithm_names:\n",
    "            base_result = all_scenario_results[base_scenario][algo_name]['best_distance']\n",
    "            current_result = all_scenario_results[scenario_name][algo_name]['best_distance']\n",
    "            degradation_pct = ((current_result - base_result) / base_result) * 100\n",
    "            degradation.append(degradation_pct)\n",
    "\n",
    "        performance_degradation[scenario_name] = degradation\n",
    "\n",
    "    # Создаем подграфик для ухудшения\n",
    "    noise_scenarios = [s for s in scenario_names if s != base_scenario]\n",
    "    x_degradation = np.arange(len(noise_scenarios))\n",
    "\n",
    "    if performance_degradation:\n",
    "        # Разные цвета для ухудшения (красные оттенки)\n",
    "        degradation_colors = plt.cm.Reds(np.linspace(0.3, 0.9, len(algorithm_names)))\n",
    "\n",
    "        for i, (algo_name, short_name) in enumerate(zip(algorithm_names, short_names)):\n",
    "            algo_values = [performance_degradation[scenario][i] for scenario in noise_scenarios]\n",
    "            bars = ax3.bar(x_degradation + i*width - width*(len(algorithm_names)-1)/2,\n",
    "                          algo_values, width, label=short_name,\n",
    "                          color=degradation_colors[i], alpha=0.8)\n",
    "\n",
    "            # Добавляем значения на столбцы\n",
    "            for j, value in enumerate(algo_values):\n",
    "                ax3.text(x_degradation[j] + i*width - width*(len(algorithm_names)-1)/2,\n",
    "                        value + (1 if value >= 0 else -3),\n",
    "                        f'{value:.1f}%', ha='center',\n",
    "                        va='bottom' if value >= 0 else 'top', fontsize=7)\n",
    "\n",
    "    ax3.set_xlabel('Сценарии с шумом', fontsize=12)\n",
    "    ax3.set_ylabel('Ухудшение производительности (%)', fontsize=12)\n",
    "    ax3.set_title('Ухудшение относительно сценария без шума\\n(положительные значения = ухудшение)',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax3.set_xticks(x_degradation)\n",
    "    ax3.set_xticklabels(noise_scenarios, rotation=90, fontsize=11)\n",
    "    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.axhline(y=0, color='black', linestyle='-', alpha=0.5, linewidth=0.8)\n",
    "\n",
    "    # График 4. Устойчивость к шуму\n",
    "    stability_scores = {}\n",
    "\n",
    "    for algo_name in algorithm_names:\n",
    "        scores = []\n",
    "        base_perf = all_scenario_results[base_scenario][algo_name]['best_distance']\n",
    "\n",
    "        for scenario_name in scenario_names:\n",
    "            if scenario_name == base_scenario:\n",
    "                continue\n",
    "\n",
    "            scenario_perf = all_scenario_results[scenario_name][algo_name]['best_distance']\n",
    "            degradation = ((scenario_perf - base_perf) / base_perf) * 100\n",
    "            stability_score = 100 - abs(degradation)\n",
    "            scores.append(stability_score)\n",
    "\n",
    "        stability_scores[algo_name] = np.mean(scores) if scores else 100\n",
    "\n",
    "    # Сортируем методы по устойчивости\n",
    "    sorted_algorithms = sorted(stability_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    algorithms_sorted = [x[0].replace(' (базовый)', ' (б.)').replace(' (улучшенный)', ' (у.)')\n",
    "                        for x in sorted_algorithms]\n",
    "    scores_sorted = [x[1] for x in sorted_algorithms]\n",
    "\n",
    "    # Цвета для столбцов\n",
    "    bar_colors = []\n",
    "    for algo in algorithms_sorted:\n",
    "        if 'у.' in algo:\n",
    "            bar_colors.append('green')\n",
    "        elif 'б.' in algo:\n",
    "            bar_colors.append('blue')\n",
    "        else:\n",
    "            bar_colors.append('gray')\n",
    "\n",
    "    bars = ax4.barh(algorithms_sorted, scores_sorted, color=bar_colors, alpha=0.7)\n",
    "    ax4.set_xlabel('Оценка устойчивости к шуму', fontsize=12)\n",
    "    ax4.set_ylabel('Методы RL', fontsize=12)\n",
    "    ax4.set_title('Рейтинг методов по устойчивости к шуму\\n(выше = лучше)',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3, axis='x')\n",
    "    ax4.set_xlim([0, 105])\n",
    "\n",
    "    # Добавляем значения на столбцы\n",
    "    for bar, score in zip(bars, scores_sorted):\n",
    "        width = bar.get_width()\n",
    "        ax4.text(width + 1, bar.get_y() + bar.get_height()/2,\n",
    "                f'{score:.1f}', ha='left', va='center', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rl_noise_scenarios_comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Визуализация сохранена: 'rl_noise_scenarios_comprehensive_analysis.png'\")\n",
    "\n",
    "    return stability_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqO9zfMCS38x"
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(all_scenario_results, max_methods_per_plot=6):\n",
    "    \"\"\"\n",
    "    Визуализация кривых обучения для всех сценариев и методов\n",
    "\n",
    "    Аргументы:\n",
    "    - all_scenario_results: результаты всех сценариев\n",
    "    - max_methods_per_plot: максимальное количество методов на одном графике\n",
    "    \"\"\"\n",
    "\n",
    "    scenario_names = list(all_scenario_results.keys())\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Визуализация кривых обучения\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Создаем графики кривых обучения для каждого сценария\n",
    "    for scenario_name in scenario_names:\n",
    "        results = all_scenario_results[scenario_name]\n",
    "        method_names = list(results.keys())\n",
    "\n",
    "        # Разбиваем методы на группы для лучшей читаемости\n",
    "        num_groups = (len(method_names) + max_methods_per_plot - 1) // max_methods_per_plot\n",
    "\n",
    "        for group_idx in range(num_groups):\n",
    "            start_idx = group_idx * max_methods_per_plot\n",
    "            end_idx = min(start_idx + max_methods_per_plot, len(method_names))\n",
    "            group_methods = method_names[start_idx:end_idx]\n",
    "\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            for idx, method_name in enumerate(group_methods):\n",
    "                if idx >= len(axes):\n",
    "                    break\n",
    "\n",
    "                ax = axes[idx]\n",
    "                result = results[method_name]\n",
    "                episode_distances = result['episode_distances']\n",
    "\n",
    "                if episode_distances:\n",
    "                    # Сглаживаем кривую обучения\n",
    "                    smoothed = pd.Series(episode_distances).rolling(50, min_periods=1).mean()\n",
    "\n",
    "                    # Основная кривая\n",
    "                    ax.plot(smoothed, color='blue', alpha=0.7, linewidth=2, label='Сглаженная')\n",
    "\n",
    "                    # Лучший результат\n",
    "                    best_dist = result['best_distance']\n",
    "                    ax.axhline(y=best_dist, color='red', linestyle='--', alpha=0.7,\n",
    "                              label=f'Лучшее: {best_dist:.1f} км')\n",
    "\n",
    "                    # Средний результат\n",
    "                    avg_last_100 = result['avg_last_100']\n",
    "                    ax.axhline(y=avg_last_100, color='green', linestyle=':', alpha=0.7,\n",
    "                              label=f'Среднее: {avg_last_100:.1f} км')\n",
    "\n",
    "                    # Эпизод сходимости\n",
    "                    convergence_episode = np.argmin(episode_distances)\n",
    "                    ax.axvline(x=convergence_episode, color='orange', linestyle='-.', alpha=0.5,\n",
    "                              label=f'Сходимость: {convergence_episode}')\n",
    "\n",
    "                    ax.set_xlabel('Эпизод', fontsize=10)\n",
    "                    ax.set_ylabel('Расстояние (км)', fontsize=10)\n",
    "\n",
    "                    # Сокращаем название метода для заголовка\n",
    "                    short_name = method_name.replace(' (базовый)', '\\n(б.)').replace(' (улучшенный)', '\\n(у.)')\n",
    "                    ax.set_title(short_name, fontsize=11, fontweight='bold')\n",
    "\n",
    "                    ax.legend(fontsize=8)\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "\n",
    "                    # Добавляем аннотацию с информацией\n",
    "                    info_text = f\"Сценарий: {scenario_name}\\n\"\n",
    "                    info_text += f\"Лучшее: {best_dist:.1f} км\\n\"\n",
    "                    info_text += f\"Среднее: {avg_last_100:.1f} км\\n\"\n",
    "                    info_text += f\"Сходимость: {convergence_episode} эп.\"\n",
    "\n",
    "                    ax.text(0.02, 0.98, info_text, transform=ax.transAxes,\n",
    "                           fontsize=8, verticalalignment='top',\n",
    "                           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "            # Скрываем пустые оси\n",
    "            for idx in range(len(group_methods), len(axes)):\n",
    "                axes[idx].axis('off')\n",
    "\n",
    "            group_title = f\"Кривые обучения: {scenario_name}\"\n",
    "            if num_groups > 1:\n",
    "                group_title += f\" (Группа {group_idx + 1}/{num_groups})\"\n",
    "\n",
    "            plt.suptitle(group_title, fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename = f'learning_curves_{scenario_name.replace(\" \", \"_\").replace(\"+\", \"_\")}'\n",
    "            if num_groups > 1:\n",
    "                filename += f'_group_{group_idx + 1}'\n",
    "            filename += '.png'\n",
    "\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f\"  Сохранено: {filename}\")\n",
    "\n",
    "\n",
    "def analyze_noise_scenario_results(all_scenario_results, training_times):\n",
    "    \"\"\"\n",
    "    Основная функция анализа результатов по сценариям с шумом\n",
    "\n",
    "    Выполняет:\n",
    "    1. Создание сводных таблиц\n",
    "    2. Построение графиков\n",
    "    3. Визуализацию кривых обучения\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Анализ результатов\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 1. Сводные таблицы\n",
    "    print(\"\\n Шаг 1: Создание сводных таблиц\")\n",
    "    summary_df, pivot_best, pivot_avg, pivot_time = create_summary_tables(all_scenario_results)\n",
    "\n",
    "    # 2. Графики\n",
    "    print(\"\\n Шаг 2: Построение графиков\")\n",
    "    stability_scores = plot_results_by_scenario(all_scenario_results)\n",
    "\n",
    "    # 3. Кривые обучения\n",
    "    print(\"\\n Шаг 3: Визуализация кривых обучения\")\n",
    "    plot_learning_curves(all_scenario_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRNBsMTx5fZv",
    "outputId": "9f747581-86d4-47b1-d0db-ff6537e4b4df"
   },
   "outputs": [],
   "source": [
    "# Запускаем обучение и тестирование\n",
    "all_scenario_results, scenario_matrices, training_times = train_and_test_on_noise_scenarios(num_episodes=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nmyp46JZKSEQ",
    "outputId": "02335ef6-e3bb-4ba4-f9df-b9073c9bb5a5"
   },
   "outputs": [],
   "source": [
    "analysis_results = analyze_noise_scenario_results(all_scenario_results, training_times)\n",
    "analysis_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC9OUg00S1gI"
   },
   "source": [
    "## RL на большем количестве складов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKViCFXSUCEy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MR3CS7DXXOXU"
   },
   "outputs": [],
   "source": [
    "def train_rl_all_warehouses_noise_scenarios():\n",
    "    \"\"\"\n",
    "    Обучение RL методов на всех складах со всеми сценариями шума\n",
    "    \"\"\"\n",
    "    # Загрузка всех складов\n",
    "    print(\"\\nЗагружаем все склады\")\n",
    "    with open('all_warehouses_coordinates.json', 'r', encoding='utf-8') as f:\n",
    "        all_warehouses_json = json.load(f)\n",
    "\n",
    "    all_warehouses_data = []\n",
    "    for warehouse in all_warehouses_json:\n",
    "        all_warehouses_data.append({\n",
    "            \"name\": warehouse['name'],\n",
    "            \"latitude\": warehouse['latitude'],\n",
    "            \"longitude\": warehouse['longitude'],\n",
    "            \"type\": \"РЦ/РФЦ\" if \"РЦ\" in warehouse['name'] or \"РФЦ\" in warehouse['name'] else \"СЦ\"\n",
    "        })\n",
    "\n",
    "    all_warehouses_df = pd.DataFrame(all_warehouses_data)\n",
    "\n",
    "    # Привязка складов к узлам графа\n",
    "    all_warehouse_nodes = []\n",
    "    for idx, warehouse in all_warehouses_df.iterrows():\n",
    "        point = (warehouse['latitude'], warehouse['longitude'])\n",
    "        nearest_node = ox.distance.nearest_nodes(graph, point[1], point[0])\n",
    "        all_warehouse_nodes.append(nearest_node)\n",
    "\n",
    "    all_warehouses_df['node_id'] = all_warehouse_nodes\n",
    "    print(f\"Загружено складов: {len(all_warehouses_df)}\")\n",
    "    print(f\"Распределение по типам: {dict(all_warehouses_df['type'].value_counts())}\")\n",
    "\n",
    "    # Создание матриц расстояний для всех сценариев\n",
    "    print(\"\\nСоздание матриц расстояний для всех сценариев\")\n",
    "    scenario_matrices_all = {}\n",
    "\n",
    "    scenarios_all = {\n",
    "        \"Без шума\": graph,\n",
    "        \"Пробки\": graph_traffic_jam,\n",
    "        \"Блокировки+Пробки\": graph_combined,\n",
    "        \"Умеренные условия\": graph_moderate\n",
    "    }\n",
    "\n",
    "    for scenario_name, scenario_graph in scenarios_all.items():\n",
    "        print(f\"  Вычисление: {scenario_name}...\")\n",
    "\n",
    "        n_warehouses = len(all_warehouse_nodes)\n",
    "        distance_matrix = np.zeros((n_warehouses, n_warehouses))\n",
    "\n",
    "        for i in range(n_warehouses):\n",
    "            for j in range(i+1, n_warehouses):\n",
    "                try:\n",
    "                    distance = nx.shortest_path_length(scenario_graph, all_warehouse_nodes[i], all_warehouse_nodes[j], weight='length')\n",
    "                    distance_matrix[i][j] = distance\n",
    "                    distance_matrix[j][i] = distance\n",
    "                except nx.NetworkXNoPath:\n",
    "                    distance_matrix[i][j] = 1e9\n",
    "                    distance_matrix[j][i] = 1e9\n",
    "\n",
    "        scenario_matrices_all[scenario_name] = distance_matrix.astype(np.float32)\n",
    "        print(f\"    Завершено: {scenario_name}\")\n",
    "\n",
    "    # Выбранные RL методы для тестирования\n",
    "    algorithms = {\n",
    "        'SAC (улучшенный)': lambda n: EnhancedSACAgent(n),\n",
    "        'DQN (улучшенный)': lambda n: ImprovedDQNAgent(n, use_per=True, use_noisy=False),\n",
    "        'Double DQN (улучшенный)': lambda n: ImprovedDoubleDQNAgent(n, use_per=True, use_noisy=False),\n",
    "        'PPO (базовый)': lambda n: PPOAgent(n),\n",
    "        'A2C (улучшенный)': lambda n: EnhancedA2CAgent(n),\n",
    "        'Policy Gradient (базовый)': lambda n: PolicyGradientAgent(n)\n",
    "    }\n",
    "\n",
    "    n_warehouses_all = len(all_warehouses_df)\n",
    "    num_episodes = 1500\n",
    "    comprehensive_results = {}\n",
    "\n",
    "    print(f\"\\nНачинаем обучение на {n_warehouses_all} складах\")\n",
    "    print(f\"Тестируемые методы: {', '.join(algorithms.keys())}\")\n",
    "    print(f\"Количество эпизодов: {num_episodes}\")\n",
    "\n",
    "    # Обучение каждого алгоритма на каждом сценарии\n",
    "    for scenario_name, distance_matrix in scenario_matrices_all.items():\n",
    "        print(f\"\\nСценарий: {scenario_name}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        scenario_results = {}\n",
    "\n",
    "        for agent_name, agent_constructor in algorithms.items():\n",
    "            print(f\"  Обучение: {agent_name}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Создаем агента и среду\n",
    "            agent = agent_constructor(n_warehouses_all)\n",
    "            env = TSPEnvironment(distance_matrix, all_warehouses_df)\n",
    "\n",
    "            # Обучение на 1500 эпизодов\n",
    "            episode_distances = []\n",
    "            episode_rewards = []\n",
    "            best_route = None\n",
    "            best_distance = float('inf')\n",
    "\n",
    "            for episode in range(num_episodes):\n",
    "                state = env.reset()\n",
    "                episode_reward = 0.0\n",
    "                done = False\n",
    "                step_count = 0\n",
    "                max_steps = n_warehouses_all * 2\n",
    "\n",
    "                step_rewards = []\n",
    "\n",
    "                while not done and step_count < max_steps:\n",
    "                    # Выбор действия (особенность SAC)\n",
    "                    if 'SAC' in agent_name:\n",
    "                        action_result = agent.select_action(state)\n",
    "                        if isinstance(action_result, tuple):\n",
    "                            action = action_result[0]  # SAC возвращает (action, log_prob)\n",
    "                        else:\n",
    "                            action = action_result\n",
    "                    else:\n",
    "                        action = agent.select_action(state)\n",
    "\n",
    "                    # Шаг в среде\n",
    "                    next_state, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    step_rewards.append(reward)\n",
    "\n",
    "                    # Сохраняем опыт для off-policy методов\n",
    "                    if hasattr(agent, 'store_experience'):\n",
    "                        agent.store_experience(state, action, reward, next_state, done)\n",
    "\n",
    "                    # Сохраняем награды для on-policy методов\n",
    "                    if hasattr(agent, 'rewards'):\n",
    "                        agent.rewards.append(reward)\n",
    "\n",
    "                    state = next_state\n",
    "                    step_count += 1\n",
    "\n",
    "                # Обновление политики\n",
    "                # Для PPO передаем финальную награду\n",
    "                if 'PPO' in agent_name:\n",
    "                    agent.update(episode_reward)\n",
    "                # Для остальных on-policy методов обновляем каждые 10 эпизодов\n",
    "                elif hasattr(agent, 'update') and not hasattr(agent, 'store_experience'):\n",
    "                    if episode % 10 == 0 or done:\n",
    "                        agent.update()\n",
    "                # Для off-policy методов обновляем всегда\n",
    "                elif hasattr(agent, 'update'):\n",
    "                    agent.update()\n",
    "\n",
    "                # Сохраняем результаты\n",
    "                route_info = env.get_route_info()\n",
    "                current_distance = route_info['distance_km']\n",
    "\n",
    "                episode_distances.append(current_distance)\n",
    "                episode_rewards.append(episode_reward)\n",
    "\n",
    "                if current_distance < best_distance and len(env.visited) == n_warehouses_all:\n",
    "                    best_distance = current_distance\n",
    "                    best_route = route_info['route']\n",
    "\n",
    "                # Прогресс каждые 150 эпизодов\n",
    "                if episode % 150 == 0:\n",
    "                    visited_count = len(env.visited)\n",
    "                    epsilon = getattr(agent, 'epsilon', 0)\n",
    "                    print(f\"    Эпизод {episode:4d}: {current_distance:7.2f} км, \"\n",
    "                          f\"ε={epsilon:.3f}, Посещено: {visited_count}/{n_warehouses_all}\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            training_time = end_time - start_time\n",
    "\n",
    "            # Анализ результатов\n",
    "            avg_distance = np.mean(episode_distances)\n",
    "            std_distance = np.std(episode_distances)\n",
    "            avg_reward = np.mean(episode_rewards)\n",
    "\n",
    "            scenario_results[agent_name] = {\n",
    "                'best_distance': best_distance,\n",
    "                'avg_distance': avg_distance,\n",
    "                'std_distance': std_distance,\n",
    "                'avg_reward': avg_reward,\n",
    "                'training_time': training_time,\n",
    "                'episode_distances': episode_distances,\n",
    "                'episode_rewards': episode_rewards,\n",
    "                'best_route': best_route\n",
    "            }\n",
    "\n",
    "            print(f\"    Завершено: {agent_name}\")\n",
    "            print(f\"      Лучший результат: {best_distance:.2f} км\")\n",
    "            print(f\"      Средний результат: {avg_distance:.2f} км\")\n",
    "            print(f\"      Время обучения: {training_time:.1f} секунд\")\n",
    "\n",
    "        comprehensive_results[scenario_name] = scenario_results\n",
    "\n",
    "    return comprehensive_results, all_warehouses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWGT1K0sXhla"
   },
   "outputs": [],
   "source": [
    "def create_rl_summary_tables(all_scenario_results):\n",
    "    \"\"\"\n",
    "    Создание сводных таблиц результатов RL методов\n",
    "    \"\"\"\n",
    "    scenario_names = list(all_scenario_results.keys())\n",
    "    algorithm_names = list(all_scenario_results[scenario_names[0]].keys())\n",
    "\n",
    "    # Создание полной таблицы\n",
    "    summary_data = []\n",
    "    for scenario_name in scenario_names:\n",
    "        for algorithm_name in algorithm_names:\n",
    "            result = all_scenario_results[scenario_name][algorithm_name]\n",
    "            summary_data.append({\n",
    "                'Сценарий': scenario_name,\n",
    "                'Метод': algorithm_name,\n",
    "                'Лучший результат (км)': f\"{result['best_distance']:.2f}\",\n",
    "                'Среднее расстояние (км)': f\"{result['avg_distance']:.2f}\",\n",
    "                'Станд. отклонение': f\"{result['std_distance']:.2f}\",\n",
    "                'Средняя награда': f\"{result['avg_reward']:.2f}\",\n",
    "                'Время обучения (сек)': f\"{result['training_time']:.2f}\",\n",
    "                'Время на эпизод (сек)': f\"{result['training_time']/1500:.3f}\"\n",
    "            })\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "    # Создание pivot таблиц\n",
    "    print(\"\\nЛучшие результаты по сценариям:\")\n",
    "    pivot_best = summary_df.pivot(index='Метод', columns='Сценарий', values='Лучший результат (км)')\n",
    "    print(pivot_best)\n",
    "\n",
    "    print(\"\\nСредние расстояния по сценариям:\")\n",
    "    pivot_avg = summary_df.pivot(index='Метод', columns='Сценарий', values='Среднее расстояние (км)')\n",
    "    print(pivot_avg)\n",
    "\n",
    "    print(\"\\nВремя обучения по сценариям:\")\n",
    "    pivot_time = summary_df.pivot(index='Метод', columns='Сценарий', values='Время обучения (сек)')\n",
    "    print(pivot_time)\n",
    "\n",
    "    return summary_df, pivot_best, pivot_avg, pivot_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chUKJVLhYOF6"
   },
   "outputs": [],
   "source": [
    "def plot_rl_comprehensive_results(all_scenario_results):\n",
    "    \"\"\"\n",
    "    Визуализация комплексных результатов RL методов\n",
    "    \"\"\"\n",
    "    scenario_names = list(all_scenario_results.keys())\n",
    "    algorithm_names = list(all_scenario_results[scenario_names[0]].keys())\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "    # График 1: Лучшие результаты по сценариям\n",
    "    best_data = {algo: [] for algo in algorithm_names}\n",
    "    for scenario_name in scenario_names:\n",
    "        for algo_name in algorithm_names:\n",
    "            result = all_scenario_results[scenario_name][algo_name]\n",
    "            best_data[algo_name].append(result['best_distance'])\n",
    "\n",
    "    x = np.arange(len(scenario_names))\n",
    "    width = 0.12\n",
    "\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "\n",
    "    for i, algo_name in enumerate(algorithm_names):\n",
    "        ax1.bar(x + i * width, best_data[algo_name], width, label=algo_name, color=colors[i], alpha=0.8)\n",
    "\n",
    "    ax1.set_xlabel('Сценарии')\n",
    "    ax1.set_ylabel('Лучшее расстояние (км)')\n",
    "    ax1.set_title('Лучшие результаты RL методов по сценариям\\n(Все склады, 1500 эпизодов)')\n",
    "    ax1.set_xticks(x + width * (len(algorithm_names) - 1) / 2)\n",
    "    ax1.set_xticklabels(scenario_names, rotation=45)\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Добавление значений на столбцы (только для лучших в каждом сценарии)\n",
    "    for j, scenario in enumerate(scenario_names):\n",
    "        min_value = min([all_scenario_results[scenario][algo]['best_distance'] for algo in algorithm_names])\n",
    "        ax1.text(j + width * (len(algorithm_names) - 1) / 2, min_value - max(min_value*0.02, 1),\n",
    "                f'{min_value:.1f}', ha='center', va='top', fontsize=8, fontweight='bold', color='red')\n",
    "\n",
    "    # График 2: Средние результаты по сценариям\n",
    "    avg_data = {algo: [] for algo in algorithm_names}\n",
    "    for scenario_name in scenario_names:\n",
    "        for algo_name in algorithm_names:\n",
    "            result = all_scenario_results[scenario_name][algo_name]\n",
    "            avg_data[algo_name].append(result['avg_distance'])\n",
    "\n",
    "    for i, algo_name in enumerate(algorithm_names):\n",
    "        ax2.bar(x + i * width, avg_data[algo_name], width, label=algo_name, color=colors[i], alpha=0.8)\n",
    "\n",
    "    ax2.set_xlabel('Сценарии')\n",
    "    ax2.set_ylabel('Среднее расстояние (км)')\n",
    "    ax2.set_title('Средние результаты RL методов по сценариям\\n(Все склады, 1500 эпизодов)')\n",
    "    ax2.set_xticks(x + width * (len(algorithm_names) - 1) / 2)\n",
    "    ax2.set_xticklabels(scenario_names, rotation=45)\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # График 3: Устойчивость к шуму (ухудшение производительности)\n",
    "    base_scenario = \"Без шума\"\n",
    "    degradation_data = {algo: [] for algo in algorithm_names}\n",
    "\n",
    "    for algo_name in algorithm_names:\n",
    "        base_avg = all_scenario_results[base_scenario][algo_name]['avg_distance']\n",
    "        for scenario_name in scenario_names:\n",
    "            if scenario_name != base_scenario:\n",
    "                current_avg = all_scenario_results[scenario_name][algo_name]['avg_distance']\n",
    "                degradation_pct = ((current_avg - base_avg) / base_avg) * 100\n",
    "                degradation_data[algo_name].append(degradation_pct)\n",
    "\n",
    "    x_degrade = np.arange(len(scenario_names) - 1)\n",
    "    for i, algo_name in enumerate(algorithm_names):\n",
    "        ax3.bar(x_degrade + i * width, degradation_data[algo_name], width,\n",
    "                label=algo_name, color=colors[i], alpha=0.8)\n",
    "\n",
    "    ax3.set_xlabel('Сценарии с шумом')\n",
    "    ax3.set_ylabel('Ухудшение производительности (%)')\n",
    "    ax3.set_title('Устойчивость методов к шуму\\n(Относительно базового сценария)')\n",
    "    ax3.set_xticks(x_degrade + width * (len(algorithm_names) - 1) / 2)\n",
    "    ax3.set_xticklabels([name for name in scenario_names if name != base_scenario], rotation=45)\n",
    "    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # График 4: Рейтинг методов по комплексной оценке\n",
    "    algorithm_scores = {}\n",
    "\n",
    "    for algo_name in algorithm_names:\n",
    "        # Оценка качества\n",
    "        quality_score = 0\n",
    "        for scenario_name in scenario_names:\n",
    "            avg_dist = all_scenario_results[scenario_name][algo_name]['avg_distance']\n",
    "            quality_score += 1000 / avg_dist\n",
    "\n",
    "        # Оценка устойчивости\n",
    "        stability_score = 0\n",
    "        base_avg = all_scenario_results[base_scenario][algo_name]['avg_distance']\n",
    "        for scenario_name in scenario_names:\n",
    "            if scenario_name != base_scenario:\n",
    "                current_avg = all_scenario_results[scenario_name][algo_name]['avg_distance']\n",
    "                degradation = abs((current_avg - base_avg) / base_avg)\n",
    "                stability_score += (1 - degradation) * 100\n",
    "\n",
    "        total_score = 0.5 * (quality_score / len(scenario_names)) + 0.5 * (stability_score / (len(scenario_names) - 1))\n",
    "        algorithm_scores[algo_name] = total_score\n",
    "\n",
    "    sorted_algorithms = sorted(algorithm_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    algo_names_sorted = [x[0] for x in sorted_algorithms]\n",
    "    scores_sorted = [x[1] for x in sorted_algorithms]\n",
    "\n",
    "    # Разные цвета для базовых и улучшенных методов\n",
    "    bar_colors = []\n",
    "    for algo in algo_names_sorted:\n",
    "        if 'улучшенный' in algo:\n",
    "            bar_colors.append('#2E8B57')  # зеленый\n",
    "        elif 'базовый' in algo:\n",
    "            bar_colors.append('#4169E1')  # синий\n",
    "        else:\n",
    "            bar_colors.append('#DC143C')  # красный\n",
    "\n",
    "    bars = ax4.barh(algo_names_sorted, scores_sorted, color=bar_colors, alpha=0.7)\n",
    "    ax4.set_xlabel('Комплексная оценка')\n",
    "    ax4.set_ylabel('Методы')\n",
    "    ax4.set_title('Рейтинг RL методов\\n(Качество + Устойчивость к шуму)')\n",
    "    ax4.grid(True, alpha=0.3, axis='x')\n",
    "    ax4.set_xlim([0, 105])\n",
    "\n",
    "    for bar, score in zip(bars, scores_sorted):\n",
    "        width = bar.get_width()\n",
    "        ax4.text(width + 1, bar.get_y() + bar.get_height()/2,\n",
    "                f'{score:.1f}', ha='left', va='center', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rl_comprehensive_results_all_warehouses_1500_episodes.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return sorted_algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0oGDQXKYXN0"
   },
   "outputs": [],
   "source": [
    "def plot_rl_learning_curves(all_scenario_results, max_methods_per_plot=4):\n",
    "    \"\"\"\n",
    "    Визуализация кривых обучения RL методов\n",
    "    \"\"\"\n",
    "    print(\"\\nВизуализация кривых обучения\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    scenario_names = list(all_scenario_results.keys())\n",
    "\n",
    "    for scenario_name in scenario_names:\n",
    "        results = all_scenario_results[scenario_name]\n",
    "        method_names = list(results.keys())\n",
    "\n",
    "        num_groups = (len(method_names) + max_methods_per_plot - 1) // max_methods_per_plot\n",
    "\n",
    "        for group_idx in range(num_groups):\n",
    "            start_idx = group_idx * max_methods_per_plot\n",
    "            end_idx = min(start_idx + max_methods_per_plot, len(method_names))\n",
    "            group_methods = method_names[start_idx:end_idx]\n",
    "\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            for idx, method_name in enumerate(group_methods):\n",
    "                if idx >= len(axes):\n",
    "                    break\n",
    "\n",
    "                ax = axes[idx]\n",
    "                result = results[method_name]\n",
    "                episode_distances = result['episode_distances']\n",
    "\n",
    "                if episode_distances:\n",
    "                    # Сглаживание кривой обучения\n",
    "                    smoothed = pd.Series(episode_distances).rolling(50, min_periods=1).mean()\n",
    "\n",
    "                    # Основная кривая\n",
    "                    ax.plot(smoothed, color='blue', alpha=0.7, linewidth=2, label='Сглаженная')\n",
    "\n",
    "                    # Лучший результат\n",
    "                    best_dist = result['best_distance']\n",
    "                    ax.axhline(y=best_dist, color='red', linestyle='--', alpha=0.7,\n",
    "                              label=f'Лучшее: {best_dist:.1f} км')\n",
    "\n",
    "                    # Средний результат\n",
    "                    avg_distance = result['avg_distance']\n",
    "                    ax.axhline(y=avg_distance, color='green', linestyle=':', alpha=0.7,\n",
    "                              label=f'Среднее: {avg_distance:.1f} км')\n",
    "\n",
    "                    ax.set_xlabel('Эпизод')\n",
    "                    ax.set_ylabel('Расстояние (км)')\n",
    "                    ax.set_title(method_name, fontsize=11, fontweight='bold')\n",
    "                    ax.legend(fontsize=8)\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "\n",
    "            for idx in range(len(group_methods), len(axes)):\n",
    "                axes[idx].axis('off')\n",
    "\n",
    "            group_title = f\"Кривые обучения: {scenario_name}\"\n",
    "            if num_groups > 1:\n",
    "                group_title += f\" (Группа {group_idx + 1}/{num_groups})\"\n",
    "\n",
    "            plt.suptitle(group_title, fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename = f'learning_curves_{scenario_name.replace(\" \", \"_\").replace(\"+\", \"_\")}'\n",
    "            if num_groups > 1:\n",
    "                filename += f'_group_{group_idx + 1}'\n",
    "            filename += '.png'\n",
    "\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f\"  Сохранено: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ef7rS6ACYsFJ"
   },
   "outputs": [],
   "source": [
    "def analyze_rl_results(all_scenario_results):\n",
    "    \"\"\"\n",
    "    Детальный анализ результатов RL методов\n",
    "    \"\"\"\n",
    "    print(\"\\nДетальный анализ результатов\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    scenario_names = list(all_scenario_results.keys())\n",
    "    algorithm_names = list(all_scenario_results[scenario_names[0]].keys())\n",
    "\n",
    "    print(\"\\nЛучшие методы по сценариям:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    best_methods_by_scenario = {}\n",
    "    for scenario_name in scenario_names:\n",
    "        best_method = min(algorithm_names,\n",
    "                         key=lambda algo: all_scenario_results[scenario_name][algo]['best_distance'])\n",
    "        best_distance = all_scenario_results[scenario_name][best_method]['best_distance']\n",
    "        best_methods_by_scenario[scenario_name] = (best_method, best_distance)\n",
    "        print(f\"{scenario_name:25}: {best_method:25} ({best_distance:.2f} км)\")\n",
    "\n",
    "    print(\"\\nНаиболее устойчивые методы:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    base_scenario = \"Без шума\"\n",
    "    stability_scores = {}\n",
    "\n",
    "    for algo_name in algorithm_names:\n",
    "        base_avg = all_scenario_results[base_scenario][algo_name]['avg_distance']\n",
    "        degradation_scores = []\n",
    "\n",
    "        for scenario_name in scenario_names:\n",
    "            if scenario_name != base_scenario:\n",
    "                current_avg = all_scenario_results[scenario_name][algo_name]['avg_distance']\n",
    "                degradation = abs((current_avg - base_avg) / base_avg)\n",
    "                degradation_scores.append(degradation)\n",
    "\n",
    "        avg_degradation = np.mean(degradation_scores) if degradation_scores else 0\n",
    "        stability_scores[algo_name] = 100 * (1 - avg_degradation)\n",
    "\n",
    "    sorted_stability = sorted(stability_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for algo_name, stability in sorted_stability:\n",
    "        print(f\"{algo_name:30}: {stability:.1f}%\")\n",
    "\n",
    "    return best_methods_by_scenario, sorted_stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UyhmCxLnZEoH",
    "outputId": "c51772b2-085b-4719-a946-e5a8b643dfbd"
   },
   "outputs": [],
   "source": [
    "# Запуск обучения\n",
    "all_scenario_results, warehouses_df = train_rl_all_warehouses_noise_scenarios()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kwG9GgTZRRL",
    "outputId": "b5e9f3c8-941e-4e19-ce6a-d1bbc55052e7"
   },
   "outputs": [],
   "source": [
    "# Создание сводных таблиц\n",
    "summary_df, pivot_best, pivot_avg, pivot_time = create_rl_summary_tables(all_scenario_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811
    },
    "id": "R6mdlSr9qNCX",
    "outputId": "e8a243d7-459b-4319-8e4e-6a51a60a972b"
   },
   "outputs": [],
   "source": [
    "# Визуализация результатов\n",
    "sorted_algorithms = plot_rl_comprehensive_results(all_scenario_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHwFkxiEqVwR",
    "outputId": "d36fc1c6-596b-4c1f-eb87-16a6b429c3ca"
   },
   "outputs": [],
   "source": [
    "# Анализ результатов\n",
    "best_methods_by_scenario, sorted_stability = analyze_rl_results(all_scenario_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "U-dLQJk_qbFK",
    "outputId": "8391e7bc-65cf-4c6f-b5d8-3f0d18f06b7f"
   },
   "outputs": [],
   "source": [
    "# Визуализация кривых обучения (опционально)\n",
    "plot_rl_learning_curves(all_scenario_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSSbsMlCtih1",
    "outputId": "6a1c66af-98b5-4f41-efed-f3f82596fbdd"
   },
   "outputs": [],
   "source": [
    "def save_rl_routes_to_json(all_scenario_results, warehouses_df, filename='rl_routes.json'):\n",
    "    \"\"\"\n",
    "    Сохраняет найденные RL маршруты в JSON файл\n",
    "\n",
    "    Args:\n",
    "        all_scenario_results: результаты RL методов\n",
    "        warehouses_df: DataFrame со складами\n",
    "        filename: имя файла для сохранения\n",
    "    \"\"\"\n",
    "\n",
    "    # Создаем структуру данных\n",
    "    routes_dict = {\n",
    "        'metadata': {\n",
    "            'total_scenarios': len(all_scenario_results),\n",
    "            'total_routes': sum(len(scenario) for scenario in all_scenario_results.values()),\n",
    "            'saved_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        },\n",
    "        'warehouses': [],\n",
    "        'scenarios': {}\n",
    "    }\n",
    "\n",
    "    # Сохраняем информацию о складах\n",
    "    for idx, warehouse in warehouses_df.iterrows():\n",
    "        routes_dict['warehouses'].append({\n",
    "            'id': idx,\n",
    "            'name': warehouse['name'],\n",
    "            'latitude': float(warehouse['latitude']),\n",
    "            'longitude': float(warehouse['longitude']),\n",
    "            'type': warehouse.get('type', 'unknown')\n",
    "        })\n",
    "\n",
    "    # Сохраняем маршруты по сценариям\n",
    "    for scenario_name, scenario_results in all_scenario_results.items():\n",
    "        routes_dict['scenarios'][scenario_name] = {}\n",
    "\n",
    "        for algorithm_name, result in scenario_results.items():\n",
    "            route_indices = result['best_route']\n",
    "\n",
    "            # Детальная информация о маршруте\n",
    "            route_details = []\n",
    "            for i, idx in enumerate(route_indices[:-1]):  # Исключаем последний дубликат\n",
    "                if idx < len(warehouses_df):\n",
    "                    warehouse = warehouses_df.iloc[idx]\n",
    "                    route_details.append({\n",
    "                        'order': i + 1,\n",
    "                        'warehouse_id': idx,\n",
    "                        'warehouse_name': warehouse['name'],\n",
    "                        'latitude': float(warehouse['latitude']),\n",
    "                        'longitude': float(warehouse['longitude'])\n",
    "                    })\n",
    "\n",
    "            routes_dict['scenarios'][scenario_name][algorithm_name] = {\n",
    "                'distance_km': float(result['best_distance']),\n",
    "                'avg_distance_km': float(result.get('avg_distance', 0)),\n",
    "                'training_time_sec': float(result.get('training_time', 0)),\n",
    "                'route_indices': [int(idx) for idx in route_indices],\n",
    "                'route_details': route_details\n",
    "            }\n",
    "\n",
    "    # Сохраняем в JSON файл\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(routes_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Маршруты сохранены в JSON файл: {filename}\")\n",
    "    return routes_dict\n",
    "\n",
    "# Сохраняем в JSON\n",
    "rl_routes_json = save_rl_routes_to_json(all_scenario_results, warehouses_df, 'rl_routes_detailed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrFseqA2J0__"
   },
   "outputs": [],
   "source": [
    "# Создаем словарь с графами для каждого сценария\n",
    "scenarios_graphs = {\n",
    "    \"Без шума\": graph,\n",
    "    \"Пробки\": graph_traffic_jam,\n",
    "    \"Блокировки+Пробки\": graph_combined,\n",
    "    \"Умеренные условия\": graph_moderate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OCpTry_uZDm"
   },
   "outputs": [],
   "source": [
    "def get_full_route_path_on_roads(graph, route_indices, warehouse_nodes):\n",
    "    \"\"\"\n",
    "    Получает полный путь по дорожной сети для заданного маршрута\n",
    "\n",
    "    Args:\n",
    "        graph: дорожный граф\n",
    "        route_indices: список индексов складов\n",
    "        warehouse_nodes: список узлов графа для складов\n",
    "\n",
    "    Returns:\n",
    "        list: список координат (lat, lon) пути по дорогам\n",
    "    \"\"\"\n",
    "    full_path = []\n",
    "\n",
    "    for i in range(len(route_indices) - 1):\n",
    "        from_idx = route_indices[i]\n",
    "        to_idx = route_indices[i + 1]\n",
    "\n",
    "        from_node = warehouse_nodes[from_idx]\n",
    "        to_node = warehouse_nodes[to_idx]\n",
    "\n",
    "        try:\n",
    "            # Находим кратчайший путь по дорожной сети\n",
    "            path_nodes = nx.shortest_path(graph, from_node, to_node, weight='length')\n",
    "\n",
    "            # Получаем координаты всех узлов пути\n",
    "            for node in path_nodes:\n",
    "                node_data = graph.nodes[node]\n",
    "                full_path.append((node_data['y'], node_data['x']))  # (lat, lon)\n",
    "        except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "            # Если пути нет, используем прямую линию между складами\n",
    "            from_data = graph.nodes[from_node]\n",
    "            to_data = graph.nodes[to_node]\n",
    "            full_path.append((from_data['y'], from_data['x']))\n",
    "            full_path.append((to_data['y'], to_data['x']))\n",
    "\n",
    "    return full_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ioR__LTVuPEw",
    "outputId": "c5a12b03-04ba-4527-cf50-2abd0043a858"
   },
   "outputs": [],
   "source": [
    "print(\"\\n1. Визуализация лучших RL маршрутов по сценариям (по дорогам)...\")\n",
    "\n",
    "# Создаем сетку 2x2 для лучших маршрутов\n",
    "fig_best, axes_best = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes_best = axes_best.flatten()\n",
    "\n",
    "scenario_names = list(all_scenario_results.keys())\n",
    "\n",
    "# Получаем узлы графа для всех складов\n",
    "# (должны быть определены в коде RL)\n",
    "if 'all_warehouse_nodes' in locals() or 'all_warehouse_nodes' in globals():\n",
    "    warehouse_nodes = all_warehouse_nodes\n",
    "else:\n",
    "    # Если не найдено, создаем заново\n",
    "    print(\"Создаем привязку складов к узлам графа...\")\n",
    "    warehouse_nodes = []\n",
    "    for idx, warehouse in warehouses_df.iterrows():\n",
    "        point = (warehouse['latitude'], warehouse['longitude'])\n",
    "        nearest_node = ox.distance.nearest_nodes(graph, point[1], point[0])\n",
    "        warehouse_nodes.append(nearest_node)\n",
    "\n",
    "for idx, scenario_name in enumerate(scenario_names):\n",
    "    ax = axes_best[idx]\n",
    "    scenario_graph = scenarios_graphs[scenario_name]\n",
    "\n",
    "    # Находим лучший метод для этого сценария\n",
    "    scenario_results = all_scenario_results[scenario_name]\n",
    "    best_method = min(scenario_results.keys(),\n",
    "                     key=lambda x: scenario_results[x]['best_distance'])\n",
    "\n",
    "    best_result = scenario_results[best_method]\n",
    "    best_route = best_result['best_route']\n",
    "    best_distance = best_result['best_distance']\n",
    "\n",
    "    # Получаем полный путь по дорогам\n",
    "    road_path = get_full_route_path_on_roads(scenario_graph, best_route, warehouse_nodes)\n",
    "\n",
    "    # Преобразуем координаты\n",
    "    if road_path:\n",
    "        road_lats = [coord[0] for coord in road_path]\n",
    "        road_lons = [coord[1] for coord in road_path]\n",
    "\n",
    "    # Рисуем дорожную сеть\n",
    "    ox.plot_graph(\n",
    "        scenario_graph,\n",
    "        ax=ax,\n",
    "        node_size=0,\n",
    "        edge_linewidth=0.3,\n",
    "        edge_color='lightgray',\n",
    "        bgcolor='white',\n",
    "        show=False,\n",
    "        close=False\n",
    "    )\n",
    "\n",
    "    # Рисуем склады\n",
    "    ax.scatter(warehouses_df['longitude'], warehouses_df['latitude'],\n",
    "              c='lightgray', s=20, alpha=0.3, edgecolors='none', zorder=2)\n",
    "\n",
    "    # Рисуем маршрут по дорогам\n",
    "    if road_path and len(road_lats) > 1 and len(road_lons) > 1:\n",
    "        ax.plot(road_lons, road_lats,\n",
    "               color='blue', linewidth=2.5, alpha=0.8, zorder=3,\n",
    "               label='Маршрут по дорогам')\n",
    "\n",
    "        # Добавляем стрелки направления\n",
    "        for i in range(0, len(road_lats)-1, max(1, len(road_lats)//20)):\n",
    "            ax.annotate('', xy=(road_lons[i+1], road_lats[i+1]),\n",
    "                       xytext=(road_lons[i], road_lats[i]),\n",
    "                       arrowprops=dict(arrowstyle='->', color='red',\n",
    "                                     lw=2, alpha=0.6))\n",
    "\n",
    "    # Рисуем склады из маршрута\n",
    "    for i, idx_point in enumerate(best_route[:-1]):  # Исключаем последний (дубликат начала)\n",
    "        if idx_point < len(warehouses_df):\n",
    "            warehouse = warehouses_df.iloc[idx_point]\n",
    "            color = 'red' if i == 0 else 'green'\n",
    "            size = 100 if i == 0 else 60\n",
    "            marker = '*' if i == 0 else 'o'\n",
    "\n",
    "            ax.scatter(warehouse['longitude'], warehouse['latitude'],\n",
    "                      c=color, s=size, marker=marker, edgecolors='black',\n",
    "                      linewidth=1.5, zorder=5,\n",
    "                      label='Начальная точка' if i == 0 else None)\n",
    "\n",
    "    # Подписываем начальный склад\n",
    "    if best_route and best_route[0] < len(warehouses_df):\n",
    "        start_warehouse = warehouses_df.iloc[best_route[0]]\n",
    "        ax.annotate(start_warehouse['name'][:15] + '...'\n",
    "                   if len(start_warehouse['name']) > 15\n",
    "                   else start_warehouse['name'],\n",
    "                   (start_warehouse['longitude'], start_warehouse['latitude']),\n",
    "                   xytext=(5, 5), textcoords='offset points',\n",
    "                   fontsize=8, fontweight='bold',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    ax.set_title(f'{scenario_name}\\n{best_method}\\n{best_distance:.2f} км',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Долгота')\n",
    "    ax.set_ylabel('Широта')\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "    # Добавляем легенду только для первого графика\n",
    "    if idx == 0:\n",
    "        from matplotlib.lines import Line2D\n",
    "        legend_elements = [\n",
    "            Line2D([0], [0], color='blue', lw=3, label='Маршрут по дорогам'),\n",
    "            Line2D([0], [0], color='red', lw=0, marker='*', markersize=10,\n",
    "                  label='Начальная точка', markeredgecolor='black'),\n",
    "            Line2D([0], [0], color='green', lw=0, marker='o', markersize=8,\n",
    "                  label='Склады в маршруте', markeredgecolor='black'),\n",
    "            Line2D([0], [0], color='lightgray', lw=0, marker='o', markersize=6,\n",
    "                  label='Все склады', alpha=0.3, markeredgecolor='none')\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='upper right', fontsize=8)\n",
    "\n",
    "plt.suptitle('Лучшие RL маршруты по сценариям (по дорогам)',\n",
    "            fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rl_best_routes_all_scenarios_roads.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DALWCarvpylC",
    "wxgF5rDwBxyB",
    "99BeSsvVsAZ0"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
